{
  "hash": "ad868712ae5f01c087006d53b42f19ce",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Dempster-Shafer Theory, notes\"\nauthor: \"Author: Andrew D. Nguyen\"\ndate: \"2023-08-27\"\ntoc: TRUE\n---\n\n\\newpage\n\n# Introduction - plain English understanding\n\nDempster-Shafer Theory (DST) is a way to account for uncertainty when making a decision. For example, imagine the answer to a question has two mutually exclusive outcomes: yes or no. Then, DST, would describe this as the *frame of discernment*, or $\\theta$, where,\n\n$$ \\theta = \\{yes,no\\}$$ But, when we try to estimate these answers, there are multiple possibilities, especially if you're unsure. For example, sometimes when you measure something, you're not sure if the answer is yes or no. All possible states are represented as a power set, $2^\\theta$, such that\n\n$$2^\\theta = \\{\\{\\emptyset\\},\\{yes\\},\\{no\\},\\{\\theta\\}\\}$$ and notice that $\\theta = \\{yes,no\\}$, which is the set where you're sure whether the answer is yes or no.\n\nNow, each set can have a numerical value assigned to them and can be expressed as:\n\n$$m:2^\\theta -> [0,1]$$\\\nand the value is referred to as a mass. From the formulation above, the mass of the powerset is between 0 and 1, referred to as the mass function. The sum of the masses of the powerset add up to 1:\n\n$$\\sum_{A\\in2^\\theta} m(A)=1$$. In other words, for all members (A) within the powerset $2^\\theta$, the sum of all these members is 1. Naturally, values closer to 1 means there is more evidence for that particular set (m(A)). For example,\n\n| $2^\\theta$    | Mass |\n|---------------|------|\n| {yes}         | 0.2  |\n| {no}          | 0.6  |\n| {yes, no}     | 0.2  |\n| {$\\emptyset$} | 0    |\n\nthe mass assignments all sum to 1. Notice that the $\\emptyset$ is 0, which is a feature of DST expressed in this way. And notice that typically, in say a logistic regression, the posterior probabilities are a way to fill in these masses for a {yes} or {no} answer.\n\nMost notably, the support for what we care about, $\\theta$ have overlaps in the sets. For example, {yes,no} overlaps with {yes}. So how can we express the real answer given this framework? DST attempts to do this by forming two levels of support for an answer in $\\theta$ such that:\n\n-   The belief is the lowest level of support\\\n-   The plausibility is the highest level of support\n\nThe \\underline{belief} in {yes} is the sum of the masses of \\colorbox{yellow}{subset} (B) of {yes}, expressed as:\n\n$$bel(\\{yes\\}) = \\sum_{B\\subset \\{yes\\}} m(B).$$\n\nThe \\underline{plausibility} of {yes} is the sum of all masses of sets B that \\colorbox{yellow}{intersects} with {yes}, expressed as:\n\n$$pl(\\{yes\\} = \\sum_{B\\cap \\{yes\\}} m(B).$$\\\nIn our example from the mass assignments, they would induce the following beliefs and plausibilities:\n\n| $2^\\theta$    | Mass | Belief | Plausibility |\n|---------------|------|--------|--------------|\n| {yes}         | 0.2  | 0.2    | 0.4          |\n| {no}          | 0.6  | 0.6    | 0.8          |\n| {yes, no}     | 0.2  | 1.0    | 1.0          |\n| {$\\emptyset$} | 0.0  | 0.0    | 0.0          |\n\nFor example, the belief in {yes} is the mass of {yes} (0.2). The plausibility in {yes} is the mass of {yes} and {yes,no} because they intersect (0.2 + 0.2 = 0.4).\n\n# Possible ways to decide based on beliefs and plausibility\n\nGiven this evidence, how do we decide on whether the answer to the question? There would be 3 outcomes instead of 2:\n\n-   Yes\n-   No\n-   Don't know ({yes,no})\n\nI'll simulate some data and show how. Load packages first.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) # data wrangling, visualization\nlibrary(EvCombR) # package for dempster shafer\n##ggplot2 settings\nT<-theme_bw()+theme(,text=element_text(size=14),\n                    axis.text=element_text(size=14),\n                    panel.grid.major=element_blank(),\n                    panel.grid.minor.x = element_blank(),\n                    panel.grid = element_blank(),\n                    legend.key = element_blank(),                    axis.title.y=element_text(margin=margin(t=0,r=15,b=0,l=0)),\n                    \naxis.title.x=element_text(margin=margin(t=15,r=,b=0,l=0)))+\ntheme(legend.position=\"none\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# simulate some beliefs \n# and plausibilities\n# that would lead to each type of outcome\noutcome<-rep(c(\"{yes}\",\"{no}\",\"{yes,no}\"),3)\ndecision<-c(rep(\"No\",3),rep(c(\"Yes\"),3),rep(c(\"Don't know\"),3))\nbelief<-c(c(.2,.6,.2),c(.6,.2,.2),c(.25,.25,.5))\npl<-c(c(0.4,.8,1),c(0.8,.4,1),c(0.75,.75,1))\nd<-data.frame(outcome,decision,belief,pl)\nd<-d%>%\n  dplyr::filter(outcome!=\"{yes,no}\")\n#filter out {yes,no}\nd$decision<-factor(d$decision,levels=c(\"Yes\",\"No\",\"Don't know\"))\n\n#plot it out\nggplot(d,aes(x=outcome))+\n  geom_errorbar(aes(ymax=pl,ymin=belief),\n                width=.5,linewidth=.85)+\n  facet_wrap(~decision)+\n  scale_y_continuous(limits = c(0,1),\n                     breaks=seq(0,1,.1),\n                     labels=seq(0,1,.1))+\n  ylab(\"Support\")+xlab(\"Outcome\")+\n  geom_hline(yintercept=.5,linetype=\"dotdash\")+T\n```\n\n::: {.cell-output-display}\n![](DSTmodel_uncertainty_files/figure-html/simulation-1.png){width=672}\n:::\n:::\n\n\nHere, you can see that the lower bound of the range in each outcome is the belief and the upper bound of the range is the plausibility. Each panel would show the decision being made. Notice that in the \"Don't know\" panel, the levels of support overlap. There are ways to make decisions even if the levels of support overlap but we won't get into that.\n\nDST is very flexible and there are no set rules for constructing mass functions.\n\nTypically, posterior probabilities from say a logistic regression would show a flat level of support (only the belief). However, the posterior probabilities can be restructured into a mass function by accounting for model uncertainty. Model uncertainty can be extracted from a confusion matrix. So the posterior probability for yes would be scaled by the positive predictive value, which is the degree of confidence in the model to make a yes call. This is how we can account for uncertainty when using a predictive model.\n\n# Simulations of model uncertainty\n\nHere, I'll simulate posterior probabilities, and model uncertainties to determine how it impacts decision making.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#simulate a range of model uncertainty \n#both in the positive predictive value \n# and negative predictive value \n\n## simulate a 1000 samples with posterior probs\nyes<-rep(seq(0,1,.1),11)\n\n#number of different model uncertainties\nmu<-sort(rep(seq(0,1,.1),11)) #simulate model\n# uncertainty from 0 to 1 in .1 steps \ndat<-data.frame(cbind(yes,mu))\ndat$no<- (1-dat$yes) # get the no post prob\n#assuming model uncertainty is the same\n# same PPV and NPV\n##now we can construct mass assignments\ndat<-dat%>%\n  mutate(y.mass=yes*mu,n.mass=no*mu,yn.mass=1-y.mass-n.mass,y.pl=yn.mass+y.mass,n.pl=yn.mass+n.mass)\n\n##lets set up factors by decision point\n#if the uppper bound of yes is less than\n#lower bound of no, then decide NO\n\n#if upper bound of no is less than \n#lower bound of yes, then decide YES \ndat<-dat%>%\n  mutate(decision=ifelse(y.pl<n.mass,\"No\",ifelse(n.pl<y.mass,\"Yes\",\"Don't know\")))\nglimpse(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 121\nColumns: 9\n$ yes      <dbl> 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 0.0, 0…\n$ mu       <dbl> 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0…\n$ no       <dbl> 1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0, 1.0, 0…\n$ y.mass   <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ n.mass   <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ yn.mass  <dbl> 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9, 0…\n$ y.pl     <dbl> 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1…\n$ n.pl     <dbl> 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1…\n$ decision <chr> \"Don't know\", \"Don't know\", \"Don't know\", \"Don't know\", \"Don'…\n```\n\n\n:::\n\n```{.r .cell-code}\n#need to convert to long\nd.long<-dat%>%\n  pivot_longer(cols=y.mass:n.mass,names_to = \"Outcome\",values_to = \"support\")%>%\n  mutate(plaus=support+yn.mass)\n\n##let's plot out the simulation \nggplot(d.long,aes(x=yes,colour=decision))+\n  geom_errorbar(aes(ymax=plaus,ymin=support),\n                width=.5,linewidth=.85)+\n  facet_wrap(~mu,nrow=4)+\n  scale_y_continuous(limits = c(0,1),\n                     breaks=seq(0,1,.1),\n                     labels=seq(0,1,.1))+\n  ylab(\"Support\")+xlab(\"Simulated Posterior Probabilities\")+\n  geom_hline(yintercept=.5,linetype=\"dotdash\")+T+theme(legend.position = \"top\")\n```\n\n::: {.cell-output-display}\n![](DSTmodel_uncertainty_files/figure-html/simulating model uncertainty-1.png){width=672}\n:::\n:::\n\n\nThoughts:\n\nWhat you can see is that decisions can't be made if there is too much uncertainty (0-0.5 model performance). But, at 0.6 model performance, posterior probabilities have to be very high in order to make a confident decision. And then you can see traditionally, if we assume a perfect model, model performance = 1, then the support bands are flat, which is typically how we treat predictive models in general.\n\n# Summary\n\nDST offers a flexible and creative approach to accounting for uncertainty when making a decision. DST re-frames sources of evidence such that uncertain cases can have some level of evidence assigned to it. The assignment of the degree of evidence, or masses can resemble probabilities across all answers. The approach outlined here involves restructuring poster probabilities from a model into masses. This is done by scaling the posterior probabilities by model performance, which is derived from the confusion matrix. In traditional machine learning approaches, data are split into training and testing. The confusion matrix of the model applied to the testing dataset could be used. Based on the masses, then the degree of support can be calculated to help inform decision-making.\n\n# RshinyApp\n\nTry plugging in your own posterior probabilities or masses and plug in your own PPV or NPV in this [**RShinyApp**](https://antdrewdnguyen.shinyapps.io/dempster_shafer/)**.**\n\n# References\n\n-   Rathman JF, Yang C, Zhou H. [**Dempster-Shafer theory for combining in silico evidence and estimating uncertainty in chemical risk assessment**](https://www.sciencedirect.com/science/article/abs/pii/S2468111318300197). Comput Toxicol. 2018;6:16-31. <doi:10.1016/j.comtox.2018.03.001>\n\n# Session info\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR version 4.5.0 (2025-04-11 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 26100)\n\nMatrix products: default\n  LAPACK version 3.12.1\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] EvCombR_0.1-4   lubridate_1.9.4 forcats_1.0.0   stringr_1.5.1  \n [5] dplyr_1.1.4     purrr_1.0.4     readr_2.1.5     tidyr_1.3.1    \n [9] tibble_3.2.1    ggplot2_3.5.2   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6      jsonlite_2.0.0    compiler_4.5.0    tidyselect_1.2.1 \n [5] scales_1.3.0      yaml_2.3.10       fastmap_1.2.0     R6_2.6.1         \n [9] labeling_0.4.3    generics_0.1.3    knitr_1.50        htmlwidgets_1.6.4\n[13] munsell_0.5.1     pillar_1.10.2     tzdb_0.5.0        rlang_1.1.6      \n[17] stringi_1.8.7     xfun_0.52         timechange_0.3.0  cli_3.6.4        \n[21] withr_3.0.2       magrittr_2.0.3    digest_0.6.37     grid_4.5.0       \n[25] hms_1.1.3         lifecycle_1.0.4   vctrs_0.6.5       evaluate_1.0.3   \n[29] glue_1.8.0        farver_2.1.2      colorspace_2.1-1  rmarkdown_2.29   \n[33] tools_4.5.0       pkgconfig_2.0.3   htmltools_0.5.8.1\n```\n\n\n:::\n:::\n\n",
    "supporting": [
      "DSTmodel_uncertainty_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}