
@misc{zhang_cran_2021,
	title = {{CRAN} {Task} {View}: {Clinical} {Trial} {Design}, {Monitoring}, and {Analysis}},
	shorttitle = {{CRAN} {Task} {View}},
	url = {https://CRAN.R-project.org/view=ClinicalTrials},
	abstract = {This task view gathers information on specific R packages for design, monitoring and analysis of data from clinical trials. It focuses on including packages for clinical trial design and monitoring in general plus data analysis packages for a specific type of design. Also, it gives a brief introduction to important packages for analyzing clinical trial data. Please refer to task views ExperimentalDesign, Survival, Pharmacokinetics, Meta-analysis for more details on these topics.},
	urldate = {2023-12-27},
	author = {Zhang, Ed and Zhang, W. G. and Zhang, R. G.},
	month = dec,
	year = {2021},
	note = {Publisher: Comprehensive R Archive Network (CRAN)},
	file = {Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\VKYC3L8F\\ClinicalTrials.html:text/html},
}

@book{higgins_2023,
	title = {Chapter 20 {Randomization} for {Clinical} {Trials} with {R} {\textbar} {Reproducible} {Medical} {Research} with {R}},
	url = {https://bookdown.org/pdr_higgins/rmrwr/randomization-for-clinical-trials-with-r.html},
	abstract = {This book is for anyone in the medical field interested in learning R to analyze available health data.},
	urldate = {2023-12-27},
	author = {Peter D. R. Higgins},
	year = {2023},
	file = {Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\2GBWXCBH\\randomization-for-clinical-trials-with-r.html:text/html},
}


@article{stuart_using_2014,
	title = {Using propensity scores in difference-in-differences models to estimate the effects of a policy change {\textbar} {Health} {Services} and {Outcomes} {Research} {Methodology}},
	url = {https://link.springer.com/article/10.1007/s10742-014-0123-z},
	urldate = {2024-04-23},
	author = {Stuart, Elizabeth},
	year = {2014},
	keywords = {causal inference, DiD, propensity score},
	file = {Using propensity scores in difference-in-differenc.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\HKBTVMA4\\Using propensity scores in difference-in-differenc.pdf:application/pdf;Using propensity scores in difference-in-differences models to estimate the effects of a policy change | Health Services and Outcomes Research Methodology:C\:\\Users\\anbe6\\Zotero\\storage\\HLWHLLK3\\s10742-014-0123-z.html:text/html},
}

@article{dahabreh_causal_2024,
	title = {Causal {Inference} {About} the {Effects} of {Interventions} {From} {Observational} {Studies} in {Medical} {Journals}},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.2024.7741},
	doi = {10.1001/jama.2024.7741},
	abstract = {Many medical journals, including JAMA, restrict the use of causal language to the reporting of randomized clinical trials. Although well-conducted randomized clinical trials remain the preferred approach for answering causal questions, methods for observational studies have advanced such that causal interpretations of the results of well-conducted observational studies may be possible when strong assumptions hold. Furthermore, observational studies may be the only practical source of information for answering some questions about the causal effects of medical or policy interventions, can support the study of interventions in populations and settings that reflect practice, and can help identify interventions for further experimental investigation. Identifying opportunities for the appropriate use of causal language when describing observational studies is important for communication in medical journals.A structured approach to whether and how causal language may be used when describing observational studies would enhance the communication of research goals, support the assessment of assumptions and design and analytic choices, and allow for more clear and accurate interpretation of results. Building on the extensive literature on causal inference across diverse disciplines, we suggest a framework for observational studies that aim to provide evidence about the causal effects of interventions based on 6 core questions: what is the causal question; what quantity would, if known, answer the causal question; what is the study design; what causal assumptions are being made; how can the observed data be used to answer the causal question in principle and in practice; and is a causal interpretation of the analyses tenable?Adoption of the proposed framework to identify when causal interpretation is appropriate in observational studies promises to facilitate better communication between authors, reviewers, editors, and readers. Practical implementation will require cooperation between editors, authors, and reviewers to operationalize the framework and evaluate its effect on the reporting of empirical research.},
	urldate = {2024-05-14},
	journal = {JAMA},
	author = {Dahabreh, Issa J. and Bibbins-Domingo, Kirsten},
	month = may,
	year = {2024},
	keywords = {causal inference},
	file = {Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\ISPW22J2\\2818746.html:text/html},
}

@article{baker_dpyd_2023,
	title = {{DPYD} {Testing}: {Time} to {Put} {Patient} {Safety} {First}},
	copyright = {© 2023 by American Society of Clinical Oncology},
	shorttitle = {{DPYD} {Testing}},
	url = {https://ascopubs.org/doi/10.1200/JCO.22.02364},
	doi = {10.1200/JCO.22.02364},
	language = {EN},
	urldate = {2024-05-14},
	journal = {Journal of Clinical Oncology},
	author = {Baker, Sharyn D. and Bates, Susan E. and Brooks, Gabriel A. and Dahut, William L. and Diasio, Robert B. and El-Deiry, Wafik S. and Evans, William E. and Figg, William D. and Hertz, Dan L. and Hicks, J. Kevin and Kamath, Suneel and Kasi, Pashtoon Murtaza and Knepper, Todd C. and McLeod, Howard L. and O'Donnell, Peter H. and Relling, Mary V. and Rudek, Michelle A. and Sissung, Tristan M. and Smith, D. Max and Sparreboom, Alex and Swain, Sandra M. and Walko, Christine M.},
	month = feb,
	year = {2023},
	note = {Publisher: Wolters Kluwer Health},
	file = {Baker et al. - 2023 - DPYD Testing Time to Put Patient Safety First.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\XJNGZ2JY\\Baker et al. - 2023 - DPYD Testing Time to Put Patient Safety First.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\R2356FSP\\JCO.22.html:text/html},
}

@article{hu_cimtx_2022,
	title = {{CIMTx}: {An} {R} {Package} for {Causal} {Inference} with {Multiple} {Treatments} using {Observational} {Data}},
	volume = {14},
	issn = {2073-4859},
	shorttitle = {{CIMTx}},
	url = {https://doi.org/10.32614/RJ-2022-058/},
	doi = {10.32614/RJ-2022-058},
	abstract = {[CIMTx](https://CRAN.R-project.org/package=CIMTx) provides efficient and unified functions to implement modern methods for causal inferences with multiple treatments using observational data with a focus on binary outcomes. The methods include regression adjustment, inverse probability of treatment weighting, Bayesian additive regression trees, regression adjustment with multivariate spline of the generalized propensity score, vector matching and targeted maximum likelihood estimation. In addition, [CIMTx](https://CRAN.R-project.org/package=CIMTx) illustrates ways in which users can simulate data adhering to the complex data structures in the multiple treatment setting. Furthermore, the [CIMTx](https://CRAN.R-project.org/package=CIMTx) package offers a unique set of features to address the key causal assumptions: positivity and ignorability. For the positivity assumption, [CIMTx](https://CRAN.R-project.org/package=CIMTx) demonstrates techniques to identify the common support region for retaining inferential units using inverse probability of treatment weighting, Bayesian additive regression trees and vector matching. To handle the ignorability assumption, [CIMTx](https://CRAN.R-project.org/package=CIMTx) provides a flexible Monte Carlo sensitivity analysis approach to evaluate how causal conclusions would be altered in response to different magnitude of departure from ignorable treatment assignment.},
	number = {3},
	urldate = {2024-05-13},
	journal = {The R Journal},
	author = {Hu, Liangyuan and Ji, Jiayi},
	month = dec,
	year = {2022},
	keywords = {causal inference},
	pages = {213--230},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\GHG4ZFFF\\Hu and Ji - 2022 - CIMTx An R Package for Causal Inference with Mult.pdf:application/pdf},
}

@article{noauthor_assessing_nodate,
	title = {Assessing covariate balance when using the generalized propensity score with quantitative or continuous exposures},
	url = {https://journals.sagepub.com/doi/epub/10.1177/0962280218756159},
	language = {en},
	urldate = {2024-05-12},
	doi = {10.1177/0962280218756159},
	keywords = {causal inference, propensity score},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\SIS7Q6IC\\Assessing covariate balance when using the general.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\W3MR5LP9\\0962280218756159.html:text/html},
}

@article{khoshnevis_gpcerf_2024,
	title = {{GPCERF} - {An} {R} package for implementing {Gaussianprocesses} for estimating causal exposure response curves},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	issn = {2475-9066},
	url = {https://joss.theoj.org/papers/10.21105/joss.05465},
	doi = {10.21105/joss.05465},
	abstract = {We present the GPCERF R package, which employs a novel Bayesian approach based on Gaussian Process (GP) to estimate the causal exposure-response function (CERF) for continuous exposures, along with associated uncertainties. R packages that target causal effects under a binary exposure setting exist (e.g., Ho et al., 2011), as well as in the continuous exposure setting (e.g., Khoshnevis et al., 2023). However, they often rely on a separate resampling stage to quantify uncertainty of the estimates. GPCERF provides a two-step end-to-end solution for causal inference with continuous exposures that is equipped with automatic and efficient uncertainty quantification. During the first step (the design phase), the algorithm searches for optimal hyperparameters (using the exposures and covariates) that achieve optimal covariate balance in the induced pseudo-population, i.e., that the correlation between the exposure and each covariate is close to zero. The selected hyperparameters are then used in the second step (the analysis phase) to estimate the CERF on the balanced data set and its associated uncertainty using two different types of GPs: a standard GP and a nearest-neighbor GP (nnGP). The standard GP offers high accuracy in estimating CERF but is also computationally intensive. The nnGP is a computationally efficient approximation of the standard GP and is well-suited for the analysis of large-scale datasets.},
	language = {en},
	number = {95},
	urldate = {2024-05-10},
	journal = {JOSS},
	author = {Khoshnevis, Naeem and Ren, Boyu and Braun, Danielle},
	month = mar,
	year = {2024},
	keywords = {causal inference, Bayesian},
	pages = {5465},
	file = {Khoshnevis et al. - 2024 - GPCERF - An R package for implementing Gaussianpro.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\A2C8BIVQ\\Khoshnevis et al. - 2024 - GPCERF - An R package for implementing Gaussianpro.pdf:application/pdf},
}

@article{kunzmann_review_2021,
	title = {A {Review} of {Bayesian} {Perspectives} on {Sample} {Size} {Derivation} for {Confirmatory} {Trials}},
	volume = {75},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2021.1901782},
	doi = {10.1080/00031305.2021.1901782},
	abstract = {Sample size derivation is a crucial element of planning any confirmatory trial. The required sample size is typically derived based on constraints on the maximal acceptable Type I error rate and minimal desired power. Power depends on the unknown true effect and tends to be calculated either for the smallest relevant effect or a likely point alternative. The former might be problematic if the minimal relevant effect is close to the null, thus requiring an excessively large sample size, while the latter is dubious since it does not account for the a priori uncertainty about the likely alternative effect. A Bayesian perspective on sample size derivation for a frequentist trial can reconcile arguments about the relative a priori plausibility of alternative effects with ideas based on the relevance of effect sizes. Many suggestions as to how such “hybrid” approaches could be implemented in practice have been put forward. However, key quantities are often defined in subtly different ways in the literature. Starting from the traditional entirely frequentist approach to sample size derivation, we derive consistent definitions for the most commonly used hybrid quantities and highlight connections, before discussing and demonstrating their use in sample size derivation for clinical trials.},
	number = {4},
	urldate = {2024-05-09},
	journal = {The American Statistician},
	author = {Kunzmann, Kevin and Grayling, Michael J. and Lee, Kim May and Robertson, David S. and Rufibach, Kaspar and Wason, James M. S.},
	month = oct,
	year = {2021},
	pmid = {34992303},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00031305.2021.1901782},
	keywords = {sample size, Bayesian, trial design},
	pages = {424--432},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\56GNXTVY\\Kunzmann et al. - 2021 - A Review of Bayesian Perspectives on Sample Size D.pdf:application/pdf},
}

@article{ionan_bayesian_2023,
	title = {Bayesian {Methods} in {Human} {Drug} and {Biological} {Products} {Development} in {CDER} and {CBER}},
	volume = {57},
	issn = {2168-4804},
	url = {https://doi.org/10.1007/s43441-022-00483-0},
	doi = {10.1007/s43441-022-00483-0},
	abstract = {The Center for Drug Evaluation and Research (CDER) and the Center for Biologics Evaluation and Research (CBER) of the U.S. Food and Drug Administration (FDA) have been leaders in protecting and promoting the U.S. public health by helping to ensure that safe and effective drugs and biological products are available in the United States for those who need them. The null hypothesis significance testing approach, along with other considerations, is typically used to demonstrate the effectiveness of a drug or biological product. The Bayesian framework presents an alternative approach to demonstrate the effectiveness of a treatment. This article discusses the Bayesian framework for drug and biological product development, highlights key settings in which Bayesian approaches may be appropriate, and provides recent examples of the use of Bayesian approaches within CDER and CBER.},
	language = {en},
	number = {3},
	urldate = {2024-05-09},
	journal = {Ther Innov Regul Sci},
	author = {Ionan, Alexei C. and Clark, Jennifer and Travis, James and Amatya, Anup and Scott, John and Smith, James P. and Chattopadhyay, Somesh and Salerno, Mary Jo and Rothmann, Mark},
	month = may,
	year = {2023},
	keywords = {Bayesian, trial design},
	pages = {436--444},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\CVHMN3T3\\Ionan et al. - 2023 - Bayesian Methods in Human Drug and Biological Prod.pdf:application/pdf},
}

@misc{williams_causal_2020,
	title = {Causal inference for multiple continuous exposures via the multivariate generalized propensity score},
	url = {http://arxiv.org/abs/2008.13767},
	doi = {10.48550/arXiv.2008.13767},
	abstract = {The generalized propensity score (GPS) is an extension of the propensity score for use with quantitative or continuous exposures (e.g., dose of medication or years of education). Current GPS methods allow estimation of the dose-response relationship between a single continuous exposure and an outcome. However, in many real-world settings, there are multiple exposures occurring simultaneously that could be causally related to the outcome. We propose a multivariate GPS method (mvGPS) that allows estimation of a dose-response surface that relates the joint distribution of multiple continuous exposure variables to an outcome. The method involves generating weights under a multivariate normality assumption on the exposure variables. Focusing on scenarios with two exposure variables, we show via simulation that the mvGPS method can achieve balance across sets of confounders that may differ for different exposure variables and reduces bias of the treatment effect estimates under a variety of data generating scenarios. We apply the mvGPS method to an analysis of the joint effect of two types of intervention strategies to reduce childhood obesity rates.},
	urldate = {2024-05-09},
	publisher = {arXiv},
	author = {Williams, Justin R. and Crespi, Catherine M.},
	month = aug,
	year = {2020},
	note = {arXiv:2008.13767 [stat]},
	keywords = {Statistics - Methodology},
	file = {arXiv.org Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\Y6MKL2GV\\2008.html:text/html},
}

@article{lee_using_2024,
	title = {Using {Bayesian} statistics in confirmatory clinical trials in the regulatory setting: a tutorial review},
	volume = {24},
	issn = {1471-2288},
	shorttitle = {Using {Bayesian} statistics in confirmatory clinical trials in the regulatory setting},
	url = {https://doi.org/10.1186/s12874-024-02235-0},
	doi = {10.1186/s12874-024-02235-0},
	abstract = {Bayesian statistics plays a pivotal role in advancing medical science by enabling healthcare companies, regulators, and stakeholders to assess the safety and efficacy of new treatments, interventions, and medical procedures. The Bayesian framework offers a unique advantage over the classical framework, especially when incorporating prior information into a new trial with quality external data, such as historical data or another source of co-data. In recent years, there has been a significant increase in regulatory submissions using Bayesian statistics due to its flexibility and ability to provide valuable insights for decision-making, addressing the modern complexity of clinical trials where frequentist trials are inadequate. For regulatory submissions, companies often need to consider the frequentist operating characteristics of the Bayesian analysis strategy, regardless of the design complexity. In particular, the focus is on the frequentist type I error rate and power for all realistic alternatives. This tutorial review aims to provide a comprehensive overview of the use of Bayesian statistics in sample size determination, control of type I error rate, multiplicity adjustments, external data borrowing, etc., in the regulatory environment of clinical trials. Fundamental concepts of Bayesian sample size determination and illustrative examples are provided to serve as a valuable resource for researchers, clinicians, and statisticians seeking to develop more complex and innovative designs.},
	number = {1},
	urldate = {2024-05-09},
	journal = {BMC Medical Research Methodology},
	author = {Lee, Se Yoon},
	month = may,
	year = {2024},
	keywords = {Bayesian hypothesis testing, Frequentist operating characteristics, Regulatory environment, Sample size determination},
	pages = {110},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\H3MLYWQR\\Lee - 2024 - Using Bayesian statistics in confirmatory clinical.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\WB9R3M69\\s12874-024-02235-0.html:text/html},
}

@article{noauthor_synth2_nodate,
	title = {synth2: {Synthetic} control method with placebo tests, robustness test, and visualization - {Guanpeng} {Yan}, {Qiang} {Chen}, 2023},
	url = {https://journals.sagepub.com/doi/abs/10.1177/1536867X231195278?journalCode=stja},
	urldate = {2024-05-01},
	keywords = {synthetic controls},
	file = {synth2\: Synthetic control method with placebo tests, robustness test, and visualization - Guanpeng Yan, Qiang Chen, 2023:C\:\\Users\\anbe6\\Zotero\\storage\\3UU9287W\\1536867X231195278.html:text/html},
}

@article{martin_palliative_2024,
	title = {Palliative care integration and end-of-life care intensity for patients with {NSCLC}},
	issn = {0169-5002},
	url = {https://www.sciencedirect.com/science/article/pii/S0169500224003349},
	doi = {10.1016/j.lungcan.2024.107800},
	abstract = {Background
Non-small cell lung cancer (NSCLC) without oncogenic driver mutations is considered to have a poor prognosis, although recent therapeutic progress. This study aims to assess the real-life integration of palliative care (PC) and the intensity of end-of-life (EOL) care for this population.
Methods
This was an observational cohort study of decedent patients from metastatic NSCLC without oncogenic driver mutations over the period 01/2018 to 12/2022, treated in first line with immunotherapy +/- chemotherapy. We analysed PC integration and aggressiveness criteria of EOL care in the last month before death: systemic anti-cancer treatment administration, emergency room visits, intensive care unit admission, hospitalization, hospitalization duration {\textgreater} 14 days, and hospital death.
Results
Among 149 patients, 75 (50 \%) met the PC team at least once, and the median time from the first encounter to death was 2.3 months. In the last month before death, at least one criterion of aggressive EOL care was present for 97 patients (70 \%). For patients with PC use {\textless} 30 days and for patients with PC use {\textless} 90 days before death, there were significant changes: increase in the frequency of systemic anti-cancer treatment (respectively 51.1 \% vs 20 \%; p {\textless} 0.001 and 58.7 \% vs 6.2 \%; p {\textless} 0.001); decrease in hospitalization lasting {\textgreater} 14 days (respectively 30 \% vs 7 \%; p = 0.001 and 36 \% vs 6.2 \%; p = 0.018) and in death hospitalisation (respectively 66 \% and 18 \%; p {\textless} 0.001 and 58.7 \% and 10.3 \%; p {\textless} 0.001). After adjusting for the factors tested, patients with no PC or late PC use in the last month before death or in the last three month before death, the odds ratio (OR) remained significantly greater than 1 (respectively OR = 3.97 [1.70; 9.98]; p = 0.001 and OR = 23.1 [5.21–177.0], p {\textless} 0.0001).
Conclusion
PC is still insufficiently integrated for patients with NSCL cancer. Cancer centres should monitor key indicators such as PC use and aggressiveness criteria of EOL care.},
	urldate = {2024-04-30},
	journal = {Lung Cancer},
	author = {Martin, A. and Carton, M. and Thery, L. and Burnod, A. and Daniel, C. and Du Rusquec, P. and Girard, N. and Bouleuc, C.},
	month = apr,
	year = {2024},
	keywords = {palliative care},
	pages = {107800},
	file = {Martin et al. - 2024 - Palliative care integration and end-of-life care i.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\D6YEBGBW\\Martin et al. - 2024 - Palliative care integration and end-of-life care i.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\SGCCYHAZ\\S0169500224003349.html:text/html},
}

@article{valenti_nurse-led_2023,
	title = {Nurse-led telephone follow-up for early palliative care patients with advanced cancer},
	volume = {32},
	issn = {1365-2702},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jocn.16403},
	doi = {10.1111/jocn.16403},
	abstract = {Aim and objectives To present our experience of a nursing telephone consultation service, describing patient and caregiver requests, and outlining ensuing nursing or medical interventions. Background Recently, there has been an increase in the use of telephone consultation for cancer patients. However, there is still limited data on the characteristics of this type of service and on the nature of the interventions carried out. Design and methods In this observational retrospective study, we evaluated the phone calls made over a 6-month period by patients or caregivers to the early palliative care team of a cancer institute. Information regarding telephone calls (frequency, reason and management) was systematically collected by a nursing case manager. The study complies with the STROBE checklist File S1. Results 171 patients used the service, for a total of 323 phone calls. The majority (80.8\%) were from patients followed at the outpatient clinic and the most common requests were for pain management (38.4\%) and for updates on the clinical situation (23.8\%). Other frequent requests were for medication management (18.9\%) and scheduling (18.3\%). 210 of the 323 phone calls were handled by the nurse, while 22 were managed in collaboration with a physician. An 87.6\% effectiveness in telephone management was observed. Conclusion The overall use of the phone service was higher for early palliative care patients. The majority of phone calls were effectively handled by the nursing case manager. Relevance to clinical practice An effective and feasible nurse-led telephone follow-up of early palliative care patients with advanced cancer could improve their care experience. Specifically, it could impact on patients and families improving quality of life and symptom control securing access to timely care without travel or additional cost.It can also improve continuity of care, adherence to oncological treatments and minimise acute care visits.},
	language = {en},
	number = {11-12},
	urldate = {2024-04-30},
	journal = {Journal of Clinical Nursing},
	author = {Valenti, Vanessa and Rossi, Romina and Scarpi, Emanuela and Ricci, Marianna and Pallotti, Maria Caterina and Dall'Agata, Monia and Montalti, Sandra and Maltoni, Marco},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jocn.16403},
	keywords = {early palliative care, nursing case manager, symptom control, telemedicine},
	pages = {2846--2853},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\P5JRZ7E2\\Valenti et al. - 2023 - Nurse-led telephone follow-up for early palliative.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\BQGCLAFW\\jocn.html:text/html},
}

@article{gill_implementation_2023,
	title = {Implementation of early palliative care in an oncologic outpatient clinic – an observational study of the first year},
	volume = {62},
	issn = {0284-186X, 1651-226X},
	url = {https://www.tandfonline.com/doi/full/10.1080/0284186X.2023.2212410},
	doi = {10.1080/0284186X.2023.2212410},
	language = {en},
	number = {5},
	urldate = {2024-04-30},
	journal = {Acta Oncologica},
	author = {Gill, Sabine Ute Alice and Hollegaard, Stine and Schønnemann, Katrine Rahbek},
	month = may,
	year = {2023},
	keywords = {palliative care},
	pages = {522--527},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\WDVBUBRZ\\Gill et al. - 2023 - Implementation of early palliative care in an onco.pdf:application/pdf},
}

@article{bandieri_early_2020,
	title = {Early versus delayed palliative/supportive care in advanced cancer: an observational study},
	volume = {10},
	copyright = {© Author(s) (or their employer(s)) 2020. No commercial re-use. See rights and permissions. Published by BMJ.},
	issn = {2045-435X, 2045-4368},
	shorttitle = {Early versus delayed palliative/supportive care in advanced cancer},
	url = {https://spcare.bmj.com/content/10/4/e32},
	doi = {10.1136/bmjspcare-2019-001794},
	abstract = {Objective The positive impact of early palliative care interventions in advanced cancer patients has so far been largely evaluated in randomised controlled trials. This study aimed at providing information on the value of early palliative/supportive care, integrated with standard oncologic care, in a real-life setting.
Methods This was a retrospective observational study of 292 advanced cancer patients consecutively admitted at Carpi Hospital in Modena, Italy, between 2014 and 2017. For the purpose of this analysis, patients were classified into two groups (early and delayed palliative/supportive care patients), and analysed for different clinical indicators. Early and delayed palliative/supportive care were classified according to the time elapsed from advanced cancer diagnosis until palliative/supportive care start.
Results A total of 200 patients (68\%), with at least three visits, were included in the analyses. The frequency of chemotherapy use in the last 60 days of life was 3.4\% and 24.6\% in the early and delayed groups, respectively (adjusted OR=0.1; 95\% CI 0.0 to 0.4; p=0.002). The estimated survival probability at 1 year was 74.5\% (95\% CI 65.0\% to 85.4\%) and 45.5\% (95\% CI 37.6\% to 55.0\%), in the early and delayed groups, respectively. Performance status, pain and all the Edmonton Symptom Assessment Scale items, assessed at baseline and at 1 to 12 weeks after the intervention, showed significant improvement over time. However, no between-group differences were found with regard to symptom outcomes.
Conclusions An earlier palliative/supportive care intervention was associated with reduced aggressiveness of therapy, in patients receiving community oncology care. Symptom burden was improved by early palliative/supportive care, independently of the timing of patient referral.},
	language = {en},
	number = {4},
	urldate = {2024-04-30},
	journal = {BMJ Supportive \& Palliative Care},
	author = {Bandieri, Elena and Banchelli, Federico and Artioli, Fabrizio and Mucciarini, Claudia and Razzini, Giorgia and Cruciani, Massimiliano and Potenza, Leonardo and D'Amico, Roberto and Efficace, Fabio and Bruera, Eduardo and Luppi, Mario},
	month = dec,
	year = {2020},
	pmid = {31201152},
	note = {Publisher: British Medical Journal Publishing Group
Section: Original research},
	keywords = {palliative care},
	pages = {e32--e32},
}

@article{temel_patient-centered_2022,
	title = {Patient-{Centered} {Palliative} {Care} for {Patients} {With} {Advanced} {Lung} {Cancer}},
	volume = {40},
	issn = {0732-183X},
	url = {https://ascopubs.org/doi/abs/10.1200/JCO.21.01710},
	doi = {10.1200/JCO.21.01710},
	abstract = {The evidence base demonstrating the benefits of an early focus on palliative care for patients with serious cancers, including advanced lung cancer, is substantial. Early involvement of specialty-trained palliative care clinicians in the care of patients with advanced lung cancer improves patient-reported outcomes, such as quality of life, and health care delivery, including hospice utilization. Since the time that many of these palliative care trials were conducted, the paradigm of cancer care for many cancers, including lung cancer, has changed dramatically. The majority of patients with advanced lung cancer are now treated with immune checkpoint inhibitors or targeted therapies, both of which have had a significant impact on patient's experience and outcomes. With this changing landscape of lung cancer therapeutics, patients are facing new and different challenges, including dealing with novel side effect profiles and coping with greater uncertainty regarding their prognosis. Patients who are living longer with their advanced cancer also struggle with how to address survivorship issues, such as sexual health and exercise, and decision making about end-of-life care. Although palliative care clinicians remain well-suited to address these care needs, they may need to learn new skills to support patients treated with novel therapies. Additionally, as the experience of patients with advanced lung cancer is becoming more varied and individualized, palliative care research interventions and clinical programs should also be delivered in a patient-centered manner to best meet patient's needs and improve their outcomes. Tailored and technology-based palliative care interventions are promising strategies for delivering patient-centered palliative care.},
	number = {6},
	urldate = {2024-04-29},
	journal = {JCO},
	author = {Temel, Jennifer S. and Petrillo, Laura A. and Greer, Joseph A.},
	month = feb,
	year = {2022},
	note = {Publisher: Wolters Kluwer},
	keywords = {palliative care},
	pages = {626--634},
}

@article{temel_effects_2017,
	title = {Effects of {Early} {Integrated} {Palliative} {Care} in {Patients} {With} {Lung} and {GI} {Cancer}: {A} {Randomized} {Clinical} {Trial}},
	volume = {35},
	issn = {0732-183X},
	shorttitle = {Effects of {Early} {Integrated} {Palliative} {Care} in {Patients} {With} {Lung} and {GI} {Cancer}},
	url = {https://ascopubs.org/doi/10.1200/JCO.2016.70.5046},
	doi = {10.1200/JCO.2016.70.5046},
	abstract = {Purpose
We evaluated the impact of early integrated palliative care (PC) in patients with newly diagnosed lung and GI cancer.
Patients and Methods
We randomly assigned patients with newly diagnosed incurable lung or noncolorectal GI cancer to receive either early integrated PC and oncology care (n = 175) or usual care (n = 175) between May 2011 and July 2015. Patients who were assigned to the intervention met with a PC clinician at least once per month until death, whereas those who received usual care consulted a PC clinician upon request. The primary end point was change in quality of life (QOL) from baseline to week 12, per scoring by the Functional Assessment of Cancer Therapy-General scale. Secondary end points included change in QOL from baseline to week 24, change in depression per the Patient Health Questionnaire-9, and differences in end-of-life communication.
Results
Intervention patients (v usual care) reported greater improvement in QOL from baseline to week 24 (1.59 v −3.40; P = .010) but not week 12 (0.39 v −1.13; P = .339). Intervention patients also reported lower depression at week 24, controlling for baseline scores (adjusted mean difference, −1.17; 95\% CI, −2.33 to −0.01; P = .048). Intervention effects varied by cancer type, such that intervention patients with lung cancer reported improvements in QOL and depression at 12 and 24 weeks, whereas usual care patients with lung cancer reported deterioration. Patients with GI cancers in both study groups reported improvements in QOL and mood by week 12. Intervention patients versus usual care patients were more likely to discuss their wishes with their oncologist if they were dying (30.2\% v 14.5\%; P = .004).
Conclusion
For patients with newly diagnosed incurable cancers, early integrated PC improved QOL and other salient outcomes, with differential effects by cancer type. Early integrated PC may be most effective if targeted to the specific needs of each patient population.},
	number = {8},
	urldate = {2024-04-29},
	journal = {JCO},
	author = {Temel, Jennifer S. and Greer, Joseph A. and El-Jawahri, Areej and Pirl, William F. and Park, Elyse R. and Jackson, Vicki A. and Back, Anthony L. and Kamdar, Mihir and Jacobsen, Juliet and Chittenden, Eva H. and Rinaldi, Simone P. and Gallagher, Emily R. and Eusebio, Justin R. and Li, Zhigang and Muzikansky, Alona and Ryan, David P.},
	month = mar,
	year = {2017},
	note = {Publisher: Wolters Kluwer},
	keywords = {palliative care},
	pages = {834--841},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\4RPHQ4VN\\Temel et al. - 2017 - Effects of Early Integrated Palliative Care in Pat.pdf:application/pdf},
}

@article{melamed_association_2021,
	title = {Association {Between} {Overall} {Survival} and the {Tendency} for {Cancer} {Programs} to {Administer} {Neoadjuvant} {Chemotherapy} for {Patients} {With} {Advanced} {Ovarian} {Cancer}},
	volume = {7},
	issn = {2374-2437},
	url = {https://doi.org/10.1001/jamaoncol.2021.4252},
	doi = {10.1001/jamaoncol.2021.4252},
	abstract = {Randomized clinical trials have found that, in patients with advanced-stage epithelial ovarian cancer, neoadjuvant chemotherapy has similar long-term survival and improved perioperative outcomes compared with primary cytoreductive surgery. Despite this, considerable controversy remains about the appropriate use of neoadjuvant chemotherapy, and the proportion of patients who receive this treatment varies considerably among cancer programs in the US.To evaluate the association between high levels of neoadjuvant chemotherapy administration and overall survival in patients with advanced ovarian cancer.This difference-in-differences comparative effectiveness analysis leveraged differential adoption of neoadjuvant chemotherapy in Commission on Cancer–accredited cancer programs in the US and included women with a diagnosis of stage IIIC and IV epithelial ovarian cancer between January 2004 and December 2015 who were followed up through the end of 2018. The data were analyzed between September 2020 and January 2021.Treatment in a cancer program with high levels of neoadjuvant chemotherapy administration (more often than expected based on case mix) or in a program that continued to restrict its use after the 2010 publication of a clinical trial demonstrating the noninferiority of neoadjuvant chemotherapy compared with primary surgery for the treatment of patients with advanced ovarian cancer.Case mix–standardized median overall survival time and 1-year all-cause mortality assessed with a flexible parametric survival model.We identified 19 562 patients (mean [SD] age, 63.9 [12.6] years; 3.2\% Asian, 8.0\% Black, 4.8\% Hispanic, 82.5\% White individuals) who were treated in 332 cancer programs that increased use of neoadjuvant chemotherapy from 21.7\% in 2004 to 2009 to 42.2\% in 2010 to 2015 and 19 737 patients (mean [SD] age, 63.5 [12.6] years; 3.1\% Asian, 7.7\% Black, 6.5\% Hispanic, 81.8\% White individuals) who were treated in 332 programs that marginally increased use of neoadjuvant chemotherapy (20.1\% to 22.5\%) over these periods. The standardized median overall survival times improved by similar magnitudes in programs with high (from 31.6 [IQR, 12.3-70.1] to 37.9 [IQR, 17.0-84.9] months; 6.3-month difference; 95\% CI, 4.2-8.3) and low (from 31.4 [IQR, 12.1-67.2] to 36.8 [IQR, 15.0-80.3] months; 5.4-month difference, 95\% CI, 3.5-7.3) use of neoadjuvant chemotherapy after 2010 (difference-in-differences, 0.9 months; 95\% CI, −1.9 to 3.7). One-year mortality declined more in programs with high (from 25.6\% to 19.3\%; risk difference, −5.2\%; 95\% CI, −6.4 to −4.1) than with low (from 24.9\% to 21.8\%; risk difference, −3.2\%, 95\% CI, −4.3 to −2.0) use of neoadjuvant chemotherapy (difference-in-differences, −2.1\%; 95\% CI, −3.7 to −0.5).In this comparative effectiveness research study, compared with cancer programs with low use of neoadjuvant chemotherapy, those with high use had similar improvements in median overall survival and larger declines in short-term mortality.},
	number = {12},
	urldate = {2024-04-24},
	journal = {JAMA Oncology},
	author = {Melamed, Alexander and Rauh-Hain, J. Alejandro and Gockley, Allison A. and Nitecki, Roni and Ramirez, Pedro T. and Hershman, Dawn L. and Keating, Nancy and Wright, Jason D.},
	month = dec,
	year = {2021},
	keywords = {DiD},
	pages = {1782--1790},
	file = {Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\AV4QHRWV\\2784404.html:text/html},
}

@article{akinyemi_causal_2024,
	title = {Causal {Effects} of the {Affordable} {Care} {Act} ({ACA}) {Implementation} on {Non}-{Hodgkin}'s {Lymphoma} {Survival}: {A} {Difference}-in-{Differences} {Analysis}},
	issn = {2168-8184},
	shorttitle = {Causal {Effects} of the {Affordable} {Care} {Act} ({ACA}) {Implementation} on {Non}-{Hodgkin}'s {Lymphoma} {Survival}},
	url = {https://www.cureus.com/articles/213270-causal-effects-of-the-affordable-care-act-aca-implementation-on-non-hodgkins-lymphoma-survival-a-difference-in-differences-analysis},
	doi = {10.7759/cureus.52571},
	abstract = {Introduction: Non-Hodgkin's Lymphoma (NHL) accounts for a substantial number of cancer cases in the United States, with a significant prevalence and mortality rate. The implementation of the Affordable Care Act (ACA) has the potential to impact cancer-specific survival among NHL patients by improving access to healthcare services and treatments.
Objective: This study aims to assess the impact of the implementation of the ACA on cancer-specific survival among patients diagnosed with NHL.
Methodology: In this retrospective analysis, we leveraged data from the Surveillance, Epidemiology, and End
Results (SEER) registry to assess the impact of the ACA on cancer-specific survival among NHL patients. The study covered the years 2000-2020, divided into pre-ACA (2000-2013) and post-ACA (2017-2020) periods, with a three-year washout (2014-2016). Using a Difference-in-Differences approach, we compared Georgia (a non-expansion state) to New Jersey (an expansion state since 2014). We adjusted for patient demographics, income, metropolitan status, disease stage, and treatment modalities. Results: Among 74,762 patients, 56.2\% were in New Jersey (42,005), while 43.8\% were in Georgia (32,757). The pre-ACA period included 32,851 patients (51.7\% in Georgia and 56.7\% in New Jersey), and 27,447 patients were in the post-ACA period (48.3\% in Georgia and 43.4\% in New Jersey). The post-ACA period exhibited a 34\% survival improvement (OR=0.66, 95\% CI 0.58-0.75). ACA implementation was associated with a 16\% survival boost among NHL patients in New Jersey (OR=0.84, 95\% CI 0.74-0.95). Other factors linked to improved survival included surgery (OR=0.86, 95\% CI 0.81-0.91), radiotherapy (OR=0.77, 95\% CI 0.72-0.82), and married status (OR=0.67, 95\% CI 0.64-0.71).
Conclusion: The study underscores the ACA's potential positive impact on cancer-specific survival among NHL patients, emphasizing the importance of healthcare policy interventions in improving patient outcomes.},
	language = {en},
	urldate = {2024-04-24},
	journal = {Cureus},
	author = {Akinyemi, Oluwasegun A and Weldeslase, Terhas Asfiha and Fasokun, Mojisola E and Odusanya, Eunice and Mejulu, Eunice O and Salihu, Ejura Y and Akueme, Ngozi T and Hughes, Kakra and Micheal, Miriam},
	month = jan,
	year = {2024},
	keywords = {causal inference, survival analysis, DiD},
	file = {Akinyemi et al. - 2024 - Causal Effects of the Affordable Care Act (ACA) Im.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\KDHVTD8U\\Akinyemi et al. - 2024 - Causal Effects of the Affordable Care Act (ACA) Im.pdf:application/pdf},
}

@article{temel_early_2010,
	title = {Early {Palliative} {Care} for {Patients} with {Metastatic} {Non}–{Small}-{Cell} {Lung} {Cancer}},
	volume = {363},
	issn = {0028-4793, 1533-4406},
	url = {http://www.nejm.org/doi/abs/10.1056/NEJMoa1000678},
	doi = {10.1056/NEJMoa1000678},
	language = {en},
	number = {8},
	urldate = {2024-04-24},
	journal = {N Engl J Med},
	author = {Temel, Jennifer S. and Greer, Joseph A. and Muzikansky, Alona and Gallagher, Emily R. and Admane, Sonal and Jackson, Vicki A. and Dahlin, Constance M. and Blinderman, Craig D. and Jacobsen, Juliet and Pirl, William F. and Billings, J. Andrew and Lynch, Thomas J.},
	month = aug,
	year = {2010},
	keywords = {palliative care},
	pages = {733--742},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\YRW9XX5V\\Temel et al. - 2010 - Early Palliative Care for Patients with Metastatic.pdf:application/pdf},
}

@article{ferrell_palliative_2011,
	title = {Palliative {Care} in {Lung} {Cancer}},
	volume = {91},
	issn = {0039-6109},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3655433/},
	doi = {10.1016/j.suc.2010.12.003},
	number = {2},
	urldate = {2024-04-24},
	journal = {Surg Clin North Am},
	author = {Ferrell, Betty and Koczywas, Marianna and Grannis, Fred and Harrington, Annie},
	month = apr,
	year = {2011},
	pmid = {21419260},
	pmcid = {PMC3655433},
	keywords = {palliative care},
	pages = {403--ix},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\LSDA3YRV\\Ferrell et al. - 2011 - Palliative Care in Lung Cancer.pdf:application/pdf},
}

@article{arch_randomized_2022,
	title = {A randomized controlled trial of a multi-modal palliative care intervention to promote advance care planning and psychological well-being among adults with advanced cancer: study protocol},
	volume = {21},
	issn = {1472-684X},
	shorttitle = {A randomized controlled trial of a multi-modal palliative care intervention to promote advance care planning and psychological well-being among adults with advanced cancer},
	url = {https://doi.org/10.1186/s12904-022-01087-z},
	doi = {10.1186/s12904-022-01087-z},
	abstract = {Up to half of adults with advanced cancer report anxiety or depression symptoms, which can cause avoidance of future planning. We present a study protocol for an innovative, remotely-delivered, acceptance-based, multi-modal palliative care intervention that addresses advance care planning (ACP) and unmet psychological needs commonly experienced by adults with metastatic cancer.},
	number = {1},
	urldate = {2024-04-24},
	journal = {BMC Palliative Care},
	author = {Arch, Joanna J. and Mitchell, Jill L. and Schmiege, Sarah J. and Levin, Michael E. and Genung, Sarah R. and Nealis, Madeline S. and Fink, Regina M. and Bright, Emma E. and Andorsky, David J. and Kutner, Jean S.},
	month = nov,
	year = {2022},
	keywords = {palliative care},
	pages = {198},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\E44UWND7\\Arch et al. - 2022 - A randomized controlled trial of a multi-modal pal.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\GJC6J6DL\\s12904-022-01087-z.html:text/html},
}

@article{anhoj_run_2014,
	title = {Run {Charts} {Revisited}: {A} {Simulation} {Study} of {Run} {Chart} {Rules} for {Detection} of {Non}-{Random} {Variation} in {Health} {Care} {Processes}},
	volume = {9},
	issn = {1932-6203},
	shorttitle = {Run {Charts} {Revisited}},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0113825},
	doi = {10.1371/journal.pone.0113825},
	abstract = {Background A run chart is a line graph of a measure plotted over time with the median as a horizontal line. The main purpose of the run chart is to identify process improvement or degradation, which may be detected by statistical tests for non-random patterns in the data sequence. Methods We studied the sensitivity to shifts and linear drifts in simulated processes using the shift, crossings and trend rules for detecting non-random variation in run charts. Results The shift and crossings rules are effective in detecting shifts and drifts in process centre over time while keeping the false signal rate constant around 5\% and independent of the number of data points in the chart. The trend rule is virtually useless for detection of linear drift over time, the purpose it was intended for.},
	language = {en},
	number = {11},
	urldate = {2024-03-14},
	journal = {PLOS ONE},
	author = {Anhøj, Jacob and Olesen, Anne Vingaard},
	month = nov,
	year = {2014},
	note = {Publisher: Public Library of Science},
	keywords = {quality, run charts},
	pages = {e113825},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\FYSGC9WH\\Anhøj and Olesen - 2014 - Run Charts Revisited A Simulation Study of Run Ch.pdf:application/pdf},
}

@article{anhoj_diagnostic_2015,
	title = {Diagnostic {Value} of {Run} {Chart} {Analysis}: {Using} {Likelihood} {Ratios} to {Compare} {Run} {Chart} {Rules} on {Simulated} {Data} {Series}},
	volume = {10},
	issn = {1932-6203},
	shorttitle = {Diagnostic {Value} of {Run} {Chart} {Analysis}},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0121349},
	doi = {10.1371/journal.pone.0121349},
	abstract = {Run charts are widely used in healthcare improvement, but there is little consensus on how to interpret them. The primary aim of this study was to evaluate and compare the diagnostic properties of different sets of run chart rules. A run chart is a line graph of a quality measure over time. The main purpose of the run chart is to detect process improvement or process degradation, which will turn up as non-random patterns in the distribution of data points around the median. Non-random variation may be identified by simple statistical tests including the presence of unusually long runs of data points on one side of the median or if the graph crosses the median unusually few times. However, there is no general agreement on what defines “unusually long” or “unusually few”. Other tests of questionable value are frequently used as well. Three sets of run chart rules (Anhoej, Perla, and Carey rules) have been published in peer reviewed healthcare journals, but these sets differ significantly in their sensitivity and specificity to non-random variation. In this study I investigate the diagnostic values expressed by likelihood ratios of three sets of run chart rules for detection of shifts in process performance using random data series. The study concludes that the Anhoej rules have good diagnostic properties and are superior to the Perla and the Carey rules.},
	language = {en},
	number = {3},
	urldate = {2024-03-14},
	journal = {PLOS ONE},
	author = {Anhøj, Jacob},
	month = mar,
	year = {2015},
	note = {Publisher: Public Library of Science},
	keywords = {quality, run charts},
	pages = {e0121349},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\VEMRY7JM\\Anhøj - 2015 - Diagnostic Value of Run Chart Analysis Using Like.pdf:application/pdf},
}

@article{chen_best_2022,
	title = {Best {Practice} {Guidelines} for {Propensity} {Score} {Methods} in {Medical} {Research}: {Consideration} on {Theory}, {Implementation}, and {Reporting}. {A} {Review}},
	volume = {38},
	issn = {07498063},
	shorttitle = {Best {Practice} {Guidelines} for {Propensity} {Score} {Methods} in {Medical} {Research}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0749806321008203},
	doi = {10.1016/j.arthro.2021.06.037},
	abstract = {Rigorous and reproducible methodology of controlling for bias is essential for high-quality, evidence-based studies. Propensity score matching (PSM) is a valuable way to control for bias and achieve pseudo-randomization in retrospective observation studies. The purpose of this review is to 1) provide a clear conceptual framework for PSM, 2) recommend how to best report its use in studies, and 3) offer some practical examples of implementation. First, this article covers the concepts behind PSM, discusses its pros and cons, and compares it with other methods of controlling for bias, namely, hard/exact matching and regression analysis. Second, recommendations are given for what to report in a manuscript when PSM is used. Finally, a worked example is provided, which can also serve as a template for the reader’s own studies. A study’s conclusions are only as strong as its methods. PSM is an invaluable tool for producing rigorous and reproducible results in observational studies. The goal of this article is to give practicing clinical physicians not only a better understanding of PSM and its implications but the ability to implement it for their own studies. Study Design: Review.},
	language = {en},
	number = {2},
	urldate = {2024-04-23},
	journal = {Arthroscopy: The Journal of Arthroscopic \& Related Surgery},
	author = {Chen, Jeffrey W. and Maldonado, David R. and Kowalski, Brooke L. and Miecznikowski, Kara B. and Kyin, Cynthia and Gornbein, Jeffrey A. and Domb, Benjamin G.},
	month = feb,
	year = {2022},
	keywords = {causal inference, propensity score},
	pages = {632--642},
	file = {Chen et al. - 2022 - Best Practice Guidelines for Propensity Score Meth.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\FNB6B4YC\\Chen et al. - 2022 - Best Practice Guidelines for Propensity Score Meth.pdf:application/pdf},
}

@article{tchetgen_tchetgen_universal_2024,
	title = {Universal {Difference}-in-{Differences} for {Causal} {Inference} in {Epidemiology}},
	volume = {35},
	issn = {1044-3983},
	url = {https://journals.lww.com/epidem/fulltext/2024/01000/universal_difference_in_differences_for_causal.3.aspx},
	doi = {10.1097/EDE.0000000000001676},
	abstract = {Difference-in-differences is undoubtedly one of the most widely used methods for evaluating the causal effect of an intervention in observational (i.e., nonrandomized) settings. The approach is typically used when pre- and postexposure outcome measurements are available, and one can reasonably assume that the association of the unobserved confounder with the outcome has the same absolute magnitude in the two exposure arms and is constant over time; a so-called parallel trends assumption. The parallel trends assumption may not be credible in many practical settings, for example, if the outcome is binary, a count, or polytomous, as well as when an uncontrolled confounder exhibits nonadditive effects on the distribution of the outcome, even if such effects are constant over time. We introduce an alternative approach that replaces the parallel trends assumption with an odds ratio equi-confounding assumption under which an association between treatment and the potential outcome under no treatment is identified with a well-specified generalized linear model relating the pre-exposure outcome and the exposure. Because the proposed method identifies any causal effect that is conceivably identified in the absence of confounding bias, including nonlinear effects such as quantile treatment effects, the approach is aptly called universal difference-in-differences. We describe and illustrate both fully parametric and more robust semiparametric universal difference-in-differences estimators in a real-world application concerning the causal effects of a Zika virus outbreak on birth rate in Brazil.
          A supplementary digital video is available at: https://links.lww.com/EDE/C90},
	language = {en-US},
	number = {1},
	urldate = {2024-04-22},
	journal = {Epidemiology},
	author = {Tchetgen Tchetgen, Eric J. and Park, Chan and Richardson, David B.},
	month = jan,
	year = {2024},
	keywords = {causal inference, DiD},
	pages = {16},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\MRRH8RMP\\Tchetgen Tchetgen et al. - 2024 - Universal Difference-in-Differences for Causal Inf.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\ZX3W2LRV\\universal_difference_in_differences_for_causal.3.html:text/html},
}

@article{qiu_multivariate_nodate,
	title = {Multivariate {Bayesian} {Structural} {Time} {Series} {Model}},
	abstract = {This paper deals with inference and prediction for multiple correlated time series, where one also has the choice of using a candidate pool of contemporaneous predictors for each target series. Starting with a structural model for time series, we use Bayesian tools for model ﬁtting, prediction and feature selection, thus extending some recent works along these lines for the univariate case. The Bayesian paradigm in this multivariate setting helps the model avoid overﬁtting, as well as captures correlations among multiple target time series with various state components. The model provides needed ﬂexibility in selecting a diﬀerent set of components and available predictors for each target series. The cyclical component in the model can handle large variations in the short term, which may be caused by external shocks. Extensive simulations were run to investigate properties such as estimation accuracy and performance in forecasting. This was followed by an empirical study with one-step-ahead prediction on the max log return of a portfolio of stocks that involve four leading ﬁnancial institutions. Both the simulation studies and the extensive empirical study conﬁrm that this multivariate model outperforms three other benchmark models, viz. a model that treats each target series as independent, the autoregressive integrated moving average model with regression (ARIMAX), and the multivariate ARIMAX (MARIMAX) model.},
	language = {en},
	author = {Qiu, Jinwen and Jammalamadaka, S Rao and Ning, Ning},
	keywords = {causal inference, Bayesian, synthetic controls},
	file = {Qiu et al. - Multivariate Bayesian Structural Time Series Model.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\8GSVTZX9\\Qiu et al. - Multivariate Bayesian Structural Time Series Model.pdf:application/pdf},
}

@article{dagostino_mcgowan_why_2024,
	title = {The “{Why}” behind including “{Y}” in your imputation model},
	issn = {0962-2802},
	url = {https://doi.org/10.1177/09622802241244608},
	doi = {10.1177/09622802241244608},
	abstract = {Missing data is a common challenge when analyzing epidemiological data, and imputation is often used to address this issue. Here, we investigate the scenario where a covariate used in an analysis has missingness and will be imputed. There are recommendations to include the outcome from the analysis model in the imputation model for missing covariates, but it is not necessarily clear if this recommendation always holds and why this is sometimes true. We examine deterministic imputation (i.e. single imputation with fixed values) and stochastic imputation (i.e. single or multiple imputation with random values) methods and their implications for estimating the relationship between the imputed covariate and the outcome. We mathematically demonstrate that including the outcome variable in imputation models is not just a recommendation but a requirement to achieve unbiased results when using stochastic imputation methods. Moreover, we dispel common misconceptions about deterministic imputation models and demonstrate why the outcome should not be included in these models. This article aims to bridge the gap between imputation in theory and in practice, providing mathematical derivations to explain common statistical recommendations. We offer a better understanding of the considerations involved in imputing missing covariates and emphasize when it is necessary to include the outcome variable in the imputation model.},
	language = {en},
	urldate = {2024-04-19},
	journal = {Stat Methods Med Res},
	author = {D’Agostino McGowan, Lucy and Lotspeich, Sarah C and Hepler, Staci A},
	month = apr,
	year = {2024},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {multiple imputation},
	pages = {09622802241244608},
	file = {Submitted Version:C\:\\Users\\anbe6\\Zotero\\storage\\Z4TEENAT\\D’Agostino McGowan et al. - 2024 - The “Why” behind including “Y” in your imputation .pdf:application/pdf},
}

@misc{chen_flexible_2024,
	title = {A flexible {Bayesian} g-formula for causal survival analyses with time-dependent confounding},
	url = {http://arxiv.org/abs/2402.02306},
	abstract = {In longitudinal observational studies with a time-to-event outcome, a common objective in causal analysis is to estimate the causal survival curve under hypothetical intervention scenarios within the study cohort. The g-formula is a particularly useful tool for this analysis. To enhance the traditional parametric g-formula approach, we developed a more adaptable Bayesian g-formula estimator. This estimator facilitates both longitudinal predictive and causal inference. It incorporates Bayesian additive regression trees in the modeling of the time-evolving generative components, aiming to mitigate bias due to model misspecification. Specifically, we introduce a more general class of g-formulas for discrete survival data. These formulas can incorporate the longitudinal balancing scores, which serve as an effective method for dimension reduction and are vital when dealing with an expanding array of time-varying confounders. The minimum sufficient formulation of these longitudinal balancing scores is linked to the nature of treatment regimes, whether static or dynamic. For each type of treatment regime, we provide posterior sampling algorithms, which are grounded in the Bayesian additive regression trees framework. We have conducted simulation studies to illustrate the empirical performance of our proposed Bayesian g-formula estimators, and to compare them with existing parametric estimators. We further demonstrate the practical utility of our methods in real-world scenarios using data from the Yale New Haven Health System's electronic health records.},
	urldate = {2024-04-19},
	publisher = {arXiv},
	author = {Chen, Xinyuan and Hu, Liangyuan and Li, Fan},
	month = feb,
	year = {2024},
	note = {arXiv:2402.02306 [stat]},
	keywords = {causal inference, Bayesian, g-computation},
	file = {arXiv.org Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\A9NPKECW\\2402.html:text/html;Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\2SXXZ983\\Chen et al. - 2024 - A flexible Bayesian g-formula for causal survival .pdf:application/pdf},
}

@article{yang_changes_2023,
	title = {Changes in {Cardiovascular} {Disease} {Burden} in {China} after {Release} of the 2011 {Chinese} {Guidelines} for {Cardiovascular} {Disease} {Prevention}: {A} {Bayesian} {Causal} {Impact} {Analysis}},
	volume = {8},
	issn = {2009-8618},
	shorttitle = {Changes in {Cardiovascular} {Disease} {Burden} in {China} after {Release} of the 2011 {Chinese} {Guidelines} for {Cardiovascular} {Disease} {Prevention}},
	url = {https://www.scienceopen.com/hosted-document?doi=10.15212/CVIA.2023.0069},
	doi = {10.15212/CVIA.2023.0069},
	abstract = {{\textless}p xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" class="first" dir="auto" id="d241309e162"{\textgreater} \textbf{Objective:} This study aimed to investigate the effects of the 2011 Chinese Society of Cardiology guidelines (2011 CSC guidelines) on the overall and subtype specific cardiovascular disease (CVD) burden in China. {\textless}/p{\textgreater}{\textless}p xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" dir="auto" id="d241309e167"{\textgreater} \textbf{Methods:} We conducted a Bayesian causal impact analysis to investigate changes in the burden of CVD overall and 13 subcategories, before and after release of the 2011 CSC guidelines, by using publicly available data during 1990–2019. {\textless}/p{\textgreater}{\textless}p xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" dir="auto" id="d241309e172"{\textgreater} \textbf{Results:} The 2011 CSC guidelines were associated with moderate declines in CVD mortality (5.7\%; equivalent to 161 per 100,000) and DALYs (2.9\%; 1429 per 100,000), but small increases in incidence and prevalence, with an approximately 1-year lagged effect. Similar impact patterns were observed for ischemic stroke, cardiomyopathy and myocarditis, and aortic aneurysm. Release of the 2011 CSC guidelines increased intracerebral hemorrhage incidence, but sharply decreased rheumatic, ischemic, and non-rheumatic valvular heart disease mortality and DALY rates. The burden of other CVD subcategories was unchanged. Health worker numbers, population size, disposable income, hospital admission rates, and crude death rates were critical contributors to CVD burden beyond the 2011 CSC guidelines. {\textless}/p{\textgreater}{\textless}p xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" dir="auto" id="d241309e177"{\textgreater} \textbf{Conclusion:} The 2011 CSC guidelines decreased the burden of CVD and several subcategories. However, efforts to enhance health promotion and strengthen healthcare remain urgently needed in China. {\textless}/p{\textgreater}},
	urldate = {2024-04-19},
	journal = {Cardiovascular Innovations and Applications},
	author = {Yang, Zhao E. and Kwok, Man Ki and Schooling, Catherine Mary and Liu, Jing},
	month = nov,
	year = {2023},
	note = {Publisher: Compuscript},
	keywords = {causal inference, Bayesian, synthetic controls},
	pages = {945},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\TR8DLSE9\\Yang et al. - 2023 - Changes in Cardiovascular Disease Burden in China .pdf:application/pdf},
}

@article{menchetti_combining_2023,
	title = {Combining counterfactual outcomes and {ARIMA} models for policy evaluation},
	volume = {26},
	copyright = {https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model},
	issn = {1368-4221, 1368-423X},
	url = {https://academic.oup.com/ectj/article/26/1/1/6713620},
	doi = {10.1093/ectj/utac024},
	abstract = {The Rubin Causal Model (RCM) is a framework that allows to deﬁne the causal effect of an intervention as a contrast of potential outcomes. In recent years, several methods have been developed under the RCM to estimate causal effects in time series settings. None of these makes use of autoregressive integrated moving average (ARIMA) models, which are instead very common in the econometrics literature. In this paper, we propose a novel approach, named Causal-ARIMA (C-ARIMA), to deﬁne and estimate the causal effect of an intervention in observational time series settings under the RCM. We ﬁrst formalise the assumptions enabling the deﬁnition, the estimation and the attribution of the effect to the intervention. We then check the validity of the proposed method with a simulation study. In the empirical application, we use C-ARIMA to assess the causal effect of a permanent price reduction on supermarket sales. The CausalArima R package provides an implementation of the proposed approach.},
	language = {en},
	number = {1},
	urldate = {2024-04-19},
	journal = {The Econometrics Journal},
	author = {Menchetti, Fiammetta and Cipollini, Fabrizio and Mealli, Fabrizia},
	month = jan,
	year = {2023},
	keywords = {R, causal inference, synthetic controls},
	pages = {1--24},
	file = {Menchetti et al. - 2023 - Combining counterfactual outcomes and ARIMA models.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\8FKMDSD9\\Menchetti et al. - 2023 - Combining counterfactual outcomes and ARIMA models.pdf:application/pdf},
}

@article{abadie_synth_2011,
	title = {\textbf{{Synth}} : {An} \textit{{R}} {Package} for {Synthetic} {Control} {Methods} in {Comparative} {Case} {Studies}},
	volume = {42},
	issn = {1548-7660},
	shorttitle = {\textbf{{Synth}}},
	url = {http://www.jstatsoft.org/v42/i13/},
	doi = {10.18637/jss.v042.i13},
	abstract = {The R package Synth implements synthetic control methods for comparative case studies designed to estimate the causal eﬀects of policy interventions and other events of interest (Abadie and Gardeazabal 2003; Abadie, Diamond, and Hainmueller 2010). These techniques are particularly well-suited to investigate events occurring at an aggregate level (i.e., countries, cities, regions, etc.) and aﬀecting a relatively small number of units. Beneﬁts and features of the Synth package are illustrated using data from Abadie and Gardeazabal (2003), which examined the economic impact of the terrorist conﬂict in the Basque Country.},
	language = {en},
	number = {13},
	urldate = {2024-04-19},
	journal = {J. Stat. Soft.},
	author = {Abadie, Alberto and Diamond, Alexis and Hainmueller, Jens},
	year = {2011},
	keywords = {R, causal inference, synthetic controls},
	file = {Abadie et al. - 2011 - Synth  An R Package for Synthetic C.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\V29W6QC4\\Abadie et al. - 2011 - Synth  An R Package for Synthetic C.pdf:application/pdf},
}

@article{sun_effect_2024,
	title = {The {Effect} of {Nationwide} {Organized} {Cancer} {Screening} {Programs} on {Gastric} {Cancer} {Mortality}: {A} {Synthetic} {Control} {Study}},
	volume = {166},
	issn = {00165085},
	shorttitle = {The {Effect} of {Nationwide} {Organized} {Cancer} {Screening} {Programs} on {Gastric} {Cancer} {Mortality}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0016508523055622},
	doi = {10.1053/j.gastro.2023.11.286},
	language = {en},
	number = {3},
	urldate = {2024-04-19},
	journal = {Gastroenterology},
	author = {Sun, Dianqin and Mülder, Duco T. and Li, Yige and Nieboer, Daan and Park, Jin Young and Suh, Mina and Hamashima, Chisato and Han, Weiran and O’Mahony, James F. and Lansdorp-Vogelaar, Iris},
	month = mar,
	year = {2024},
	keywords = {causal inference, synthetic controls},
	pages = {503--514},
	file = {Sun et al. - 2024 - The Effect of Nationwide Organized Cancer Screenin.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\ZN3DSHWX\\Sun et al. - 2024 - The Effect of Nationwide Organized Cancer Screenin.pdf:application/pdf},
}

@article{jackson_long-term_2020,
	title = {Long-term impact of the expansion of a hospital liaison psychiatry service on patient care and costs following emergency department attendances for self-harm},
	volume = {6},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	issn = {2056-4724},
	url = {https://www.cambridge.org/core/product/identifier/S2056472420000186/type/journal_article},
	doi = {10.1192/bjo.2020.18},
	abstract = {Background
              In September 2014, as part of a national initiative to increase access to liaison psychiatry services, the liaison psychiatry services at Bristol Royal Infirmary received new investment of £250 000 per annum, expanding its availability from 40 to 98 h per week. The long-term impact on patient outcomes and costs, of patients presenting to the emergency department with self-harm, is unknown.


              Aims
              To assess the long-term impact of the investment on patient care outcomes and costs, of patients presenting to the emergency department with self-harm.


              Method
              Monthly data for all self-harm emergency department attendances between 1 September 2011 and 30 September 2017 was modelled using Bayesian structural time series to estimate expected outcomes in the absence of expanded operating hours (the counterfactual). The difference between the observed and expected trends for each outcome were interpreted as the effects of the investment.


              Results
              Over the 3 years after service expansion, the mean number of self-harm attendances increased 13\%. Median waiting time from arrival to psychosocial assessment was 2 h shorter (18.6\% decrease, 95\% Bayesian credible interval (BCI) −30.2\% to −2.8\%), there were 45 more referrals to other agencies (86.1\% increase, 95\% BCI 60.6\% to 110.9\%) and a small increase in the number of psychosocial assessments (11.7\% increase, 95\% BCI −3.4\% to 28.5\%) per month. Monthly mean net hospital costs were £34 more per episode (5.3\% increase, 95\% BCI −11.6\% to 25.5\%).


              Conclusions
              Despite annual increases in emergency department attendances, investment was associated with reduced waiting times for psychosocial assessment and more referrals to other agencies, with only a small increase in cost per episode.},
	language = {en},
	number = {3},
	urldate = {2024-04-19},
	journal = {BJPsych open},
	author = {Jackson, Joni and Nugawela, Manjula D. and De Vocht, Frank and Moran, Paul and Hollingworth, William and Knipe, Duleeka and Munien, Nik and Gunnell, David and Redaniel, Maria Theresa},
	month = may,
	year = {2020},
	keywords = {causal inference, Bayesian, synthetic controls},
	pages = {e34},
	file = {Jackson et al. - 2020 - Long-term impact of the expansion of a hospital li.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\IQR43AXD\\Jackson et al. - 2020 - Long-term impact of the expansion of a hospital li.pdf:application/pdf},
}

@article{papadogeorgou_evaluating_2023,
	title = {Evaluating federal policies using {Bayesian} time series models: estimating the causal impact of the hospital readmissions reduction program},
	volume = {23},
	issn = {1572-9400},
	shorttitle = {Evaluating federal policies using {Bayesian} time series models},
	url = {https://doi.org/10.1007/s10742-022-00294-8},
	doi = {10.1007/s10742-022-00294-8},
	abstract = {Researchers are often faced with evaluating the effect of a policy or program that was simultaneously initiated across an entire population of units at a single point in time, and its effects over the targeted population can manifest at any time period afterwards. In the presence of data measured over time, Bayesian time series models have been used to impute what would have happened after the policy was initiated, had the policy not taken place, in order to estimate causal effects. However, the considerations regarding the definition of the target estimands, the underlying assumptions, the plausibility of such assumptions, and the choice of an appropriate model have not been thoroughly investigated. In this paper, we establish useful estimands for the evaluation of large-scale policies. We discuss that imputation of missing potential outcomes relies on an assumption which, even though untestable, can be partially evaluated using observed data. We illustrate an approach to evaluate this key causal assumption and facilitate model elicitation based on data from the time interval before policy initiation and using classic statistical techniques. As an illustration, we study the Hospital Readmissions Reduction Program (HRRP), a US federal intervention aiming to improve health outcomes for patients with pneumonia, acute myocardial infraction, or congestive failure admitted to a hospital. We evaluate the effect of the HRRP on population mortality among the elderly across the US and in four geographic subregions, and at different time windows. We find that the HRRP increased mortality from pneumonia and acute myocardial infraction across at least one geographical region and time horizon, and is likely to have had a detrimental effect on public health.},
	language = {en},
	number = {4},
	urldate = {2024-04-19},
	journal = {Health Serv Outcomes Res Method},
	author = {Papadogeorgou, Georgia and Menchetti, Fiammetta and Choirat, Christine and Wasfy, Jason H. and Zigler, Corwin M. and Mealli, Fabrizia},
	month = oct,
	year = {2023},
	keywords = {causal inference, Bayesian, synthetic controls},
	pages = {433--451},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\BUXDWRA4\\Papadogeorgou et al. - 2023 - Evaluating federal policies using Bayesian time se.pdf:application/pdf},
}

@article{wen_parametric_2021,
	title = {Parametric g-formula implementations for causal survival analyses},
	volume = {77},
	issn = {0006-341X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9044498/},
	doi = {10.1111/biom.13321},
	abstract = {The g-formula can be used to estimate the survival curve under a sustained treatment strategy. Two available estimators of the g-formula are noniterative conditional expectation and iterative conditional expectation. We propose a version of the iterative conditional expectation estimator and describe its procedures for deterministic and random treatment strategies. Also, because little is known about the comparative performance of noniterative and iterative conditional expectation estimators, we explore their relative efficiency via simulation studies. Our simulations show that, in the absence of model misspecification and unmeasured confounding, our proposed iterative conditional expectation estimator and the noniterative conditional expectation estimator are similarly efficient, and that both are at least as efficient as the classical iterative conditional expectation estimator. We describe an application of both noniterative and iterative conditional expectation to answer “when to start” treatment questions using data from the HIV-CAUSAL Collaboration.},
	number = {2},
	urldate = {2024-04-18},
	journal = {Biometrics},
	author = {Wen, Lan and Young, Jessica G. and Robins, James M. and Hernán, Miguel A.},
	month = jun,
	year = {2021},
	pmid = {32588909},
	pmcid = {PMC9044498},
	keywords = {causal inference, g-computation},
	pages = {740--753},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\Q2CM9898\\Wen et al. - 2021 - Parametric g-formula implementations for causal su.pdf:application/pdf},
}

@article{mcgrath_gformula_2020,
	title = {{gfoRmula}: {An} {R} {Package} for {Estimating} the {Effects} of {Sustained} {Treatment} {Strategies} via the {Parametric} g-formula},
	volume = {1},
	issn = {2666-3899},
	shorttitle = {{gfoRmula}},
	url = {https://www.sciencedirect.com/science/article/pii/S2666389920300088},
	doi = {10.1016/j.patter.2020.100008},
	abstract = {Researchers are often interested in estimating the causal effects of sustained treatment strategies, i.e., of (hypothetical) interventions involving time-varying treatments. When using observational data, estimating those effects requires adjustment for confounding. However, conventional regression methods cannot appropriately adjust for confounding in the presence of treatment-confounder feedback. In contrast, estimators derived from Robins's g-formula may correctly adjust for confounding even if treatment-confounder feedback exists. The package gfoRmula implements in R one such estimator: the parametric g-formula. This estimator can be used to estimate the effects of binary or continuous time-varying treatments as well as contrasts defined by static or dynamic, deterministic, or random interventions, as well as interventions that depend on the natural value of treatment. The package accommodates survival outcomes as well as binary or continuous outcomes measured at the end of follow-up. This paper describes the gfoRmula package, along with motivating background, features, and examples.},
	number = {3},
	urldate = {2024-04-18},
	journal = {Patterns},
	author = {McGrath, Sean and Lin, Victoria and Zhang, Zilu and Petito, Lucia C. and Logan, Roger W. and Hernán, Miguel A. and Young, Jessica G.},
	month = jun,
	year = {2020},
	keywords = {R, causal inference, g-computation},
	pages = {100008},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\2ZRXPUF6\\McGrath et al. - 2020 - gfoRmula An R Package for Estimating the Effects .pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\6D4TKZ9J\\S2666389920300088.html:text/html},
}

@misc{rue_bayesian_2016,
	title = {Bayesian {Computing} with {INLA}: {A} {Review}},
	shorttitle = {Bayesian {Computing} with {INLA}},
	url = {http://arxiv.org/abs/1604.00860},
	abstract = {The key operation in Bayesian inference, is to compute high-dimensional integrals. An old approximate technique is the Laplace method or approximation, which dates back to Pierre- Simon Laplace (1774). This simple idea approximates the integrand with a second order Taylor expansion around the mode and computes the integral analytically. By developing a nested version of this classical idea, combined with modern numerical techniques for sparse matrices, we obtain the approach of Integrated Nested Laplace Approximations (INLA) to do approximate Bayesian inference for latent Gaussian models (LGMs). LGMs represent an important model-abstraction for Bayesian inference and include a large proportion of the statistical models used today. In this review, we will discuss the reasons for the success of the INLA-approach, the R-INLA package, why it is so accurate, why the approximations are very quick to compute and why LGMs make such a useful concept for Bayesian computing.},
	urldate = {2024-04-16},
	publisher = {arXiv},
	author = {Rue, Håvard and Riebler, Andrea and Sørbye, Sigrunn H. and Illian, Janine B. and Simpson, Daniel P. and Lindgren, Finn K.},
	month = sep,
	year = {2016},
	note = {arXiv:1604.00860 [stat]},
	keywords = {Bayesian, INLA},
	file = {arXiv.org Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\Y7LJFMWS\\1604.html:text/html},
}

@misc{liu_leave-group-out_2023,
	title = {Leave-group-out cross-validation for latent {Gaussian} models},
	url = {http://arxiv.org/abs/2210.04482},
	doi = {10.48550/arXiv.2210.04482},
	abstract = {Evaluating the predictive performance of a statistical model is commonly done using cross-validation. Although the leave-one-out method is frequently employed, its application is justified primarily for independent and identically distributed observations. However, this method tends to mimic interpolation rather than prediction when dealing with dependent observations. This paper proposes a modified cross-validation for dependent observations. This is achieved by excluding an automatically determined set of observations from the training set to mimic a more reasonable prediction scenario. Also, within the framework of latent Gaussian models, we illustrate a method to adjust the joint posterior for this modified cross-validation to avoid model refitting. This new approach is accessible in the R-INLA package (www.r-inla.org).},
	urldate = {2024-04-15},
	publisher = {arXiv},
	author = {Liu, Zhedong and Rue, Haavard},
	month = oct,
	year = {2023},
	note = {arXiv:2210.04482 [stat]},
	keywords = {Bayesian, INLA},
	file = {arXiv Fulltext PDF:C\:\\Users\\anbe6\\Zotero\\storage\\8APNWGRH\\Liu and Rue - 2023 - Leave-group-out cross-validation for latent Gaussi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\EEIBIPPG\\2210.html:text/html},
}

@misc{alvares_bayesian_2024,
	title = {Bayesian survival analysis with {INLA}},
	url = {http://arxiv.org/abs/2212.01900},
	abstract = {This tutorial shows how various Bayesian survival models can be fitted using the integrated nested Laplace approximation in a clear, legible, and comprehensible manner using the INLA and INLAjoint R-packages. Such models include accelerated failure time, proportional hazards, mixture cure, competing risks, multi-state, frailty, and joint models of longitudinal and survival data, originally presented in the article "Bayesian survival analysis with BUGS" (Alvares et al., 2021). In addition, we illustrate the implementation of a new joint model for a longitudinal semicontinuous marker, recurrent events, and a terminal event. Our proposal aims to provide the reader with syntax examples for implementing survival models using a fast and accurate approximate Bayesian inferential approach.},
	urldate = {2024-03-22},
	publisher = {arXiv},
	author = {Alvares, Danilo and van Niekerk, Janet and Krainski, Elias Teixeira and Rue, Håvard and Rustand, Denis},
	month = mar,
	year = {2024},
	note = {arXiv:2212.01900 [stat]},
	keywords = {Bayesian, INLA},
	file = {arXiv.org Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\U44PY4NZ\\2212.html:text/html;Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\JBWQ3CJN\\Alvares et al. - 2024 - Bayesian survival analysis with INLA.pdf:application/pdf},
}

@article{foley_coronary_2024,
	title = {Coronary sinus reducer for the treatment of refractory angina ({ORBITA}-{COSMIC}): a randomised, placebo-controlled trial},
	volume = {0},
	issn = {0140-6736, 1474-547X},
	shorttitle = {Coronary sinus reducer for the treatment of refractory angina ({ORBITA}-{COSMIC})},
	url = {https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(24)00256-3/fulltext},
	doi = {10.1016/S0140-6736(24)00256-3},
	language = {English},
	number = {0},
	urldate = {2024-04-12},
	journal = {The Lancet},
	author = {Foley, Michael J. and Rajkumar, Christopher A. and Ahmed-Jushuf, Fiyyaz and Simader, Florentina A. and Chotai, Shayna and Pathimagaraj, Rachel H. and Mohsin, Muhammad and Salih, Ahmed and Wang, Danqi and Dixit, Prithvi and Davies, John R. and Keeble, Tom R. and Cosgrove, Claudia and Spratt, James C. and O’Kane, Peter D. and Silva, Ranil De and Hill, Jonathan M. and Nijjer, Sukhjinder S. and Sen, Sayan and Petraco, Ricardo and Mikhail, Ghada W. and Khamis, Ramzi and Kotecha, Tushar and Harrell, Frank E. and Kellman, Peter and Francis, Darrel P. and Howard, James P. and Cole, Graham D. and Shun-Shin, Matthew J. and Al-Lamee, Rasha K.},
	month = apr,
	year = {2024},
	pmid = {38604209},
	note = {Publisher: Elsevier},
	keywords = {Bayesian, trial design},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\8QYFKFR4\\Foley et al. - 2024 - Coronary sinus reducer for the treatment of refrac.pdf:application/pdf},
}

@article{gelman_weakly_2008,
	title = {A weakly informative default prior distribution for logistic and other regression models},
	volume = {2},
	issn = {1932-6157},
	url = {http://arxiv.org/abs/0901.4011},
	doi = {10.1214/08-AOAS191},
	abstract = {We propose a new prior distribution for classical (nonhierarchical) logistic regression models, constructed by first scaling all nonbinary variables to have mean 0 and standard deviation 0.5, and then placing independent Student-\$t\$ prior distributions on the coefficients. As a default choice, we recommend the Cauchy distribution with center 0 and scale 2.5, which in the simplest setting is a longer-tailed version of the distribution attained by assuming one-half additional success and one-half additional failure in a logistic regression. Cross-validation on a corpus of datasets shows the Cauchy class of prior distributions to outperform existing implementations of Gaussian and Laplace priors. We recommend this prior distribution as a default choice for routine applied use. It has the advantage of always giving answers, even when there is complete separation in logistic regression (a common problem, even when the sample size is large and the number of predictors is small), and also automatically applying more shrinkage to higher-order interactions. This can be useful in routine data analysis as well as in automated procedures such as chained equations for missing-data imputation. We implement a procedure to fit generalized linear models in R with the Student-\$t\$ prior distribution by incorporating an approximate EM algorithm into the usual iteratively weighted least squares. We illustrate with several applications, including a series of logistic regressions predicting voting preferences, a small bioassay experiment, and an imputation model for a public health data set.},
	number = {4},
	urldate = {2024-04-11},
	journal = {Ann. Appl. Stat.},
	author = {Gelman, Andrew and Jakulin, Aleks and Pittau, Maria Grazia and Su, Yu-Sung},
	month = dec,
	year = {2008},
	note = {arXiv:0901.4011 [stat]},
	keywords = {Bayesian, prior},
	file = {arXiv.org Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\6PPYJDNP\\0901.html:text/html;Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\ZWFLQ7T8\\Gelman et al. - 2008 - A weakly informative default prior distribution fo.pdf:application/pdf},
}

@article{folch_navigate_2022,
	title = {{NAVIGATE} 24-{Month} {Results}: {Electromagnetic} {Navigation} {Bronchoscopy} for {Pulmonary} {Lesions} at 37 {Centers} in {Europe} and the {United} {States}},
	volume = {17},
	issn = {1556-0864},
	shorttitle = {{NAVIGATE} 24-{Month} {Results}},
	url = {https://www.sciencedirect.com/science/article/pii/S1556086421034146},
	doi = {10.1016/j.jtho.2021.12.008},
	abstract = {Introduction
Electromagnetic navigation bronchoscopy (ENB) is a minimally invasive, image-guided approach to access lung lesions for biopsy or localization for treatment. However, no studies have reported prospective 24-month follow-up from a large, multinational, generalizable cohort. This study evaluated ENB safety, diagnostic yield, and usage patterns in an unrestricted, real-world observational design.
Methods
The NAVIGATE single-arm, pragmatic cohort study (NCT02410837) enrolled subjects at 37 academic and community sites in seven countries with prospective 24-month follow-up. Subjects underwent ENB using the superDimension navigation system versions 6.3 to 7.1. The prespecified primary end point was procedure-related pneumothorax requiring intervention or hospitalization.
Results
A total of 1388 subjects were enrolled for lung lesion biopsy (1329; 95.7\%), fiducial marker placement (272; 19.6\%), dye marking (23; 1.7\%), or lymph node biopsy (36; 2.6\%). Concurrent endobronchial ultrasound-guided staging occurred in 456 subjects. General anesthesia (78.2\% overall, 56.6\% Europe, 81.4\% United States), radial endobronchial ultrasound (50.6\%, 4.0\%, 57.4\%), fluoroscopy (85.0\%, 41.7\%, 91.0\%), and rapid on-site evaluation use (61.7\%, 17.3\%, 68.5\%) differed between regions. Pneumothorax and bronchopulmonary hemorrhage occurred in 4.7\% and 2.7\% of subjects, respectively (3.2\% [primary end point] and 1.7\% requiring intervention or hospitalization). Respiratory failure occurred in 0.6\%. The diagnostic yield was 67.8\% (range: 61.9\%–70.7\%; 55.2\% Europe, 69.8\% United States). Sensitivity for malignancy was 62.6\%. Lung cancer clinical stage was I to II in 64.7\% (55.3\% Europe, 65.8\% United States).
Conclusions
Despite a heterogeneous cohort and regional differences in procedural techniques, ENB demonstrates low complications and a 67.8\% diagnostic yield while allowing biopsy, staging, fiducial placement, and dye marking in a single procedure.},
	number = {4},
	urldate = {2024-04-10},
	journal = {Journal of Thoracic Oncology},
	author = {Folch, Erik E. and Bowling, Mark R. and Pritchett, Michael A. and Murgu, Septimiu D. and Nead, Michael A. and Flandes, Javier and Krimsky, William S. and Mahajan, Amit K. and LeMense, Gregory P. and Murillo, Boris A. and Bansal, Sandeep and Lau, Kelvin and Gildea, Thomas R. and Christensen, Merete and Arenberg, Douglas A. and Singh, Jaspal and Bhadra, Krish and Hogarth, D. Kyle and Towe, Christopher W. and Lamprecht, Bernd and Bezzi, Michela and Mattingley, Jennifer S. and Hood, Kristin L. and Lin, Haiying and Wolvers, Jennifer J. and Khandhar, Sandeep J.},
	month = apr,
	year = {2022},
	keywords = {bronchoscopy},
	pages = {519--531},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\EKUW7B5D\\Folch et al. - 2022 - NAVIGATE 24-Month Results Electromagnetic Navigat.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\H2LR2ATG\\S1556086421034146.html:text/html},
}

@article{folch_design_2016,
	title = {Design of a prospective, multicenter, global, cohort study of electromagnetic navigation bronchoscopy},
	volume = {16},
	issn = {1471-2466},
	url = {http://bmcpulmmed.biomedcentral.com/articles/10.1186/s12890-016-0228-y},
	doi = {10.1186/s12890-016-0228-y},
	abstract = {Background: Electromagnetic navigation bronchoscopy (ENB) procedures allow physicians to access peripheral lung lesions beyond the reach of conventional bronchoscopy. However, published research is primarily limited to small, single-center studies using previous-generation ENB software. The impact of user experience, patient factors, and lesion/procedural characteristics remains largely unexplored in a large, multicenter study. Methods/Design: NAVIGATE (Clinical Evaluation of superDimension™ Navigation System for Electromagnetic Navigation Bronchoscopy) is a prospective, multicenter, global, cohort study. The study aims to enroll up to 2,500 consecutive subjects presenting for evaluation of lung lesions utilizing the ENB procedure at up to 75 clinical sites in the United States, Europe, and Asia. Subjects will be assessed at baseline, at the time of procedure, and at 1, 12, and 24 months post-procedure. The pre-test probability of malignancy will be determined for peripheral lung nodules. Endpoints include procedure-related adverse events, including pneumothorax, bronchopulmonary hemorrhage, and respiratory failure, as well as quality of life, and subject satisfaction. Diagnostic yield and accuracy, repeat biopsy rate, tissue adequacy for genetic testing, and stage at diagnosis will be reported for biopsy procedures. Complementary technologies, such as fluoroscopy and endobronchial ultrasound, will be explored. Success rates of fiducial marker placement, dye marking, and lymph node biopsies will be captured when applicable. Subgroup analyses based on geography, demographics, investigator experience, and lesion and procedure characteristics are planned. Discussion: Study enrollment began in April 2015. As of February 19, 2016, 500 subjects had been enrolled at 23 clinical sites with enrollment ongoing. NAVIGATE will be the largest prospective, multicenter clinical study on ENB procedures to date and will provide real-world experience data on the utility of the ENB procedure in a broad range of clinical scenarios. Trial registration: ClinicalTrials.gov NCT02410837. Registered 31 March 2015.},
	language = {en},
	number = {1},
	urldate = {2024-04-10},
	journal = {BMC Pulm Med},
	author = {Folch, Erik E. and Bowling, Mark R. and Gildea, Thomas R. and Hood, Kristin L. and Murgu, Septimiu D. and Toloza, Eric M. and Wahidi, Momen M. and Williams, Terence and Khandhar, Sandeep J.},
	month = dec,
	year = {2016},
	keywords = {bronchoscopy},
	pages = {60},
	file = {Folch et al. - 2016 - Design of a prospective, multicenter, global, coho.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\8TWRCMST\\Folch et al. - 2016 - Design of a prospective, multicenter, global, coho.pdf:application/pdf},
}

@article{wang_g-computation_2017,
	title = {G-computation of average treatment effects on the treated and the untreated},
	volume = {17},
	issn = {1471-2288},
	url = {https://doi.org/10.1186/s12874-016-0282-4},
	doi = {10.1186/s12874-016-0282-4},
	abstract = {Average treatment effects on the treated (ATT) and the untreated (ATU) are useful when there is interest in: the evaluation of the effects of treatments or interventions on those who received them, the presence of treatment heterogeneity, or the projection of potential outcomes in a target (sub-) population. In this paper we illustrate the steps for estimating ATT and ATU using g-computation implemented via Monte Carlo simulation.},
	number = {1},
	urldate = {2024-04-09},
	journal = {BMC Medical Research Methodology},
	author = {Wang, Aolin and Nianogo, Roch A. and Arah, Onyebuchi A.},
	month = jan,
	year = {2017},
	keywords = {causal inference, g-computation},
	pages = {3},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\8GAV6X2J\\Wang et al. - 2017 - G-computation of average treatment effects on the .pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\F8V85G6M\\s12874-016-0282-4.html:text/html},
}

@article{sitlani_incorporating_2020,
	title = {Incorporating sampling weights into robust estimation of {Cox} proportional hazards regression model, with illustration in the {Multi}-{Ethnic} {Study} of {Atherosclerosis}},
	volume = {20},
	issn = {1471-2288},
	url = {https://doi.org/10.1186/s12874-020-00945-9},
	doi = {10.1186/s12874-020-00945-9},
	abstract = {Cox proportional hazards regression models are used to evaluate associations between exposures of interest and time-to-event outcomes in observational data. When exposures are measured on only a sample of participants, as they are in a case-cohort design, the sampling weights must be incorporated into the regression model to obtain unbiased estimating equations.},
	number = {1},
	urldate = {2024-04-09},
	journal = {BMC Medical Research Methodology},
	author = {Sitlani, Colleen M. and Lumley, Thomas and McKnight, Barbara and Rice, Kenneth M. and Olson, Nels C. and Doyle, Margaret F. and Huber, Sally A. and Tracy, Russell P. and Psaty, Bruce M. and Delaney, Joseph A. C.},
	month = mar,
	year = {2020},
	keywords = {survival analysis, sampling weights},
	pages = {62},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\56MEULMF\\Sitlani et al. - 2020 - Incorporating sampling weights into robust estimat.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\NTMKLNUL\\s12874-020-00945-9.html:text/html},
}

@article{austin_propensity_2018,
	title = {Propensity score matching and complex surveys},
	volume = {27},
	issn = {0962-2802},
	url = {https://doi.org/10.1177/0962280216658920},
	doi = {10.1177/0962280216658920},
	abstract = {Researchers are increasingly using complex population-based sample surveys to estimate the effects of treatments, exposures and interventions. In such analyses, statistical methods are essential to minimize the effect of confounding due to measured covariates, as treated subjects frequently differ from control subjects. Methods based on the propensity score are increasingly popular. Minimal research has been conducted on how to implement propensity score matching when using data from complex sample surveys. We used Monte Carlo simulations to examine two critical issues when implementing propensity score matching with such data. First, we examined how the propensity score model should be formulated. We considered three different formulations depending on whether or not a weighted regression model was used to estimate the propensity score and whether or not the survey weights were included in the propensity score model as an additional covariate. Second, we examined whether matched control subjects should retain their natural survey weight or whether they should inherit the survey weight of the treated subject to which they were matched. Our results were inconclusive with respect to which method of estimating the propensity score model was preferable. In general, greater balance in measured baseline covariates and decreased bias was observed when natural retained weights were used compared to when inherited weights were used. We also demonstrated that bootstrap-based methods performed well for estimating the variance of treatment effects when outcomes are binary. We illustrated the application of our methods by using the Canadian Community Health Survey to estimate the effect of educational attainment on lifetime prevalence of mood or anxiety disorders.},
	language = {en},
	number = {4},
	urldate = {2024-04-08},
	journal = {Stat Methods Med Res},
	author = {Austin, Peter C and Jembere, Nathaniel and Chiu, Maria},
	month = apr,
	year = {2018},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {propensity score, matching},
	pages = {1240--1257},
	file = {SAGE PDF Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\L25X2WJ2\\Austin et al. - 2018 - Propensity score matching and complex surveys.pdf:application/pdf},
}

@article{kent_pulmonary_2023,
	title = {Pulmonary {Open}, {Robotic}, and {Thoracoscopic} {Lobectomy} ({PORTaL}) {Study}: {Survival} {Analysis} of 6646 {Cases}},
	volume = {277},
	issn = {0003-4932},
	shorttitle = {Pulmonary {Open}, {Robotic}, and {Thoracoscopic} {Lobectomy} ({PORTaL}) {Study}},
	url = {https://journals.lww.com/annalsofsurgery/fulltext/2023/06000/Pulmonary_Open,_Robotic,_and_Thoracoscopic.21.aspx},
	doi = {10.1097/SLA.0000000000005820},
	abstract = {Objective: 
          The aim of this study was to analyze overall survival (OS) of robotic-assisted lobectomy (RL), video-assisted thoracoscopic lobectomy (VATS), and open lobectomy (OL) performed by experienced thoracic surgeons across multiple institutions.
          Summary Background Data: 
          Surgeons have increasingly adopted RL for resection of early-stage lung cancer. Comparative survival data following these approaches is largely from single-institution case series or administrative data sets.
          Methods: 
          Retrospective data was collected from 21 institutions from 2013 to 2019. Consecutive cases performed for clinical stage IA–IIIA lung cancer were included. Induction therapy patients were excluded. The propensity-score method of inverse-probability of treatment weighting was used to balance baseline characteristics. OS was estimated using the Kaplan-Meier method. Multivariable Cox proportional hazard models were used to evaluate association among OS and relevant risk factors.
          Results: 
          A total of 2789 RL, 2661 VATS, and 1196 OL cases were included. The unadjusted 5-year OS rate was highest for OL (84\%) followed by RL (81\%) and VATS (74\%); P=0.008. Similar trends were also observed after inverse-probability of treatment weighting adjustment (RL 81\%; VATS 73\%, OL 85\%, P=0.001). Multivariable Cox regression analyses revealed that OL and RL were associated with significantly higher OS compared with VATS (OL vs. VATS: hazard ratio=0.64, P{\textless}0.001 and RL vs. VATS: hazard ratio=0.79; P=0.007).
          Conclusions: 
          Our finding from this large multicenter study suggests that patients undergoing RL and OL have statistically similar OS, while the VATS group was associated with shorter OS. Further studies with longer follow-up are necessary to help evaluate these observations.},
	language = {en-US},
	number = {6},
	urldate = {2024-04-08},
	journal = {Annals of Surgery},
	author = {Kent, Michael S. and Hartwig, Matthew G. and Vallières, Eric and Abbas, Abbas E. and Cerfolio, Robert J. and Dylewski, Mark R. and Fabian, Thomas and Herrera, Luis J. and Jett, Kimble G. and Lazzaro, Richard S. and Meyers, Bryan and Reddy, Rishindra M. and Reed, Michael F. and Rice, David C. and Ross, Patrick and Sarkaria, Inderpal S. and Schumacher, Lana Y. and Spier, Lawrence N. and Tisol, William B. and Wigle, Dennis A. and Zervos, Michael},
	month = jun,
	year = {2023},
	keywords = {propensity score, surgery},
	pages = {1002},
	file = {Kent et al. - 2023 - Pulmonary Open, Robotic, and Thoracoscopic Lobecto.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\65JUA5K2\\Kent et al. - 2023 - Pulmonary Open, Robotic, and Thoracoscopic Lobecto.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\JATTKZEB\\Pulmonary_Open,_Robotic,_and_Thoracoscopic.21.html:text/html},
}

@article{snowden_implementation_2011,
	title = {Implementation of {G}-{Computation} on a {Simulated} {Data} {Set}: {Demonstration} of a {Causal} {Inference} {Technique}},
	volume = {173},
	issn = {0002-9262},
	shorttitle = {Implementation of {G}-{Computation} on a {Simulated} {Data} {Set}},
	url = {https://doi.org/10.1093/aje/kwq472},
	doi = {10.1093/aje/kwq472},
	abstract = {The growing body of work in the epidemiology literature focused on G-computation includes theoretical explanations of the method but very few simulations or examples of application. The small number of G-computation analyses in the epidemiology literature relative to other causal inference approaches may be partially due to a lack of didactic explanations of the method targeted toward an epidemiology audience. The authors provide a step-by-step demonstration of G-computation that is intended to familiarize the reader with this procedure. The authors simulate a data set and then demonstrate both G-computation and traditional regression to draw connections and illustrate contrasts between their implementation and interpretation relative to the truth of the simulation protocol. A marginal structural model is used for effect estimation in the G-computation example. The authors conclude by answering a series of questions to emphasize the key characteristics of causal inference techniques and the G-computation procedure in particular.},
	number = {7},
	urldate = {2024-04-08},
	journal = {American Journal of Epidemiology},
	author = {Snowden, Jonathan M. and Rose, Sherri and Mortimer, Kathleen M.},
	month = apr,
	year = {2011},
	keywords = {causal inference, g-computation},
	pages = {731--738},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\QNVESKXN\\Snowden et al. - 2011 - Implementation of G-Computation on a Simulated Dat.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\Y2I5DSPD\\104142.html:text/html},
}

@misc{tian_bayesian_2022,
	title = {Bayesian semi-parametric inference for clustered recurrent events with zero-inflation and a terminal event/4163305},
	url = {http://arxiv.org/abs/2202.06636},
	abstract = {Recurrent event data are common in clinical studies when participants are followed longitudinally, and are often subject to a terminal event. With the increasing popularity of large pragmatic trials with a heterogeneous source population, participants are often nested in clinics and can be either susceptible or structurally unsusceptible to the recurrent process. These complications require new modeling strategies to accommodate potential zero-event inflation as well as hierarchical data structures in both the terminal and non-terminal event processes. In this paper, we develop a Bayesian semi-parametric model to jointly characterize the zero-inflated recurrent event process and the terminal event process. We use a point mass mixture of non-homogeneous Poisson processes to describe the recurrent intensity and introduce shared random effects from different sources to bridge the non-terminal and terminal event processes. To achieve robustness, we consider nonparametric Dirichlet processes to model the residual of the accelerated failure time model for the survival process as well as the cluster-specific frailty distribution, and develop a Markov Chain Monte Carlo algorithm for posterior inference. We demonstrate the superiority of our proposed model compared with competing models via simulations and apply our method to a pragmatic cluster randomized trial for fall injury prevention among the elderly.},
	urldate = {2024-02-28},
	publisher = {arXiv},
	author = {Tian, Xinyuan and Ciarleglio, Maria and Cai, Jiachen and Greene, Erich and Esserman, Denise and Li, Fan and Zhao, Yize},
	month = dec,
	year = {2022},
	note = {arXiv:2202.06636 [stat]},
	keywords = {Bayesian, competing risks, recurrent event},
	file = {arXiv.org Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\I8VZ97LA\\2202.html:text/html;Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\362NDV7D\\Tian et al. - 2022 - Bayesian semi-parametric inference for clustered r.pdf:application/pdf},
}

@article{orloff_future_2009,
	title = {The future of drug development: advancing clinical trial design},
	volume = {8},
	url = {https://www.nature.com/articles/nrd3025},
	doi = {10.1038/nrd3025},
	abstract = {Orloff and colleagues describe how moving from the traditional approach to clinical trials based on sequential, distinct phases towards a more integrated strategy that increases flexibility and maximizes the use of accumulated knowledge could have a key role in improving the efficiency and cost-effectiveness of drug development. Using examples in which novel trial designs have been successfully applied, they also illustrate the use of the tools involved, such as Bayesian methodologies, and discuss the advantages and challenges for their more widespread implementation. Declining pharmaceutical industry productivity is well recognized by drug developers, regulatory authorities and patient groups. A key part of the problem is that clinical studies are increasingly expensive, driven by the rising costs of conducting Phase II and III trials. It is therefore crucial to ensure that these phases of drug development are conducted more efficiently and cost-effectively, and that attrition rates are reduced. In this article, we argue that moving from the traditional clinical development approach based on sequential, distinct phases towards a more integrated view that uses adaptive design tools to increase flexibility and maximize the use of accumulated knowledge could have an important role in achieving these goals. Applications and examples of the use of these tools — such as Bayesian methodologies — in early- and late-stage drug development are discussed, as well as the advantages, challenges and barriers to their more widespread implementation.},
	number = {12},
	journal = {Nature Reviews Drug Discovery 2009 8:12},
	author = {Orloff, John and Douglas, Frank and Pinheiro, Jose and Levinson, Susan and Branson, Michael and Chaturvedi, Pravin and Ette, Ene and Gallo, Paul and Hirsch, Gigi and Mehta, Cyrus and Patel, Nitin and Sabir, Sameer and Springs, Stacy and Stanski, Donald and Evers, Matthias R and Fleming, Edd and Singh, Navjot and Tramontin, Tony and Golub, Howard},
	month = oct,
	year = {2009},
	note = {Publisher: Nature Publishing Group},
	keywords = {Bayesian, trial design},
	pages = {949--957},
}

@article{oganisian_practical_2021,
	title = {A practical introduction to {Bayesian} estimation of causal effects: {Parametric} and nonparametric approaches},
	volume = {40},
	copyright = {© 2020 John Wiley \& Sons Ltd},
	issn = {1097-0258},
	shorttitle = {A practical introduction to {Bayesian} estimation of causal effects},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8761},
	doi = {10.1002/sim.8761},
	abstract = {Substantial advances in Bayesian methods for causal inference have been made in recent years. We provide an introduction to Bayesian inference for causal effects for practicing statisticians who have some familiarity with Bayesian models and would like an overview of what it can add to causal estimation in practical settings. In the paper, we demonstrate how priors can induce shrinkage and sparsity in parametric models and be used to perform probabilistic sensitivity analyses around causal assumptions. We provide an overview of nonparametric Bayesian estimation and survey their applications in the causal inference literature. Inference in the point-treatment and time-varying treatment settings are considered. For the latter, we explore both static and dynamic treatment regimes. Throughout, we illustrate implementation using off-the-shelf open source software. We hope to leave the reader with implementation-level knowledge of Bayesian causal inference using both parametric and nonparametric models. All synthetic examples and code used in the paper are publicly available on a companion GitHub repository.},
	language = {en},
	number = {2},
	urldate = {2024-03-19},
	journal = {Statistics in Medicine},
	author = {Oganisian, Arman and Roy, Jason A.},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8761},
	keywords = {causal inference, Bayesian},
	pages = {518--551},
	file = {Accepted Version:C\:\\Users\\anbe6\\Zotero\\storage\\EDDXQIEY\\Oganisian and Roy - 2021 - A practical introduction to Bayesian estimation of.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\BVGPA52J\\sim.html:text/html},
}

@article{momenyan_competing_2022,
	title = {Competing risks model for clustered data based on the subdistribution hazards with spatial random effects},
	volume = {49},
	issn = {0266-4763},
	url = {https://doi.org/10.1080/02664763.2021.1884208},
	doi = {10.1080/02664763.2021.1884208},
	abstract = {In some applications, the clustered survival data are arranged spatially such as clinical centers or geographical regions. Incorporating spatial variation in these data not only can improve the accuracy and efficiency of the parameter estimation, but it also investigates the spatial patterns of survivorship for identifying high-risk areas. Competing risks in survival data concern a situation where there is more than one cause of failure, but only the occurrence of the first one is observable. In this paper, we considered Bayesian subdistribution hazard regression models with spatial random effects for the clustered HIV/AIDS data. An intrinsic conditional autoregressive (ICAR) distribution was employed to model the areal spatial random effects. Comparison among competing models was performed by the deviance information criterion. We illustrated the gains of our model through application to the HIV/AIDS data and the simulation studies.},
	number = {7},
	urldate = {2024-03-22},
	journal = {Journal of Applied Statistics},
	author = {Momenyan, Somayeh and Ahmadi, Farzane and Poorolajal, Jalal},
	month = may,
	year = {2022},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/02664763.2021.1884208},
	keywords = {survival analysis, competing risks},
	pages = {1802--1820},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\NQMJJY8N\\Momenyan et al. - 2022 - Competing risks model for clustered data based on .pdf:application/pdf},
}

@article{lee_bayesian_2015,
	title = {Bayesian {Semiparametric} {Analysis} of {Semicompeting} {Risks} {Data}: {Investigating} {Hospital} {Readmission} {After} a {Pancreatic} {Cancer} {Diagnosis}},
	volume = {64},
	issn = {0035-9254},
	shorttitle = {Bayesian {Semiparametric} {Analysis} of {Semicompeting} {Risks} {Data}},
	url = {https://doi.org/10.1111/rssc.12078},
	doi = {10.1111/rssc.12078},
	abstract = {In the USA, the Centers for Medicare and Medicaid Services use 30-day readmission, following hospitalization, as a proxy outcome to monitor quality of care. These efforts generally focus on treatable health conditions, such as pneumonia and heart failure. Expanding quality-of-care systems to monitor conditions for which treatment options are limited or non-existent, such as pancreatic cancer, is challenging because of the non-trivial force of mortality; 30-day mortality for pancreatic cancer is approximately 30\%. In the statistical literature, data that arise when the observation of the time to some non-terminal event is subject to some terminal event are referred to as ‘semicompeting risks data’. Given such data, scientific interest may lie in at least one of three areas: estimation or inference for regression parameters, characterization of dependence between the two events and prediction given a covariate profile. Existing statistical methods focus almost exclusively on the first of these; methods are sparse or non-existent, however, when interest lies with understanding dependence and performing prediction. We propose a Bayesian semiparametric regression framework for analysing semicompeting risks data that permits the simultaneous investigation of all three of the aforementioned scientific goals. Characterization of the induced posterior and posterior predictive distributions is achieved via an efficient Metropolis–Hastings–Green algorithm, which has been implemented in an R package. The framework proposed is applied to data on 16051 individuals who were diagnosed with pancreatic cancer between 2005 and 2008, obtained from Medicare part A. We found that increased risk for readmission is associated with a high comorbidity index, a long hospital stay at initial hospitalization, non-white race, being male and discharge to home care.},
	number = {2},
	urldate = {2024-03-15},
	journal = {Journal of the Royal Statistical Society Series C: Applied Statistics},
	author = {Lee, Kyu Ha and Haneuse, Sebastien and Schrag, Deborah and Dominici, Francesca},
	month = feb,
	year = {2015},
	keywords = {survival analysis, Bayesian, competing risks},
	pages = {253--273},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\N9F8BNVN\\Lee et al. - 2015 - Bayesian Semiparametric Analysis of Semicompeting .pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\GLUC57DI\\7058259.html:text/html},
}

@article{depaoli_improving_2017,
	title = {Improving transparency and replication in {Bayesian} statistics: {The} {WAMBS}-{Checklist}.},
	volume = {22},
	issn = {1939-1463, 1082-989X},
	shorttitle = {Improving transparency and replication in {Bayesian} statistics},
	url = {https://doi.apa.org/doi/10.1037/met0000065},
	doi = {10.1037/met0000065},
	abstract = {Bayesian statistical methods are slowly creeping into all fields of science and are becoming ever more popular in applied research. Although it is very attractive to use Bayesian statistics, our personal experience has led us to believe that naively applying Bayesian methods can be dangerous for at least three main reasons: the potential influence of priors, misinterpretation of Bayesian features and results, and improper reporting of Bayesian results. To deal with these three points of potential danger, we have developed a succinct checklist: the WAMBS-checklist (When to worry and how to Avoid the Misuse of Bayesian Statistics). The purpose of the questionnaire is to describe 10 main points that should be thoroughly checked when applying Bayesian analysis. We provide an account of “when to worry” for each of these issues related to: (a) issues to check before estimating the model, (b) issues to check after estimating the model but before interpreting results, (c) understanding the influence of priors, and (d) actions to take after interpreting results. To accompany these key points of concern, we will present diagnostic tools that can be used in conjunction with the development and assessment of a Bayesian model. We also include examples of how to interpret results when “problems” in estimation arise, as well as syntax and instructions for implementation. Our aim is to stress the importance of openness and transparency of all aspects of Bayesian estimation, and it is our hope that the WAMBS questionnaire can aid in this process.},
	language = {en},
	number = {2},
	urldate = {2024-03-13},
	journal = {Psychological Methods},
	author = {Depaoli, Sarah and Van De Schoot, Rens},
	month = jun,
	year = {2017},
	keywords = {Bayesian},
	pages = {240--261},
	file = {Depaoli and Van De Schoot - 2017 - Improving transparency and replication in Bayesian.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\AEDJI2MB\\Depaoli and Van De Schoot - 2017 - Improving transparency and replication in Bayesian.pdf:application/pdf},
}

@article{brilleman_joint_2019,
	title = {Joint longitudinal and time-to-event models for multilevel hierarchical data},
	volume = {28},
	issn = {0962-2802},
	url = {https://doi.org/10.1177/0962280218808821},
	doi = {10.1177/0962280218808821},
	abstract = {Joint modelling of longitudinal and time-to-event data has received much attention recently. Increasingly, extensions to standard joint modelling approaches are being proposed to handle complex data structures commonly encountered in applied research. In this paper, we propose a joint model for hierarchical longitudinal and time-to-event data. Our motivating application explores the association between tumor burden and progression-free survival in non-small cell lung cancer patients. We define tumor burden as a function of the sizes of target lesions clustered within a patient. Since a patient may have more than one lesion, and each lesion is tracked over time, the data have a three-level hierarchical structure: repeated measurements taken at time points (level 1) clustered within lesions (level 2) within patients (level 3). We jointly model the lesion-specific longitudinal trajectories and patient-specific risk of death or disease progression by specifying novel association structures that combine information across lower level clusters (e.g. lesions) into patient-level summaries (e.g. tumor burden). We provide user-friendly software for fitting the model under a Bayesian framework. Lastly, we discuss alternative situations in which additional clustering factor(s) occur at a level higher in the hierarchy than the patient-level, since this has implications for the model formulation.},
	language = {en},
	number = {12},
	urldate = {2024-02-05},
	journal = {Stat Methods Med Res},
	author = {Brilleman, Samuel L and Crowther, Michael J and Moreno-Betancur, Margarita and Buros Novik, Jacqueline and Dunyak, James and Al-Huniti, Nidal and Fox, Robert and Hammerbacher, Jeff and Wolfe, Rory},
	month = dec,
	year = {2019},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {survival analysis, Bayesian},
	pages = {3502--3515},
	file = {Submitted Version:C\:\\Users\\anbe6\\Zotero\\storage\\53HGP4CA\\Brilleman et al. - 2019 - Joint longitudinal and time-to-event models for mu.pdf:application/pdf},
}

@misc{brilleman_bayesian_2020,
	title = {Bayesian {Survival} {Analysis} {Using} the rstanarm {R} {Package}},
	url = {http://arxiv.org/abs/2002.09633},
	abstract = {Survival data is encountered in a range of disciplines, most notably health and medical research. Although Bayesian approaches to the analysis of survival data can provide a number of benefits, they are less widely used than classical (e.g. likelihood-based) approaches. This may be in part due to a relative absence of user-friendly implementations of Bayesian survival models. In this article we describe how the rstanarm R package can be used to fit a wide range of Bayesian survival models. The rstanarm package facilitates Bayesian regression modelling by providing a user-friendly interface (users specify their model using customary R formula syntax and data frames) and using the Stan software (a C++ library for Bayesian inference) for the back-end estimation. The suite of models that can be estimated using rstanarm is broad and includes generalised linear models (GLMs), generalised linear mixed models (GLMMs), generalised additive models (GAMs) and more. In this article we focus only on the survival modelling functionality. This includes standard parametric (exponential, Weibull, Gompertz) and flexible parametric (spline-based) hazard models, as well as standard parametric accelerated failure time (AFT) models. All types of censoring (left, right, interval) are allowed, as is delayed entry (left truncation), time-varying covariates, time-varying effects, and frailty effects. We demonstrate the functionality through worked examples. We anticipate these implementations will increase the uptake of Bayesian survival analysis in applied research.},
	urldate = {2024-03-08},
	publisher = {arXiv},
	author = {Brilleman, Samuel L. and Elci, Eren M. and Novik, Jacqueline Buros and Wolfe, Rory},
	month = feb,
	year = {2020},
	note = {arXiv:2002.09633 [stat]},
	keywords = {R, survival analysis, Bayesian},
	file = {arXiv.org Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\HZ94UDD5\\2002.html:text/html;Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\JWATN4PG\\Brilleman et al. - 2020 - Bayesian Survival Analysis Using the rstanarm R Pa.pdf:application/pdf},
}

@article{singh_area_2003,
	title = {Area {Deprivation} and {Widening} {Inequalities} in {US} {Mortality}, 1969–1998},
	volume = {93},
	issn = {0090-0036},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1447923/},
	abstract = {Objectives. This study examined age-, sex-, and race-specific gradients in US mortality by area deprivation between 1969 and 1998., Methods. A census-based area deprivation index was linked to county mortality data., Results. Area deprivation gradients in US mortality increased substantially during 1969 through 1998. The gradients were steepest for men and women aged 25 to 44 years and those younger than 25 years, with higher mortality rates observed in more deprived areas. Although area gradients were less pronounced for women in each age group, they rose sharply for women aged 25 to 44 and 45 to 64 years., Conclusions. Areal inequalities in mortality widened because of slower mortality declines in more deprived areas. Future research needs to examine population-level social, behavioral, and medical care factors that may account for the increasing gradient.},
	number = {7},
	urldate = {2024-03-28},
	journal = {Am J Public Health},
	author = {Singh, Gopal K.},
	month = jul,
	year = {2003},
	pmid = {12835199},
	pmcid = {PMC1447923},
	keywords = {ADI},
	pages = {1137--1143},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\2KQFRH8T\\Singh - 2003 - Area Deprivation and Widening Inequalities in US M.pdf:application/pdf},
}

@article{christensen_cumulative_nodate,
	title = {Cumulative {Link} {Models} for {Ordinal} {Regression} with the {R} {Package} ordinal},
	abstract = {This paper introduces the R-package ordinal for the analysis of ordinal data using cumulative link models. The model framework implemented in ordinal includes partial proportional odds, structured thresholds, scale eﬀects and ﬂexible link functions. The package also support cumulative link models with random eﬀects which are covered in a future paper. A speedy and reliable regularized Newton estimation scheme using analytical derivatives provides maximum likelihood estimation of the model class. The paper describes the implementation in the package as well as how to use the functionality in the package for analysis of ordinal data including topics on model identiﬁability and customized modelling. The package implements methods for proﬁle likelihood conﬁdence intervals, analysis of deviance tables with type I, II and III tests, predictions of various kinds as well as methods for checking the convergence of the ﬁtted models.},
	language = {en},
	author = {Christensen, Rune Haubo B},
	keywords = {ordinal regression},
	file = {Christensen - Cumulative Link Models for Ordinal Regression with.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\W648ETGY\\Christensen - Cumulative Link Models for Ordinal Regression with.pdf:application/pdf},
}

@article{wei_semi-markov_2016,
	title = {Semi-{Markov} models for interval censored transient cognitive states with back transitions and a competing risk},
	volume = {25},
	issn = {0962-2802},
	url = {https://doi.org/10.1177/0962280214534412},
	doi = {10.1177/0962280214534412},
	abstract = {Continuous-time multi-state stochastic processes are useful for modeling the flow of subjects from intact cognition to dementia with mild cognitive impairment and global impairment as intervening transient cognitive states and death as a competing risk. Each subject's cognition is assessed periodically resulting in interval censoring for the cognitive states while death without dementia is not interval censored. Since back transitions among the transient states are possible, Markov chains are often applied to this type of panel data. In this manuscript, we apply a semi-Markov process in which we assume that the waiting times are Weibull distributed except for transitions from the baseline state, which are exponentially distributed and in which we assume no additional changes in cognition occur between two assessments. We implement a quasi-Monte Carlo (QMC) method to calculate the higher order integration needed for likelihood estimation. We apply our model to a real dataset, the Nun Study, a cohort of 461 participants.},
	language = {en},
	number = {6},
	urldate = {2024-03-22},
	journal = {Stat Methods Med Res},
	author = {Wei, Shaoceng and Kryscio, Richard J},
	month = dec,
	year = {2016},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {semi-Markov},
	pages = {2909--2924},
}

@article{meira-machado_tdcmsm_2007,
	title = {tdc.msm: {An} {R} library for the analysis of multi-state survival data},
	volume = {86},
	issn = {0169-2607},
	shorttitle = {tdc.msm},
	url = {https://www.sciencedirect.com/science/article/pii/S0169260707000156},
	doi = {10.1016/j.cmpb.2007.01.010},
	abstract = {The aim of this paper is to present an R library, called tdc.msm, developed to analyze multi-state survival data. In this library, the time-dependent regression model and multi-state models are included as two possible approaches for such data. For the multi-state modelling five different models are considered, allowing the user to choose between Markov and semi-Markov property, as well as to use homogeneous or non-homogeneous models. Specifically, the following multi-state models in continuous time were implemented: Cox Markov model; Cox semi-Markov model; homogeneous Markov model; non-homogeneous piecewise model and non-parametric Markov model. This software can be used to fit multi-state models with one initial state (e.g., illness diagnosis), a finite number of intermediate states, representing, for example, a change of treatment, and one absorbing state corresponding to a terminal event of interest. Graphical output includes survival estimates, transition probabilities estimates and smooth log hazard for continuous covariates.},
	number = {2},
	urldate = {2024-03-04},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Meira-Machado, Luís and Cadarso-Suárez, Carmen and de Uña-Álvarez, Jacobo},
	month = may,
	year = {2007},
	keywords = {survival analysis, Markov},
	pages = {131--140},
}

@article{liu_scr-cusum_2023,
	title = {{SCR}-{CUSUM}: {An} illness-death semi-{Markov} model-based risk-adjusted {CUSUM} for semi-competing risk data monitoring},
	volume = {184},
	issn = {0360-8352},
	shorttitle = {{SCR}-{CUSUM}},
	url = {https://www.sciencedirect.com/science/article/pii/S0360835223005545},
	doi = {10.1016/j.cie.2023.109530},
	abstract = {Assessing the medical quality of hospitals based on control charts has received lots of attention. However, existing control charts produce delay and false alarms when applying to semi-competing risk (SCR) data, which widely exist in biomedical and clinical fields. Some studies have suggested that SCR data are important for investigating the healthcare quality of hospitals, but there are seldom targeted control charts for monitoring the medical quality implied by them. Therefore, in this paper, we propose a risk-adjusted cumulative sum control chart based on illness-death semi-Markov model, named SCR-CUSUM, to monitor the deterioration of hospitals’ medical quality by detecting the shifts of non-terminal and terminal events simultaneously. The chart statistic of SCR-CUSUM shows good interpretability. By replacing the preset log-likelihood ratio with the generalized likelihood ratio, SCR-CUSUM become more sensitive and general. Meanwhile, we provide the theoretical control limit and verify its feasibility. Both of the results of simulation and case study prove that SCR-CUSUM works better than the comparison methods when applying to SCR data. In addition, we also analyze the causes of false and delay alarms for existing control charts using simulation data.},
	urldate = {2024-03-22},
	journal = {Computers \& Industrial Engineering},
	author = {Liu, Ruoyu and Lai, Xin and Wang, Jiayin and Zhu, Xiaoyan and Liu, Yuqian},
	month = oct,
	year = {2023},
	keywords = {survival analysis, competing risks, semi-Markov},
	pages = {109530},
}

@article{krol_semimarkov_2015,
	title = {{SemiMarkov}: {An} {R} {Package} for {Parametric} {Estimation} in {Multi}-{State} {Semi}-{Markov} {Models}},
	volume = {66},
	copyright = {Copyright (c) 2013 Agnieszka Król, Philippe Saint-Pierre},
	issn = {1548-7660},
	shorttitle = {{SemiMarkov}},
	url = {https://doi.org/10.18637/jss.v066.i06},
	doi = {10.18637/jss.v066.i06},
	abstract = {Multi-state models provide a relevant tool for studying the observations of a continuoustime process at arbitrary times. Markov models are often considered even if semi-Markov are better adapted in various situations. Such models are still not frequently applied mainly due to lack of available software. We have developed the R package SemiMarkov to fit homogeneous semi-Markov models to longitudinal data. The package performs maximum likelihood estimation in a parametric framework where the distributions of the sojourn times can be chosen between exponential, Weibull or exponentiated Weibull. The package computes and displays the hazard rates of sojourn times and the hazard rates of the semi-Markov process. The effects of covariates can be studied with a Cox proportional hazards model for the sojourn times distributions. The number of covariates and the distribution of sojourn times can be specified for each possible transition providing a great flexibility in a model’s definition. This article presents parametric semi-Markov models and gives a detailed description of the package together with an application to asthma control.},
	language = {en},
	urldate = {2024-03-05},
	journal = {Journal of Statistical Software},
	author = {Król, Agnieszka and Saint-Pierre, Philippe},
	month = aug,
	year = {2015},
	keywords = {R, survival analysis, competing risks, semi-Markov},
	pages = {1--16},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\BC527R25\\Król and Saint-Pierre - 2015 - SemiMarkov An R Package for Parametric Estimation.pdf:application/pdf},
}

@article{crowther_parametric_2017,
	title = {Parametric multistate survival models: {Flexible} modelling allowing transition-specific distributions with application to estimating clinically useful measures of effect differences},
	volume = {36},
	issn = {1097-0258},
	shorttitle = {Parametric multistate survival models},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7448},
	doi = {10.1002/sim.7448},
	abstract = {Multistate models are increasingly being used to model complex disease profiles. By modelling transitions between disease states, accounting for competing events at each transition, we can gain a much richer understanding of patient trajectories and how risk factors impact over the entire disease pathway. In this article, we concentrate on parametric multistate models, both Markov and semi-Markov, and develop a flexible framework where each transition can be specified by a variety of parametric models including exponential, Weibull, Gompertz, Royston-Parmar proportional hazards models or log-logistic, log-normal, generalised gamma accelerated failure time models, possibly sharing parameters across transitions. We also extend the framework to allow time-dependent effects. We then use an efficient and generalisable simulation method to calculate transition probabilities from any fitted multistate model, and show how it facilitates the simple calculation of clinically useful measures, such as expected length of stay in each state, and differences and ratios of proportion within each state as a function of time, for specific covariate patterns. We illustrate our methods using a dataset of patients with primary breast cancer. User-friendly Stata software is provided.},
	language = {en},
	number = {29},
	urldate = {2024-03-22},
	journal = {Statistics in Medicine},
	author = {Crowther, Michael J. and Lambert, Paul C.},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.7448},
	keywords = {survival analysis, competing risks, semi-Markov},
	pages = {4719--4742},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\ABFEPKVJ\\Crowther and Lambert - 2017 - Parametric multistate survival models Flexible mo.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\L8RYMHMA\\sim.html:text/html},
}

@article{barbu_smm_2019,
	title = {{SMM}: {An} {R} {Package} for {Estimation} and {Simulation} of {Discrete}-time semi-{Markov} {Models}},
	volume = {10},
	issn = {2073-4859},
	shorttitle = {{SMM}},
	url = {https://journal.r-project.org/archive/2018/RJ-2018-050/index.html},
	doi = {10.32614/RJ-2018-050},
	abstract = {Semi-Markov models, independently introduced by Lévy (1954), Smith (1955) and Takacs (1954), are a generalization of the well-known Markov models. For semi-Markov models, sojourn times can be arbitrarily distributed, while sojourn times of Markov models are constrained to be exponentially distributed (in continuous time) or geometrically distributed (in discrete time). The aim of this paper is to present the R package SMM, devoted to the simulation and estimation of discretetime multi-state semi-Markov and Markov models. For the semi-Markov case we have considered: parametric and non-parametric estimation; with and without censoring at the beginning and/or at the end of sample paths; one or several independent sample paths. Several discrete-time distributions are considered for the parametric estimation of sojourn time distributions of semi-Markov chains: Uniform, Geometric, Poisson, Discrete Weibull and Binomial Negative.},
	language = {en},
	number = {2},
	urldate = {2024-03-26},
	journal = {The R Journal},
	author = {Barbu, Stefan, Vlad and Bérard, Caroline and Cellier, Dominique and Sautreuil, Mathilde and Vergne, Nicolas},
	year = {2019},
	keywords = {R, competing risks, semi-Markov},
	pages = {226},
	file = {Barbu et al. - 2019 - SMM An R Package for Estimation and Simulation of.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\VFV4R3UE\\Barbu et al. - 2019 - SMM An R Package for Estimation and Simulation of.pdf:application/pdf},
}

@article{asanjarani_estimation_2022,
	title = {Estimation of semi-{Markov} multi-state models: a comparison of the sojourn times and transition intensities approaches},
	volume = {18},
	copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
	issn = {1557-4679},
	shorttitle = {Estimation of semi-{Markov} multi-state models},
	url = {https://www.degruyter.com/document/doi/10.1515/ijb-2020-0083/html},
	doi = {10.1515/ijb-2020-0083},
	abstract = {Semi-Markov models are widely used for survival analysis and reliability analysis. In general, there are two competing parameterizations and each entails its own interpretation and inference properties. On the one hand, a semi-Markov process can be defined based on the distribution of sojourn times, often via hazard rates, together with transition probabilities of an embedded Markov chain. On the other hand, intensity transition functions may be used, often referred to as the hazard rates of the semi-Markov process. We summarize and contrast these two parameterizations both from a probabilistic and an inference perspective, and we highlight relationships between the two approaches. In general, the intensity transition based approach allows the likelihood to be split into likelihoods of two-state models having fewer parameters, allowing efficient computation and usage of many survival analysis tools. Nevertheless, in certain cases the sojourn time based approach is natural and has been exploited extensively in applications. In contrasting the two approaches and contemporary relevant R packages used for inference, we use two real datasets highlighting the probabilistic and inference properties of each approach. This analysis is accompanied by an R vignette.},
	language = {en},
	number = {1},
	urldate = {2024-03-06},
	journal = {The International Journal of Biostatistics},
	author = {Asanjarani, Azam and Liquet, Benoit and Nazarathy, Yoni},
	month = may,
	year = {2022},
	note = {Publisher: De Gruyter},
	keywords = {survival analysis, competing risks, semi-Markov},
	pages = {243--262},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\6KL2V8HL\\Asanjarani et al. - 2022 - Estimation of semi-Markov multi-state models a co.pdf:application/pdf},
}

@article{reeder_novel_2023,
	title = {A novel approach to joint prediction of preeclampsia and delivery timing using semicompeting risks},
	volume = {228},
	issn = {0002-9378},
	url = {https://www.sciencedirect.com/science/article/pii/S0002937822006883},
	doi = {10.1016/j.ajog.2022.08.045},
	abstract = {Background
Preeclampsia is a pregnancy complication that contributes substantially to perinatal morbidity and mortality worldwide. Existing approaches to modeling and prediction of preeclampsia typically focus either on predicting preeclampsia risk alone, or on the timing of delivery following a diagnosis of preeclampsia. As such, they are misaligned with typical healthcare interactions during which the 2 events are generally considered simultaneously.
Objective
This study aimed to describe the “semicompeting risks” framework as an innovative approach for jointly modeling the risk and timing of preeclampsia and the timing of delivery simultaneously. Through this approach, one can obtain, at any point during the pregnancy, clinically relevant summaries of an individual’s predicted outcome trajectories in 4 risk categories: not developing preeclampsia and not having delivered, not developing preeclampsia but having delivered because of other causes, developing preeclampsia but not having delivered, and developing preeclampsia and having delivered.
Study Design
To illustrate the semicompeting risks methodology, we presented an example analysis of a pregnancy cohort from the electronic health record of an urban, academic medical center in Boston, Massachusetts (n=9161 pregnancies). We fit an illness–death model with proportional-hazards regression specifications describing 3 hazards for timings of preeclampsia, delivery in the absence of preeclampsia, and delivery following preeclampsia diagnosis.
Results
The results indicated nuanced relationships between a variety of risk factors and the timings of preeclampsia diagnosis and delivery, including maternal age, race/ethnicity, parity, body mass index, diabetes mellitus, chronic hypertension, cigarette use, and proteinuria at 20 weeks’ gestation. Sample predictions for a diverse set of individuals highlighted differences in projected outcome trajectories with regard to preeclampsia risk and timing, and timing of delivery either before or after preeclampsia diagnosis.
Conclusion
The semicompeting risks framework enables characterization of the joint risk and timing of preeclampsia and delivery, providing enhanced, meaningful information regarding clinical decision-making throughout the pregnancy.},
	number = {3},
	urldate = {2024-03-22},
	journal = {American Journal of Obstetrics and Gynecology},
	author = {Reeder, Harrison T. and Haneuse, Sebastien and Modest, Anna M. and Hacker, Michele R. and Sudhof, Leanna S. and Papatheodorou, Stefania I.},
	month = mar,
	year = {2023},
	keywords = {clinical risk prediction, gestational hypertension, medically-indicated preterm birth, preeclampsia, semicompeting risks},
	pages = {338.e1--338.e12},
}

@article{fuino_long-term_2018,
	title = {Long-term care models and dependence probability tables by acuity level: {New} empirical evidence from {Switzerland}},
	volume = {81},
	issn = {0167-6687},
	shorttitle = {Long-term care models and dependence probability tables by acuity level},
	url = {https://www.sciencedirect.com/science/article/pii/S0167668717305346},
	doi = {10.1016/j.insmatheco.2018.05.002},
	abstract = {Due to the demographic changes and population aging occurring in many countries, the financing of long-term care (LTC) poses a systemic threat. The scarcity of knowledge about the probability of an elderly person needing help with activities of daily living has hindered the development of insurance solutions that complement existing social systems. In this paper, we consider two models: a frailty level model that studies the evolution of a dependent person through mild, moderate and severe dependency states to death and a type of care model that distinguishes between care received at home and care received in an institution. We develop and interpret the expressions for the state- and time-dependent transition probabilities in a semi-Markov framework. Then, we empirically assess these probabilities using a novel longitudinal dataset covering all LTC needs in Switzerland over a 20-year period. As a key result, we are the first to derive dependence probability tables by acuity level, gender and age for the Swiss population. We find that the transition probabilities differ significantly by gender, age and time spent in the frailty level and type of care states.},
	urldate = {2024-03-22},
	journal = {Insurance: Mathematics and Economics},
	author = {Fuino, Michel and Wagner, Joël},
	month = jul,
	year = {2018},
	keywords = {survival analysis, semi-Markov},
	pages = {51--70},
	file = {Accepted Version:C\:\\Users\\anbe6\\Zotero\\storage\\UZRD3A5D\\Fuino and Wagner - 2018 - Long-term care models and dependence probability t.pdf:application/pdf},
}

@article{sorensen_performance_1993,
	title = {Performance status assessment in cancer patients. {An} inter-observer variability study},
	volume = {67},
	copyright = {1993 Cancer Research Campaign},
	issn = {1532-1827},
	url = {https://www.nature.com/articles/bjc1993140},
	doi = {10.1038/bjc.1993.140},
	abstract = {The ECOG Scale of Performance Status (PS) is widely used to quantify the functional status of cancer patients, and is an important factor determining prognosis in a number of malignant conditions. The PS describes the status of symptoms and functions with respect to ambulatory status and need for care. PS 0 means normal activity, PS 1 means some symptoms, but still near fully ambulatory, PS 2 means less than 50\%, and PS 3 means more than 50\% of daytime in bed, while PS 4 means completely bedridden. An inter-observer variability study of PS assessment has been carried out to evaluate the non-chance agreement among three oncologists rating 100 consecutive cancer patients. Total unanimity was observed in 40 cases, unanimity between two observers in 53 cases, and total disagreement in seven cases. Kappa statistics reveal the ability of the observers compared to change alone and were used to evaluate non-chance agreement. Overall Kappa was 0.44, (95\% confidence limits 0.38-0.51). The Kappa for PS 0 was 0.55 (0.44-0.67), while those for PS 1, 2, 3 and four were 0.48 (0.37-0.60), 0.31 (0.19-0.42), 0.43 (0.32-0.55), and 0.33 (0.33-0.45), respectively. If one observer allocated patients to PS 0-2, then another randomly selected observed placed the patients in the same category with a probability of 0.92. For patients with PS 3-4 the probability that the same category would be chosen was 0.82. Overall, the non-chance agreement between observers was only moderate, when all ECOG Performance Status groups were considered. However, agreement with regard to allocation of patients to PS 0-2 versus 3-4 was high. This is of interest because this cut-off is often used in clinical studies.},
	language = {en},
	number = {4},
	urldate = {2024-03-22},
	journal = {Br J Cancer},
	author = {Sørensen, J. B. and Klee, M. and Palshof, T. and Hansen, H. H.},
	month = apr,
	year = {1993},
	note = {Publisher: Nature Publishing Group},
	keywords = {ECOG, qol},
	pages = {773--775},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\7GRUMVV9\\Sørensen et al. - 1993 - Performance status assessment in cancer patients. .pdf:application/pdf},
}

@article{cheung_multistate_2022,
	title = {Multistate models for the natural history of cancer progression},
	volume = {127},
	copyright = {2022 This is a U.S. Government work and not under copyright protection in the US; foreign copyright protection may apply},
	issn = {1532-1827},
	url = {https://www.nature.com/articles/s41416-022-01904-5},
	doi = {10.1038/s41416-022-01904-5},
	abstract = {Multistate models can be effectively used to characterise the natural history of cancer. Inference from such models has previously been useful for setting screening policies.},
	language = {en},
	number = {7},
	urldate = {2024-03-22},
	journal = {Br J Cancer},
	author = {Cheung, Li C. and Albert, Paul S. and Das, Shrutikona and Cook, Richard J.},
	month = oct,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {competing risks},
	pages = {1279--1288},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\EKMHBSH7\\Cheung et al. - 2022 - Multistate models for the natural history of cance.pdf:application/pdf},
}

@article{shinozaki_understanding_2020,
	title = {Understanding {Marginal} {Structural} {Models} for {Time}-{Varying} {Exposures}: {Pitfalls} and {Tips}},
	volume = {30},
	issn = {0917-5040, 1349-9092},
	shorttitle = {Understanding {Marginal} {Structural} {Models} for {Time}-{Varying} {Exposures}},
	url = {https://www.jstage.jst.go.jp/article/jea/30/9/30_JE20200226/_article},
	doi = {10.2188/jea.JE20200226},
	abstract = {Epidemiologists are increasingly encountering complex longitudinal data, in which exposures and their confounders vary during follow-up. When a prior exposure aﬀects the confounders of the subsequent exposures, estimating the eﬀects of the time-varying exposures requires special statistical techniques, possibly with structural (ie, counterfactual) models for targeted eﬀects, even if all confounders are accurately measured. Among the methods used to estimate such eﬀects, which can be cast as a marginal structural model in a straightforward way, one popular approach is inverse probability weighting. Despite the seemingly intuitive theory and easy-to-implement software, misunderstandings (or “pitfalls”) remain. For example, one may mistakenly equate marginal structural models with inverse probability weighting, failing to distinguish a marginal structural model encoding the causal parameters of interest from a nuisance model for exposure probability, and thereby failing to separate the problems of variable selection and model speciﬁcation for these distinct models. Assuming the causal parameters of interest are identiﬁed given the study design and measurements, we provide a step-by-step illustration of generalized computation of standardization (called the gformula) and inverse probability weighting, as well as the speciﬁcation of marginal structural models, particularly for time-varying exposures. We use a novel hypothetical example, which allows us access to typically hidden potential outcomes. This illustration provides steppingstones (or “tips”) to understand more concretely the estimation of the eﬀects of complex time-varying exposures.},
	language = {en},
	number = {9},
	urldate = {2024-03-22},
	journal = {Journal of Epidemiology},
	author = {Shinozaki, Tomohiro and Suzuki, Etsuji},
	month = sep,
	year = {2020},
	pages = {377--389},
	file = {Shinozaki and Suzuki - 2020 - Understanding Marginal Structural Models for Time-.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\FBPUIA8E\\Shinozaki and Suzuki - 2020 - Understanding Marginal Structural Models for Time-.pdf:application/pdf},
}

@article{mohammed_plotting_2008,
	title = {Plotting basic control charts: tutorial notes for healthcare practitioners},
	volume = {17},
	issn = {1475-3898, 1475-3901},
	shorttitle = {Plotting basic control charts},
	url = {https://qualitysafety.bmj.com/lookup/doi/10.1136/qshc.2004.012047},
	doi = {10.1136/qshc.2004.012047},
	abstract = {There is considerable interest in the use of statistical process control (SPC) in healthcare. Although SPC is part of an overall philosophy of continual improvement, the implementation of SPC usually requires the production of control charts. However, as SPC is relatively new to healthcare practitioners and is not routinely featured in medical statistics texts/courses, there is a need to explain the issues involved in the selection and construction of control charts in practice. Following a brief overview of SPC in healthcare and preliminary issues, we use a tutorial-based approach to illustrate the selection and construction of four commonly used control charts (xmr-chart, p-chart, u-chart, c-chart) using examples from healthcare. For each control chart, the raw data, the relevant formulae and their use and interpretation of the final SPC chart are provided together with a notes section highlighting important issues for the SPC practitioner.},
	language = {en},
	number = {2},
	urldate = {2024-03-21},
	journal = {Quality and Safety in Health Care},
	author = {Mohammed, M A and Worthington, P and Woodall, W H},
	month = apr,
	year = {2008},
	keywords = {QI charts, special cause variation},
	pages = {137--145},
	file = {Mohammed et al. - 2008 - Plotting basic control charts tutorial notes for .pdf:C\:\\Users\\anbe6\\Zotero\\storage\\JUSD6PFD\\Mohammed et al. - 2008 - Plotting basic control charts tutorial notes for .pdf:application/pdf},
}

@article{andersen_causal_2017,
	title = {Causal inference in survival analysis using pseudo-observations},
	volume = {36},
	copyright = {Copyright © 2017 John Wiley \& Sons, Ltd.},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7297},
	doi = {10.1002/sim.7297},
	abstract = {Causal inference for non-censored response variables, such as binary or quantitative outcomes, is often based on either (1) direct standardization (‘G-formula’) or (2) inverse probability of treatment assignment weights (‘propensity score’). To do causal inference in survival analysis, one needs to address right-censoring, and often, special techniques are required for that purpose. We will show how censoring can be dealt with ‘once and for all’ by means of so-called pseudo-observations when doing causal inference in survival analysis. The pseudo-observations can be used as a replacement of the outcomes without censoring when applying ‘standard’ causal inference methods, such as (1) or (2) earlier. We study this idea for estimating the average causal effect of a binary treatment on the survival probability, the restricted mean lifetime, and the cumulative incidence in a competing risks situation. The methods will be illustrated in a small simulation study and via a study of patients with acute myeloid leukemia who received either myeloablative or non-myeloablative conditioning before allogeneic hematopoetic cell transplantation. We will estimate the average causal effect of the conditioning regime on outcomes such as the 3-year overall survival probability and the 3-year risk of chronic graft-versus-host disease. Copyright © 2017 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {17},
	urldate = {2024-03-20},
	journal = {Statistics in Medicine},
	author = {Andersen, Per K. and Syriopoulou, Elisavet and Parner, Erik T.},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.7297},
	keywords = {causal inference, survival analysis},
	pages = {2669--2681},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\XSLMZJXY\\Andersen et al. - 2017 - Causal inference in survival analysis using pseudo.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\PQ6I9EES\\sim.html:text/html},
}

@article{lee_bayesian_2012,
	title = {Bayesian {Clinical} {Trials} in {Action}},
	volume = {31},
	issn = {0277-6715},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3495977/},
	doi = {10.1002/sim.5404},
	abstract = {Although the frequentist paradigm has been the predominant approach to clinical trial design since the 1940s, it has several notable limitations. The alternative Bayesian paradigm has been greatly enhanced by advancements in computational algorithms and computer hardware. Compared to its frequentist counterpart, the Bayesian framework has several unique advantages, and its incorporation into clinical trial design is occurring more frequently. Using an extensive literature review to assess how Bayesian methods are used in clinical trials, we find them most commonly used for dose finding, efficacy monitoring, toxicity monitoring, diagnosis/decision making, and for studying pharmacokinetics/pharmacodynamics. The additional infrastructure required for implementing Bayesian methods in clinical trials may include specialized software programs to run the study design, simulation, and analysis, and Web-based applications, which are particularly useful for timely data entry and analysis. Trial success requires not only the development of proper tools but also timely and accurate execution of data entry, quality control, adaptive randomization, and Bayesian computation. The relative merit of the Bayesian and frequentist approaches continues to be the subject of debate in statistics. However, more evidence can be found showing the convergence of the two camps, at least at the practical level. Ultimately, better clinical trial methods lead to more efficient designs, lower sample sizes, more accurate conclusions, and better outcomes for patients enrolled in the trials. Bayesian methods offer attractive alternatives for better trials. More such trials should be designed and conducted to refine the approach and demonstrate its real benefit in action.},
	number = {25},
	urldate = {2024-03-19},
	journal = {Stat Med},
	author = {Lee, J. Jack and Chu, Caleb T.},
	month = nov,
	year = {2012},
	pmid = {22711340},
	pmcid = {PMC3495977},
	keywords = {Bayesian, trial design},
	pages = {2955--2972},
	file = {PubMed Central Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\M4YA7JLR\\Lee and Chu - 2012 - Bayesian Clinical Trials in Action.pdf:application/pdf},
}

@article{zhang_infuence_2024,
	title = {Infuence of financial toxicity on quality of life in lung cancer patients undergoing immunotherapy: the mediating effect of self-perceived burden},
	url = {https://doi.org/10.21203/rs.3.rs-3834736/v1},
	doi = {10.21203/rs.3.rs-3834736/v1},
	abstract = {Background},
	author = {Zhang, Zhaoli and Xu, Zhen and Yang, Shikun and Huang, Jingui and Huang, Fengmei and Shi, Yumei},
	year = {2024},
	keywords = {qol, lung cancer},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\KS79TEVG\\Zhang et al. - 2024 - Inuence of nancial toxicity on quality of life in .pdf:application/pdf},
}

@article{cui_estimating_2023,
	title = {Estimating heterogeneous treatment effects with right-censored data via causal survival forests},
	volume = {85},
	issn = {1369-7412},
	url = {https://doi.org/10.1093/jrsssb/qkac001},
	doi = {10.1093/jrsssb/qkac001},
	abstract = {Forest-based methods have recently gained in popularity for non-parametric treatment effect estimation. Building on this line of work, we introduce causal survival forests, which can be used to estimate heterogeneous treatment effects in survival and observational setting where outcomes may be right-censored. Our approach relies on orthogonal estimating equations to robustly adjust for both censoring and selection effects under unconfoundedness. In our experiments, we find our approach to perform well relative to a number of baselines.},
	number = {2},
	urldate = {2024-03-15},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Cui, Yifan and Kosorok, Michael R and Sverdrup, Erik and Wager, Stefan and Zhu, Ruoqing},
	month = apr,
	year = {2023},
	keywords = {causal inference, survival analysis},
	pages = {179--211},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\PTX3HZKK\\Cui et al. - 2023 - Estimating heterogeneous treatment effects with ri.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\GI7B3C5C\\7058918.html:text/html},
}

@article{krol_tutorial_2017,
	title = {Tutorial in {Joint} {Modeling} and {Prediction}: {A} {Statistical} {Software} for {Correlated} {Longitudinal} {Outcomes}, {Recurrent} {Events} and a {Terminal} {Event}},
	volume = {81},
	copyright = {Copyright (c) 2017 Agnieszka Król, Audrey Mauguen, Yassin Mazroui, Alexandre Laurent, Stefan Michiels, Virginie Rondeau},
	issn = {1548-7660},
	shorttitle = {Tutorial in {Joint} {Modeling} and {Prediction}},
	url = {https://doi.org/10.18637/jss.v081.i03},
	doi = {10.18637/jss.v081.i03},
	abstract = {Extensions in the field of joint modeling of correlated data and dynamic predictions improve the development of prognosis research. The R package frailtypack provides estimations of various joint models for longitudinal data and survival events. In particular, it fits models for recurrent events and a terminal event (frailtyPenal), models for two survival outcomes for clustered data (frailtyPenal), models for two types of recurrent events and a terminal event (multivPenal), models for a longitudinal biomarker and a terminal event (longiPenal) and models for a longitudinal biomarker, recurrent events and a terminal event (trivPenal). The estimators are obtained using a standard and penalized maximum likelihood approach, each model function allows to evaluate goodness-of-fit analyses and provides plots of baseline hazard functions. Finally, the package provides individual dynamic predictions of the terminal event and evaluation of predictive accuracy. This paper presents the theoretical models with estimation techniques, applies the methods for predictions and illustrates frailtypack functions details with examples.},
	language = {en},
	urldate = {2024-03-13},
	journal = {Journal of Statistical Software},
	author = {Król, Agnieszka and Mauguen, Audrey and Mazroui, Yassin and Laurent, Alexandre and Michiels, Stefan and Rondeau, Virginie},
	month = oct,
	year = {2017},
	keywords = {R, survival analysis},
	pages = {1--52},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\PJMU8PI4\\Król et al. - 2017 - Tutorial in Joint Modeling and Prediction A Stati.pdf:application/pdf},
}

@article{lu_palliative_2024,
	title = {Palliative {Care} as a {Component} of {High}-{Value} and {Cost}-{Saving} {Care} {During} {Hospitalization} for {Metastatic} {Cancer}},
	issn = {2688-1527},
	url = {https://ascopubs.org/doi/10.1200/OP.23.00576},
	doi = {10.1200/OP.23.00576},
	abstract = {Purpose
Randomized controlled trials have demonstrated that palliative care (PC) can improve quality of life and survival for outpatients with advanced cancer, but there are limited population-based data on the value of inpatient PC. We assessed PC as a component of high-value care among a nationally representative sample of inpatients with metastatic cancer and identified hospitalization characteristics significantly associated with high costs.
Methods
Hospitalizations of patients 18 years and older with a primary diagnosis of metastatic cancer from the National Inpatient Sample from 2010 to 2019 were analyzed. We used multivariable mixed-effects logistic regression to assess medical services, patient demographics, and hospital characteristics associated with higher charges billed to insurance and hospital costs. Generalized linear mixed-effects models were used to determine cost savings associated with provision of PC.
Results
Among 397,691 hospitalizations from 2010 to 2019, the median charge per admission increased by 24.9\%, from \$44,904 in US dollars (USD) to \$56,098 USD, whereas the median hospital cost remained stable at \$14,300 USD. Receipt of inpatient PC was associated with significantly lower charges (odds ratio [OR], 0.62 [95\% CI, 0.61 to 0.64]; P {\textless} .001) and costs (OR, 0.59 [95\% CI, 0.58 to 0.61]; P {\textless} .001). Factors associated with high charges were receipt of invasive medical ventilation (P {\textless} .001) or systemic therapy (P {\textless} .001), Hispanic patients (P {\textless} .001), young age (18-49 years, P {\textless} .001), and for-profit hospitals (P {\textless} .001). PC provision was associated with a \$1,310 USD (–13.6\%, P {\textless} .001) reduction in costs per hospitalization compared with no PC, independent of the receipt of invasive care and age.
Conclusion
Inpatient PC is associated with reduced hospital costs for patients with metastatic cancer, irrespective of age and receipt of aggressive interventions. Integration of inpatient PC may de-escalate costs incurred through low-value inpatient interventions.},
	urldate = {2024-03-12},
	journal = {JCO Oncol Pract},
	author = {Lu, Sifan and Rakovitch, Eileen and Hannon, Breffni and Zimmermann, Camilla and Dharmarajan, Kavita V. and Yan, Michael and De Almeida, John R. and Yao, Christopher M.K.L. and Gillespie, Erin F. and Chino, Fumiko and Yerramilli, Divya and Goonaratne, Ethan and Abdel-Rahman, Fadwa and Othman, Hiba and Mheid, Sara and Tsai, Chiaojung Jillian},
	month = mar,
	year = {2024},
	note = {Publisher: Wolters Kluwer},
	pages = {OP.23.00576},
	file = {Lu et al. - 2024 - Palliative Care as a Component of High-Value and C.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\YPYNH9XS\\Lu et al. - 2024 - Palliative Care as a Component of High-Value and C.pdf:application/pdf},
}

@article{liu_introduction_2013,
	title = {An {Introduction} to {Sensitivity} {Analysis} for {Unobserved} {Confounding} in {Nonexperimental} {Prevention} {Research}},
	volume = {14},
	issn = {1389-4986, 1573-6695},
	url = {http://link.springer.com/10.1007/s11121-012-0339-5},
	doi = {10.1007/s11121-012-0339-5},
	language = {en},
	number = {6},
	urldate = {2024-03-11},
	journal = {Prev Sci},
	author = {Liu, Weiwei and Kuramoto, S. Janet and Stuart, Elizabeth A.},
	month = dec,
	year = {2013},
	keywords = {sensitivity analysis},
	pages = {570--580},
	file = {s11121-012-0339-5.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\RQ7YUZ8L\\s11121-012-0339-5.pdf:application/pdf},
}

@article{arah_bias_2008,
	title = {Bias {Formulas} for {External} {Adjustment} and {Sensitivity} {Analysis} of {Unmeasured} {Confounders}},
	volume = {18},
	issn = {10472797},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1047279708000914},
	doi = {10.1016/j.annepidem.2008.04.003},
	language = {en},
	number = {8},
	urldate = {2024-03-11},
	journal = {Annals of Epidemiology},
	author = {Arah, Onyebuchi A. and Chiba, Yasutaka and Greenland, Sander},
	month = aug,
	year = {2008},
	keywords = {sensitivity analysis},
	pages = {637--646},
	file = {Arah et al. - 2008 - Bias Formulas for External Adjustment and Sensitiv.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\TU877BCR\\Arah et al. - 2008 - Bias Formulas for External Adjustment and Sensitiv.pdf:application/pdf},
}

@article{cinelli_making_2020,
	title = {Making sense of sensitivity: extending omitted variable bias},
	volume = {82},
	copyright = {© 2019 Royal Statistical Society},
	issn = {1467-9868},
	shorttitle = {Making sense of sensitivity},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12348},
	doi = {10.1111/rssb.12348},
	abstract = {We extend the omitted variable bias framework with a suite of tools for sensitivity analysis in regression models that does not require assumptions on the functional form of the treatment assignment mechanism nor on the distribution of the unobserved confounders, naturally handles multiple confounders, possibly acting non-linearly, exploits expert knowledge to bound sensitivity parameters and can be easily computed by using only standard regression results. In particular, we introduce two novel sensitivity measures suited for routine reporting. The robustness value describes the minimum strength of association that unobserved confounding would need to have, both with the treatment and with the outcome, to change the research conclusions. The partial R2 of the treatment with the outcome shows how strongly confounders explaining all the residual outcome variation would have to be associated with the treatment to eliminate the estimated effect. Next, we offer graphical tools for elaborating on problematic confounders, examining the sensitivity of point estimates and t-values, as well as ‘extreme scenarios’. Finally, we describe problems with a common ‘benchmarking’ practice and introduce a novel procedure to bound the strength of confounders formally on the basis of a comparison with observed covariates. We apply these methods to a running example that estimates the effect of exposure to violence on attitudes toward peace.},
	language = {en},
	number = {1},
	urldate = {2024-03-11},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Cinelli, Carlos and Hazlett, Chad},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12348},
	keywords = {sensitivity analysis},
	pages = {39--67},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\35MLEQSP\\Cinelli and Hazlett - 2020 - Making sense of sensitivity extending omitted var.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\AC9HDBI6\\rssb.html:text/html},
}

@article{hernan_does_2008,
	title = {Does obesity shorten life? {The} importance of well-defined interventions to answer causal questions},
	volume = {32},
	copyright = {2008 Macmillan Publishers Limited},
	issn = {1476-5497},
	shorttitle = {Does obesity shorten life?},
	url = {https://www.nature.com/articles/ijo200882},
	doi = {10.1038/ijo.2008.82},
	abstract = {Many observational studies have estimated a strong effect of obesity on mortality. In this paper, we explicitly define the causal question that is asked by these studies and discuss the problems associated with it. We argue that observational studies of obesity and mortality violate the condition of consistency of counterfactual (potential) outcomes, a necessary condition for meaningful causal inference, because (1) they do not explicitly specify the interventions on body mass index (BMI) that are being compared and (2) different methods to modify BMI may lead to different counterfactual mortality outcomes, even if they lead to the same BMI value in a given person. Besides precluding the estimation of unambiguous causal effects, this violation of consistency affects the ability to address two additional conditions that are also necessary for causal inference: exchangeability and positivity. We conclude that consistency violations not only preclude the estimation of well-defined causal effects but also compromise our ability to estimate ill-defined causal effects.},
	language = {en},
	number = {3},
	urldate = {2024-03-11},
	journal = {Int J Obes},
	author = {Hernán, M. A. and Taubman, S. L.},
	month = aug,
	year = {2008},
	note = {Publisher: Nature Publishing Group},
	keywords = {causal inference},
	pages = {S8--S14},
}

@article{bascoul-mollevi_longitudinal_2021,
	title = {Longitudinal analysis of health-related quality of life in cancer clinical trials: methods and interpretation of results},
	volume = {30},
	url = {https://link.springer.com/article/10.1007/s11136-020-02605-3},
	doi = {10.1007/S11136-020-02605-3/TABLES/3},
	abstract = {Purpose: Health-related quality of life (HRQoL) is assessed by self-administered questionnaires throughout the care process. Classically, two longitudinal statistical approaches were mainly used to study HRQoL: linear mixed models (LMM) or time-to-event models for time to deterioration/time until definitive deterioration (TTD/TUDD). Recently, an alternative strategy based on generalized linear mixed models for categorical data has also been proposed: the longitudinal partial credit model (LPCM). The objective of this article is to evaluate these methods and to propose recommendations to standardize longitudinal analysis of HRQoL data in cancer clinical trials. Methods: The three methods are first described and compared through statistical, methodological, and practical arguments, then applied on real HRQoL data from clinical cancer trials or published prospective databases. In total, seven French studies from a collaborating group were selected with longitudinal collection of QLQ-C30. Longitudinal analyses were performed with the three approaches using SAS, Stata and R software. Results: We observed concordant results between LMM and LPCM. However, discordant results were observed when we considered the TTD/TUDD approach compared to the two previous methods. According to methodological and practical arguments discussed, the approaches seem to provide additional information and complementary interpretations. LMM and LPCM are the most powerful methods on simulated data, while the TTD/TUDD approach gives more clinically understandable results. Finally, for single-item scales, LPCM is more appropriate. Conclusion: These results pledge for the recommendation to use of both the LMM and TTD/TUDD longitudinal methods, except for single-item scales, establishing them as the consensual methods for publications reporting HRQoL.},
	number = {1},
	journal = {Quality of Life Research},
	author = {Bascoul-Mollevi, Caroline and Barbieri, Antoine and Bourgier, Céline and Conroy, Thierry and Chauffert, Bruno and Hebbar, Mohamed and Jacot, William and Juzyna, Beata and De Forges, Hélène and Gourgou, Sophie and Bonnetain, Franck and Touraine, Célia and Anota, Amélie},
	month = jan,
	year = {2021},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH},
	keywords = {qol},
	pages = {91--103},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\HZZQ75ZD\\Bascoul-Mollevi et al. - 2021 - Longitudinal analysis of health-related quality of.pdf:application/pdf},
}

@article{little_prevention_2012,
	title = {The {Prevention} and {Treatment} of {Missing} {Data} in {Clinical} {Trials}},
	volume = {367},
	url = {https://www.nejm.org/doi/full/10.1056/nejmsr1203730},
	doi = {10.1056/NEJMSR1203730/SUPPL_FILE/NEJMSR1203730_DISCLOSURES.PDF},
	abstract = {Randomized clinical trials are the primary tool for evaluating new medical interventions. Randomization provides for a fair comparison between treatment and control groups, balancing out, on average, distributions of known and unknown factors among the participants. Unfortunately, these studies often lack a substantial percentage of data. This missing data reduces the benefit provided by the randomization and introduces potential biases in the comparison of the treatment groups. Missing data can arise for a variety of reasons, including the inability or unwillingness of participants to meet appointments for evaluation. And in some studies, some or all of data collection ceases when participants discontinue study treatment. Existing guidelines for the design and conduct of clinical trials, and the analysis of the resulting data, provide only limited advice on how to handle missing data. Thus, approaches to the analysis of data with an appreciable amount of missing values tend to be ad hoc and variable. The Prevention and Treatment of Missing Data in Clinical Trials concludes that a more principled approach to design and analysis in the presence of missing data is both needed and possible. Such an approach needs to focus on two critical elements: (1) careful design and conduct to limit the amount and impact of missing data and (2) analysis that makes full use of information on all randomized participants and is based on careful attention to the assumptions about the nature of the missing data underlying estimates of treatment effects. In addition to the highest priority recommendations, the book offers more detailed recommendations on the conduct of clinical trials and techniques for analysis of trial data.},
	number = {14},
	journal = {New England Journal of Medicine},
	author = {Little, Roderick J and D'Agostino, Ralph and Cohen, Michael L and Dickersin, Kay and Emerson, Scott S and Farrar, John T and Frangakis, Constantine and Hogan, Joseph W and Molenberghs, Geert and Murphy, Susan A and Neaton, James D and Rotnitzky, Andrea and Scharfstein, Daniel and Shih, Weichung J and Siegel, Jay P and Stern, Hal},
	month = oct,
	year = {2012},
	note = {Publisher: Massachusetts Medical Society},
	keywords = {missing data},
	pages = {1355--1360},
}

@article{leon_role_2011,
	title = {The {Role} and {Interpretation} of {Pilot} {Studies} in {Clinical} {Research}},
	volume = {45},
	url = {/pmc/articles/PMC3081994/},
	doi = {10.1016/J.JPSYCHIRES.2010.10.008},
	abstract = {Pilot studies represent a fundamental phase of the research process. The purpose of conducting a pilot study is to examine the feasibility of an approach that is intended to be used in a larger scale study. The roles and limitations of pilot studies are described here using a clinical trial as an example. A pilot study can be used to evaluate the feasibility of recruitment, randomization, retention, assessment procedures, new methods, and implementation of the novel intervention.A pilot study is not a hypothesis testing study. Safety, efficacy and effectiveness are not evaluated in a pilot. Contrary to tradition, a pilot study does not provide a meaningful effect size estimate for planning subsequent studies due to the imprecision inherent in data from small samples. Feasibility results do not necessarily generalize beyond the inclusion and exclusion criteria of the pilot design.A pilot study is a requisite initial step in exploring a novel intervention or an innovative application of an intervention. Pilot results can inform feasibility and identify modifications needed in the design of a larger, ensuing hypothesis testing study. Investigators should be forthright in stating these objectives of a pilot study. Grant reviewers and other stakeholders should expect no more. © 2010 Elsevier Ltd.},
	number = {5},
	journal = {Journal of psychiatric research},
	author = {Leon, Andrew C. and Davis, Lori L. and Kraemer, Helena C.},
	month = may,
	year = {2011},
	note = {Publisher: NIH Public Access},
	keywords = {trial design, pilot},
	pages = {626--626},
	file = {Accepted Version:C\:\\Users\\anbe6\\Zotero\\storage\\2BSPSY4E\\Leon et al. - 2011 - The Role and Interpretation of Pilot Studies in Cl.pdf:application/pdf},
}

@article{ozenne_riskregression_nodate,
	title = {{riskRegression}: {Predicting} the {Risk} of an {Event} using {Cox} {Regression} {Models}},
	abstract = {In the presence of competing risks a prediction of the time-dynamic absolute risk of an event can be based on cause-specific Cox regression models for the event and the competing risks (Benichou and Gail, 1990). We present computationally fast and memory optimized C++ functions with an R interface for predicting the covariate specific absolute risks, their confidence intervals, and their confidence bands based on right censored time to event data. We provide explicit formulas for our implementation of the estimator of the (stratified) baseline hazard function in the presence of tied event times. As a by-product we obtain fast access to the baseline hazards (compared to survival::basehaz()) and predictions of survival probabilities, their confidence intervals and confidence bands. Confidence intervals and confidence bands are based on point-wise asymptotic expansions of the corresponding statistical functionals. The software presented here is implemented in the riskRegression package.},
	author = {Ozenne, Brice and Sørensen, Anne Lyngholm and Scheike, Thomas and Torp-Pedersen, Christian and Gerds, Thomas Alexander},
	keywords = {R, survival analysis},
}

@article{cella_reliability_1995,
	title = {Reliability and validity of the functional assessment of cancer therapy—lung ({FACT}-{L}) quality of life instrument},
	volume = {12},
	doi = {10.1016/0169-5002(95)00450-F},
	abstract = {The FACT-L (version 3) is a 44-item self-report instrument which measures multidimensional quality of life. Available in eight languages, it is currently being used in several Phase II and III lung cancer clinical trials. Reliability and validity of the 33-item version 2 of the FACT-General (FACT-G) have previously been published. This paper reports further validation data on the FACT-G with a subsample of lung cancer patients from the original publication and, more importantly, presents data on the Lung Cancer Subscale (LCS). The nine LCS questions were administered along with the FACT-G to 116 patients with lung cancer. Internal consistency (coefficient alpha) was improved from 0.53 to 0.68 by dropping two questions which were uncorrelated with the others. A subset of 41 patients was tested again at 2 months to evaluate sensitivity to change in performance status rating (PSR) and to obtain estimates of a clinically meaningful change score for the FACT-G and the 7-item LCS. Using a linear test for trend, sensitivity to change in performance status rating (PSR) was obtained with the Total score (P = 0.03), the Physical Well Being (PWB) subscale (P = 0.02), the Functional Well Being (FWB) subscale (P = 0.05), and the LCS (P = 0.03). A 21-item Trial Outcome Index (TOI), combining scores on PWB, FWB and LCS, was highly reliable (coefficient a = 0.89) and sensitive to change in PSR F(1,38) = 4.84 (P = 0.01). This TOI is probably the most relevant and precise indicator of patient-reported quality of life available for lung cancer patients who complete the FACT-L while participating in an oncology clinical trial. The FACT-L may also be of benefit in evaluating quality of life in patients with lung diseases other than cancer. © 1995.},
	number = {3},
	journal = {Lung Cancer},
	author = {Cella, David F. and Bonomi, Amy E. and Lloyd, Stephen R. and Tulsky, David S. and Kaplan, Edward and Bonomi, Philip},
	month = jun,
	year = {1995},
	note = {Publisher: Elsevier},
	keywords = {qol, FACT-G, FACT-L},
	pages = {199--220},
}

@article{salkind_nuremberg_2012,
	title = {Nuremberg {Code}},
	volume = {313},
	doi = {10.4135/9781412961288.n281},
	number = {7070},
	journal = {Encyclopedia of Research Design},
	author = {Salkind, Neil},
	year = {2012},
	keywords = {GCP},
	pages = {1--28},
}

@techreport{rizopoulos_jm_2010,
	title = {{JM}: {An} {R} {Package} for the {Joint} {Modelling} of {Longitudinal} and {Time}-to-{Event} {Data}},
	url = {http://www.jstatsoft.org/},
	abstract = {In longitudinal studies measurements are often collected on different types of outcomes for each subject. These may include several longitudinally measured responses (such as blood values relevant to the medical condition under study) and the time at which an event of particular interest occurs (e.g., death, development of a disease or dropout from the study). These outcomes are often separately analyzed; however, in many instances, a joint modeling approach is either required or may produce a better insight into the mechanisms that underlie the phenomenon under study. In this paper we present the R package JM that fits joint models for longitudinal and time-to-event data.},
	author = {Rizopoulos, Dimitris},
	year = {2010},
	note = {Volume: 35},
	keywords = {R, survival analysis, mixed effect models},
}

@article{little_estimands_2021,
	title = {Estimands, {Estimators}, and {Estimates}},
	volume = {326},
	url = {https://jamanetwork.com/journals/jama/fullarticle/2783611},
	doi = {10.1001/JAMA.2021.2886},
	number = {10},
	journal = {JAMA},
	author = {Little, Roderick J and Lewis, Roger J},
	month = sep,
	year = {2021},
	note = {Publisher: American Medical Association},
	keywords = {estimands},
	pages = {967--968},
}

@article{lachin_evaluation_1986,
	title = {Evaluation of sample size and power for analyses of survival with allowance for nonuniform patient entry, losses to follow-up, noncompliance, and stratification},
	volume = {42},
	number = {3},
	journal = {Biometrics},
	author = {Lachin, John M. and Foulkes, Mary A.},
	year = {1986},
	keywords = {sample size, power},
	pages = {507--519},
}

@techreport{kenneth_belmont_1979,
	title = {The {Belmont} {Report}},
	url = {https://www.hhs.gov/ohrp/sites/default/files/the-belmont-report-508c_FINAL.pdf},
	author = {Kenneth, Ryan John},
	year = {1979},
	doi = {10.1021/bi00780a005},
	keywords = {GCP},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\BZM6M64A\\Kenneth - 1979 - The Belmont Report.pdf:application/pdf},
}

@article{hegi_mgmt_2005,
	title = {{MGMT} {Gene} {Silencing} and {Benefit} from {Temozolomide} in {Glioblastoma}},
	volume = {352},
	url = {https://www.nejm.org/doi/10.1056/NEJMoa043331},
	doi = {10.1056/NEJMOA043331/SUPPL_FILE/997SA1.PDF},
	abstract = {BACKGROUND: Epigenetic silencing of the MGMT (O6-methylguanine- DNA methyltransferase) DNA-repair gene by promoter methylation compromises DNA repair and has been associated with longer survival in patients with glioblastoma who receive alkylating agents. METHODS: We tested the relationship between MGMT silencing in the tumor and the survival of patients who were enrolled in a randomized trial comparing radiotherapy alone with radiotherapy combined with concomitant and adjuvant treatment with temozolomide. The methylation status of the MGMT promoter was determined by methylation-specific polymerase-chain-reaction analysis. RESULTS: The MGMT promoter was methylated in 45 percent of 206 assessable cases. Irrespective of treatment, MGMT promoter methylation was an independent favorable prognostic factor (P{\textless}0.001 by the log-rank test; hazard ratio, 0.45; 95 percent confidence interval, 0.32 to 0.61). Among patients whose tumor contained a methylated MGMT promoter, a survival benefit was observed in patients treated with temozolomide and radiotherapy; their median survival was 21.7 months (95 percent confidence interval, 17.4 to 30.4), as compared with 15.3 months (95 percent confidence interval, 13.0 to 20.9) among those who were assigned to only radiotherapy (P=0.007 by the log-rank test). In the absence of methylation of the MGMT promoter, there was a smaller and statistically insignificant difference in survival between the treatment groups. CONCLUSIONS: Patients with glioblastoma containing a methylated MGMT promoter benefited from temozolomide, whereas those who did not have a methylated MGMT promoter did not have such a benefit. Copyright © 2005 Massachusetts Medical Society.},
	number = {10},
	journal = {New England Journal of Medicine},
	author = {Hegi, Monika E. and Diserens, Annie-Claire and Gorlia, Thierry and Hamou, Marie-France and de Tribolet, Nicolas and Weller, Michael and Kros, Johan M. and Hainfellner, Johannes A. and Mason, Warren and Mariani, Luigi and Bromberg, Jacoline E.C. and Hau, Peter and Mirimanoff, René O. and Cairncross, J. Gregory and Janzer, Robert C. and Stupp, Roger},
	month = mar,
	year = {2005},
	note = {Publisher: New England Journal of Medicine (NEJM/MMS)},
	keywords = {glioblastoma},
	pages = {997--1003},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\MIVQX538\\Hegi et al. - 2005 - MGMT Gene Silencing and Benefit from Temozolomide .pdf:application/pdf},
}

@article{ene-iordache_developing_2009,
	title = {Developing {Regulatory}-compliant {Electronic} {Case} {Report} {Forms} for {Clinical} {Trials}: {Experience} with {The} {Demand} {Trial}},
	volume = {16},
	url = {/pmc/articles/PMC2732224/},
	doi = {10.1197/JAMIA.M2787},
	abstract = {The use of electronic case report forms (CRF) to gather data in randomized clinical trials has grown to progressively replace paper-based forms. Computerized form designs must ensure the same data quality expected of paper CRF, by following Good Clinical Practice rules. Electronic data capture (EDC) tools must also comply with applicable statutory and regulatory requirements. Here the authors focus on the development of computerized systems for clinical trials implementing FDA and EU recommendations and regulations, and describe a laptop-based electronic CRF used in a randomized, multicenter clinical trial. © 2009 J Am Med Inform Assoc.},
	number = {3},
	journal = {Journal of the American Medical Informatics Association : JAMIA},
	author = {Ene-Iordache, Bogdan and Carminati, Sergio and Antiga, Luca and Rubis, Nadia and Ruggenenti, Piero and Remuzzi, Giuseppe and Remuzzi, Andrea},
	month = may,
	year = {2009},
	note = {Publisher: Oxford University Press},
	keywords = {trial design},
	pages = {404--404},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\MR4S285P\\Ene-Iordache et al. - 2009 - Developing Regulatory-compliant Electronic Case Re.pdf:application/pdf},
}

@article{ema_ich_2006,
	title = {{ICH} {Topic} {E} 9 {Statistical} {Principles} for {Clinical} {Trials}},
	volume = {\&NA;},
	doi = {10.2165/00128413-200615320-00004},
	abstract = {ICH Guideline},
	number = {1532},
	journal = {Inpharma Weekly},
	author = {{EMA}},
	year = {2006},
	keywords = {ICH, GCP},
	pages = {3--3},
}

@article{butt_quality_2005,
	title = {Quality of {Life} in {Lung} {Cancer}: {The} {Validity} and {Cross}-{Cultural} {Applicability} of the {Functional} {Assessment} of {Cancer} {Therapy}–{Lung} {Scale}},
	volume = {19},
	doi = {10.1016/J.HOC.2005.02.009},
	abstract = {This article discusses the validity and cross-cultural applicability of the Functional Assessment of Cancer Therapy-Lung scale, a self-report instrument that measures multidimensional quality of life. © 2005 Elsevier Inc. All rights reserved.},
	number = {2},
	journal = {Hematology/Oncology Clinics of North America},
	author = {Butt, Zeeshan and Webster, Kimberly and Eisenstein, Amy R. and Beaumont, Jennifer and Eton, David and Masters, Gregory A. and Cella, David},
	month = apr,
	year = {2005},
	note = {Publisher: Elsevier},
	keywords = {qol},
	pages = {389--420},
}

@article{clark_survival_2003,
	title = {Survival {Analysis} {Part} {I}: {Basic} concepts and first analyses},
	volume = {89},
	url = {www.bjcancer.com},
	doi = {10.1038/sj.bjc.6601118},
	journal = {British Journal of Cancer},
	author = {Clark, T G and Bradburn, M J and Love, S B and Altman, D G},
	year = {2003},
	keywords = {survival analysis},
	pages = {232--238},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\2MPZYKGP\\Clark et al. - 2003 - Survival Analysis Part I Basic concepts and first.pdf:application/pdf},
}

@article{goodyear_declaration_2007,
	title = {The declaration of {Helsinki}},
	volume = {335},
	url = {https://www.bmj.com/content/335/7621/624},
	doi = {10.1136/bmj.39339.610000.BE},
	abstract = {Mosaic tablet, dynamic document, or dinosaur? The Declaration of Helsinki is a respected institution and one of the most influential documents in research ethics,1w1-w7 having withstood five revisions and two clarifications since its conception in 1964. Its guardian, the World Medical Association, recently invited submissions for further revision.2 The history of the declaration has been well documented.3 4 5 The Nuremberg Code (1947) was one of the first statements of the ethical principles involved in human experimentation.w8 However, because of its association with Nazi war crimes it had relatively little effect on practice.w9 The Declaration of Helsinki dealt with clinical research more directly, but was portrayed as a weakening of the stringent protections of Nuremberg. Nevertheless, for a quarter of a century only minor changes were made and it became engrained in the international culture of research ethics. In 1996, the declaration added a reference to placebos in response to concerns about trials in perinatal HIV transmission in developing countries . Critics pointed out that continuing to use placebos when efficacy had been demonstrated implied a different ethical standard for developing countries than for developed ones. Having entered into the specifics of trial design the declaration was drawn into a debate on whether ethical principles are …},
	number = {7621},
	journal = {British Medical Journal},
	author = {Goodyear, Michael D E and Krleza-Jeric, Karmela and Lemmens, Trudo},
	month = sep,
	year = {2007},
	note = {Publisher: British Medical Journal Publishing Group},
	keywords = {GCP},
	pages = {624--625},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\5SN8DEB6\\Goodyear et al. - 2007 - The declaration of Helsinki.pdf:application/pdf},
}

@article{kang_incorporating_2022,
	title = {Incorporating estimands into clinical trial statistical analysis plans},
	volume = {19},
	url = {https://pubmed.ncbi.nlm.nih.gov/35257600/},
	doi = {10.1177/17407745221080463},
	abstract = {Background: International Council for Harmonisation (ICH) E9 Statistical Principles for Clinical Trials was developed as a consensus guidance document to encourage worldwide harmonization of the principles of statistical methodology in clinical trials. Addendum E9 (R1) clarified and extended ICH E9 with a focus on estimands and sensitivity analyses. Since the release of E9 (R1), clinical trial protocols have included estimands, but there is variation in how they are presented. Statistical analysis plans (SAPs) are increasingly becoming publicly available (e.g. posting on ClinicalTrials.gov) and present an opportunity to link estimands with planned analyses to present the alignment of trial objectives, design, conduct, and analysis. Methods: A table format was used to create a template for inclusion in SAPs that satisfies ICH E9 (R1) guidance to align statistical analysis to the estimand. The template provides a consistent structure for presentation of estimands and the associated analysis, and is applicable to a wide range of trial designs. We illustrate use of the template with a hypothetical clinical trial in HIV-1. Results: The estimand-to-analysis table template starts with the study objective describing the clinical question of interest as written in the trial protocol. The remainder of the table describes each attribute of the estimand (treatment, target population, variable, intercurrent events, and population-level summary) in the left column (ESTIMAND), while the right column describes how each attribute will be handled using the data collected in the clinical trial (ANALYSIS). The template was applied to a hypothetical, early-phase single-arm trial, modeled after a pediatric trial in HIV, where the objective was to determine the safety of a new antiretroviral drug as part of a combination antiretroviral treatment regimen in the pediatric population. Three intercurrent events were illustrated in the table: death, premature treatment discontinuation before 24 weeks, and pregnancy. An estimand-to-analysis table from a grant application that addresses the primary objective of a placebo-controlled randomized trial is also presented to demonstrate an alternative usage. Conclusion: We found the template to be useful in study design, providing a snapshot of the objective, target population, potential intercurrent events, analysis plan, and considerations for missing data in one place and facilitating discussion among stakeholders. The proposed standardized presentation of estimand attributes and analysis considerations in SAPs will provide guidance to SAP authors and consistency across studies to facilitate reviews.},
	number = {3},
	journal = {Clinical Trials},
	author = {Kang, Minhee and Kendall, Michelle A and Ribaudo, Heather and Tierney, Camlin and Zheng, Lu and Smeaton, Laura and Lindsey, Jane C},
	month = jun,
	year = {2022},
	note = {Publisher: Clin Trials},
	keywords = {estimands, ICH, statistical analysis plan},
	pages = {285--291},
	file = {Accepted Version:C\:\\Users\\anbe6\\Zotero\\storage\\XXYXIQKV\\Kang et al. - 2022 - Incorporating estimands into clinical trial statis.pdf:application/pdf},
}

@article{flight_this_2016,
	title = {This is a repository copy of {Practical} guide to sample size calculations: non-inferiority and equivalence trials. {Practical} guide to sample size calculations: non-inferiority and equivalence trials},
	url = {http://eprints.whiterose.ac.uk/97113/},
	doi = {10.1002/pst.1716},
	abstract = {Article: Flight, L. orcid.org/0000-0002-9569-8290 and Julious, S.A. (2016) Practical guide to sample size calculations: non-inferiority and equivalence trials. Pharmaceutical Statistics, 15 (1). This is the peer reviewed version of the following article: Flight, L., and Julious, S. A. (2016) Practical guide to sample size calculations: non-inferiority and equivalence trials. Pharmaceut. Statist., 15: 80-89. doi: 10.1002/pst.1716., which has been published in final form at http://dx. Abstract A sample size justification is a vital part of any trial design. However, estimating the number of participants required to give a meaningful result is not always straightforward. A number of components are required to facilitate a suitable sample size calculation. In this paper, the steps for conducting sample size calculations for non-inferiority and equivalence trials are summarised. Practical advice and examples are provided that illustrate how to carry out the calculations by hand and using the app SampSize.},
	author = {Flight, Laura and Julious, Steven A},
	year = {2016},
	keywords = {sample size, non-inferiority, power},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\B86K22CQ\\Flight and Julious - 2016 - This is a repository copy of Practical guide to sa.pdf:application/pdf},
}

@article{barbagallo_long-term_2014,
	title = {Long-term therapy with temozolomide is a feasible option for newly diagnosed glioblastoma: {A} single-institution experience with as many as 101 temozolomide cycles},
	volume = {37},
	doi = {10.3171/2014.9.FOCUS14502},
	abstract = {Object. The objective of this study was to report the authors' experience with the long-term administration of temozolomide (TMZ; {\textgreater} 6 cycles, up to 101) in patients with newly diagnosed glioblastoma and to analyze its feasibility and safety as well as its impact on survival. The authors also compared data obtained from the group of patients undergoing long-term TMZ treatment with data from patients treated with a standard TMZ protocol. Methods. A retrospective analysis was conducted of 37 patients who underwent operations for glioblastoma between 2004 and 2012. Volumetric analysis of postoperative Gd-enhanced MR images, obtained within 48 hours, confirmed tumor gross-total resection (GTR) in all but 2 patients. All patients received the first cycle of TMZ at a dosage of 150 mg/m2 starting on the second or third postsurgical day. Afterward, patients received concomitant radiochemotherapy according to the Stupp protocol. With regard to adjuvant TMZ therapy, the 19 patients in Group A, aged 30-72 years (mean 56.1 years), received 150 mg/m2 for 5 days every 28 days for more than 6 cycles (range 7-101 cycles). The 18 patients in Group B, aged 46-82 years (mean 64.8 years), received the same dose, but for no more than 6 cycles. O6-methylguanine-DNA methyltransferase (MGMT) promoter methylation status was analyzed for both groups and correlated with overall survival (OS) and progression-free survival (PFS). The impact of age, sex, Karnofsky Performance Scale score, and Ki 67 staining were also considered. Results. All patients but 1 in Group A survived at least 18 months (range 18-101 months), and patients in Group B survived no more than 17 months (range 2-17 months). The long-term survivors (Group A), defined as patients who survived at least 12 months after diagnosis, were 51.3\% of the total (19/37). Kaplan-Meier curve analysis showed that patients treated with more than 6 TMZ cycles had OS and PFS that was significantly longer than patients receiving standard treatment (median OS 28 months vs 8 months, respectively; p = 0.0001; median PFS 20 months vs 4 months, respectively; p = 0.0002). By univariate and multivariate Cox proportional hazard regression analysis, MGMT methylation status and number of TMZ cycles appeared to be survival prognostic factors in patients with glioblastoma. After controlling for MGMT status, highly significant differences related to OS and PFS between patients with standard and long-term TMZ treatment were still detected. Furthermore, in Group A and B, the statistical correlation of MGMT status to the number of TMZ cycles showed a significant difference only in Group A patients, suggesting that MGMT promoter methylation was predictive of response for long-term TMZ treatment. Prolonged therapy did not confer hematological toxicity or opportunistic infections in either patient group. Conclusions. This study describes the longest experience so far reported with TMZ in patients with newly diagnosed glioblastomas, with as many as 101 cycles, who were treated using GTR. Statistically significant data confirm that median survival correlates with MGMT promoter methylation status as well as with the number of TMZ cycles administered. Long-term TMZ therapy appears feasible and safe.},
	number = {6},
	journal = {Neurosurgical Focus},
	author = {Barbagallo, Giuseppe M.V. and Paratore, Sabrina and Caltabiano, Rosario and Palmucci, Stefano and Parra, Hector Soto and Privitera, Giuseppe and Motta, Fabio and Lanzafame, Salvatore and Scaglione, Giorgio and Longo, Antonio and Albanese, Vincenzo and Certo, Francesco},
	year = {2014},
	note = {Publisher: American Association of Neurological Surgeons},
	keywords = {glioblastoma},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\EJ2GSHIF\\Barbagallo et al. - 2014 - Long-term therapy with temozolomide is a feasible .pdf:application/pdf},
}

@article{wendelberger_futility_2023,
	title = {Futility in {Clinical} {Trials}},
	volume = {330},
	url = {https://jamanetwork.com/journals/jama/fullarticle/2808097},
	doi = {10.1001/JAMA.2023.14111},
	number = {8},
	journal = {JAMA},
	author = {Wendelberger, Barbara and Lewis, Roger J.},
	month = aug,
	year = {2023},
	note = {Publisher: American Medical Association},
	keywords = {futility},
	pages = {764--765},
}

@article{balana_phase_2020,
	title = {A phase {II} randomized, multicenter, open-label trial of continuing adjuvant temozolomide beyond 6 cycles in patients with glioblastoma ({GEINO} 14-01)},
	volume = {22},
	doi = {10.1093/neuonc/noaa107},
	abstract = {Background: Standard treatment for glioblastoma is radiation with concomitant and adjuvant temozolomide for 6 cycles, although the optimal number of cycles of adjuvant temozolomide has long been a subject of debate. We performed a phase II randomized trial investigating whether extending adjuvant temozolomide for more than 6 cycles improved outcome. Methods: Glioblastoma patients treated at 20 Spanish hospitals who had not progressed after 6 cycles of adjuvant temozolomide were centrally randomized to stop (control arm) or continue (experimental arm) temozolomide up to a total of 12 cycles at the same doses they were receiving in cycle 6. Patients were stratified by MGMT methylation and measurable disease. The primary endpoint was differences in 6-month progression-free survival (PFS). Secondary endpoints were PFS, overall survival (OS), and safety (Clinicaltrials.gov NCT02209948). Results: From August 2014 to November 2018, 166 patients were screened, 7 of whom were ineligible. Seventy-nine patients were included in the stop arm and 80 in the experimental arm. All patients were included in the analyses of outcomes and of safety. There were no differences in 6-month PFS (control 55.7\%; experimental 61.3\%), PFS, or OS between arms. MGMT methylation and absence of measurable disease were independent factors of better outcome. Patients in the experimental arm had more lymphopenia (P {\textless} 0.001), thrombocytopenia (P {\textless} 0.001), and nausea and vomiting (P = 0.001). Conclusions: Continuing temozolomide after 6 adjuvant cycles is associated with greater toxicity but confers no additional benefit in 6-month PFS. Key Points: 1. Extending adjuvant temozolomide to 12 cycles did not improve 6-month PFS. 2. Extending adjuvant temozolomide did not improve PFS or OS in any patient subset. 3. Extending adjuvant temozolomide was linked to increased toxicities.},
	number = {12},
	journal = {Neuro-Oncology},
	author = {Balana, Carmen and Vaz, Maria Angeles and Sepúlveda, Juan Manuel and Mesia, Carlos and Del Barco, Sonia and Pineda, Estela and Muñoz-Langa, Jose and Estival, Anna and De Las Peñas, Ramón and Fuster, Jose and Gironés, Regina and Navarro, Luis Miguel and Gil-Gil, Miguel and Alonso, Miriam and Herrero, Ana and Peralta, Sergio and Olier, Clara and Perez-Segura, Pedro and Covela, Maria and Martinez-Garciá, Maria and Berrocal, Alfonso and Gallego, Oscar and Luque, Raquel and Perez-Martín, Franciso Javier and Esteve, Anna and Munne, Nuria and Domenech, Marta and Villa, Salvador and Sanz, Carolina and Carrato, Cristina},
	month = dec,
	year = {2020},
	note = {Publisher: Oxford University Press},
	keywords = {glioblastoma},
	pages = {1851--1861},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\JLF8JMPB\\Balana et al. - 2020 - A phase II randomized, multicenter, open-label tri.pdf:application/pdf},
}

@article{saville_efficiencies_2016,
	title = {Efficiencies of platform clinical trials: {A} vision of the future},
	volume = {13},
	doi = {10.1177/1740774515626362},
	abstract = {Background: A "platform trial" is a clinical trial with a single master protocol in which multiple treatments are evaluated simultaneously. Adaptive platform designs offer flexible features such as dropping treatments for futility, declaring one or more treatments superior, or adding new treatments to be tested during the course of a trial. Methods: A simulation study explores the efficiencies of various platform trial designs relative to a traditional two-arm strategy. Results: Platform trials can find beneficial treatments with fewer patients, fewer patient failures, less time, and with greater probability of success than a traditional two-arm strategy. Conclusion: In an era of personalized medicine, platform trials provide the innovation needed to efficiently evaluate modern treatments.},
	number = {3},
	journal = {Clinical Trials},
	author = {Saville, Benjamin R. and Berry, Scott M.},
	month = jun,
	year = {2016},
	note = {Publisher: SAGE Publications Ltd},
	keywords = {trial design},
	pages = {358--366},
}

@article{stupp_effect_2017,
	title = {Effect of {Tumor}-{Treating} {Fields} {Plus} {Maintenance} {Temozolomide} vs {Maintenance} {Temozolomide} {Alone} on {Survival} in {Patients} {With} {Glioblastoma}: {A} {Randomized} {Clinical} {Trial}},
	volume = {318},
	url = {https://jamanetwork.com/journals/jama/fullarticle/2666504},
	doi = {10.1001/JAMA.2017.18718},
	abstract = {{\textless}h3{\textgreater}Importance{\textless}/h3{\textgreater}{\textless}p{\textgreater}Tumor-treating fields (TTFields) is an antimitotic treatment modality that interferes with glioblastoma cell division and organelle assembly by delivering low-intensity alternating electric fields to the tumor.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Objective{\textless}/h3{\textgreater}{\textless}p{\textgreater}To investigate whether TTFields improves progression-free and overall survival of patients with glioblastoma, a fatal disease that commonly recurs at the initial tumor site or in the central nervous system.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Design, Setting, and Participants{\textless}/h3{\textgreater}{\textless}p{\textgreater}In this randomized, open-label trial, 695 patients with glioblastoma whose tumor was resected or biopsied and had completed concomitant radiochemotherapy (median time from diagnosis to randomization, 3.8 months) were enrolled at 83 centers (July 2009-2014) and followed up through December 2016. A preliminary report from this trial was published in 2015; this report describes the final analysis.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Interventions{\textless}/h3{\textgreater}{\textless}p{\textgreater}Patients were randomized 2:1 to TTFields plus maintenance temozolomide chemotherapy (n = 466) or temozolomide alone (n = 229). The TTFields, consisting of low-intensity, 200 kHz frequency, alternating electric fields, was delivered (≥ 18 hours/d) via 4 transducer arrays on the shaved scalp and connected to a portable device. Temozolomide was administered to both groups (150-200 mg/m$^{\textrm{2}}$) for 5 days per 28-day cycle (6-12 cycles).{\textless}/p{\textgreater}{\textless}h3{\textgreater}Main Outcomes and Measures{\textless}/h3{\textgreater}{\textless}p{\textgreater}Progression-free survival (tested at α = .046). The secondary end point was overall survival (tested hierarchically at α = .048). Analyses were performed for the intent-to-treat population. Adverse events were compared by group.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Results{\textless}/h3{\textgreater}{\textless}p{\textgreater}Of the 695 randomized patients (median age, 56 years; IQR, 48-63; 473 men [68\%]), 637 (92\%) completed the trial. Median progression-free survival from randomization was 6.7 months in the TTFields-temozolomide group and 4.0 months in the temozolomide-alone group (HR, 0.63; 95\% CI, 0.52-0.76;\textit{P} \&lt; .001). Median overall survival was 20.9 months in the TTFields-temozolomide group vs 16.0 months in the temozolomide-alone group (HR, 0.63; 95\% CI, 0.53-0.76;\textit{P} \&lt; .001). Systemic adverse event frequency was 48\% in the TTFields-temozolomide group and 44\% in the temozolomide-alone group. Mild to moderate skin toxicity underneath the transducer arrays occurred in 52\% of patients who received TTFields-temozolomide vs no patients who received temozolomide alone.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Conclusions and Relevance{\textless}/h3{\textgreater}{\textless}p{\textgreater}In the final analysis of this randomized clinical trial of patients with glioblastoma who had received standard radiochemotherapy, the addition of TTFields to maintenance temozolomide chemotherapy vs maintenance temozolomide alone, resulted in statistically significant improvement in progression-free survival and overall survival. These results are consistent with the previous interim analysis.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Trial Registration{\textless}/h3{\textgreater}{\textless}p{\textgreater}clinicaltrials.gov Identifier:NCT00916409{\textless}/p{\textgreater}},
	number = {23},
	journal = {JAMA},
	author = {Stupp, Roger and Taillibert, Sophie and Kanner, Andrew and Read, William and Steinberg, David M. and Lhermitte, Benoit and Toms, Steven and Idbaih, Ahmed and Ahluwalia, Manmeet S. and Fink, Karen and Di Meco, Francesco and Lieberman, Frank and Zhu, Jay Jiguang and Stragliotto, Giuseppe and Tran, David D. and Brem, Steven and Hottinger, Andreas F. and Kirson, Eilon D. and Lavy-Shahaf, Gitit and Weinberg, Uri and Kim, Chae Yong and Paek, Sun Ha and Nicholas, Garth and Burna, Jordi and Hirte, Hal and Weller, Michael and Palti, Yoram and Hegi, Monika E. and Ram, Zvi},
	month = dec,
	year = {2017},
	note = {Publisher: American Medical Association},
	keywords = {glioblastoma},
	pages = {2306--2316},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\HE2BKMSW\\Stupp et al. - 2017 - Effect of Tumor-Treating Fields Plus Maintenance T.pdf:application/pdf;joi170143supp1_prod.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\3DSVM4UF\\joi170143supp1_prod.pdf:application/pdf},
}

@article{cannon_ezetimibe_2015,
	title = {Ezetimibe {Added} to {Statin} {Therapy} after {Acute} {Coronary} {Syndromes}},
	volume = {372},
	doi = {10.1056/nejmoa1410489},
	abstract = {BACKGROUND Statin therapy reduces low-density lipoprotein (LDL) cholesterol levels and the risk of cardiovascular events, but whether the addition of ezetimibe, a nonstatin drug that reduces intestinal cholesterol absorption, can reduce the rate of cardiovascular events further is not known. METHODS We conducted a double-blind, randomized trial involving 18,144 patients who had been hospitalized for an acute coronary syndrome within the preceding 10 days and had LDL cholesterol levels of 50 to 100 mg per deciliter (1.3 to 2.6 mmol per liter) if they were receiving lipid-lowering therapy or 50 to 125 mg per deciliter (1.3 to 3.2 mmol per liter) if they were not receiving lipid-lowering therapy. The combination of simvastatin (40 mg) and ezetimibe (10 mg) (simvastatin-ezetimibe) was compared with simvastatin (40 mg) and placebo (simvastatin monotherapy). The primary end point was a composite of cardiovascular death, nonfatal myocardial infarction, unstable angina requiring rehospitalization, coronary revascularization (≥30 days after randomization), or nonfatal stroke. The median follow-up was 6 years. RESULTS The median time-weighted average LDL cholesterol level during the study was 53.7 mg per deciliter (1.4 mmol per liter) in the simvastatin-ezetimibe group, as compared with 69.5 mg per deciliter (1.8 mmol per liter) in the simvastatin-monotherapy group (P{\textless}0.001). The Kaplan-Meier event rate for the primary end point at 7 years was 32.7\% in the simvastatin-ezetimibe group, as compared with 34.7\% in the simvastatin-monotherapy group (absolute risk difference, 2.0 percentage points; hazard ratio, 0.936; 95\% confidence interval, 0.89 to 0.99; P=0.016). Rates of prespecified muscle, gallbladder, and hepatic adverse effects and cancer were similar in the two groups. CONCLUSIONS When added to statin therapy, ezetimibe resulted in incremental lowering of LDL cholesterol levels and improved cardiovascular outcomes. Moreover, lowering LDL cholesterol to levels below previous targets provided additional benefit. (Funded by Merck; IMPROVE-IT ClinicalTrials.gov number, NCT00202878.).},
	number = {25},
	journal = {New England Journal of Medicine},
	author = {Cannon, Christopher P and Blazing, Michael A and Giugliano, Robert P and McCagg, Amy and White, Jennifer A and Theroux, Pierre and Darius, Harald and Lewis, Basil S and Ophuis, Ton Oude and Jukema, J Wouter and De Ferrari, Gaetano M and Ruzyllo, Witold and De Lucca, Paul and Im, KyungAh and Bohula, Erin A and Reist, Craig and Wiviott, Stephen D and Tershakovec, Andrew M and Musliner, Thomas A and Braunwald, Eugene and Califf, Robert M},
	year = {2015},
	pages = {2387--2397},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\MTID2FFJ\\Cannon et al. - 2015 - Ezetimibe Added to Statin Therapy after Acute Coro.pdf:application/pdf},
}

@article{sjoberg_reproducible_2021,
	title = {Reproducible {Summary} {Tables} with the gtsummary {Package}},
	volume = {13},
	url = {https://doi.org/10.32614/RJ-2021-053},
	doi = {10.32614/RJ-2021-053},
	number = {1},
	journal = {The R Journal},
	author = {Sjoberg, Daniel D. and Whiting, Karissa and Curry, Michael and Lavery, Jessica A. and Larmarange, Joseph},
	year = {2021},
	pages = {570--580},
}

@article{hassett_efficacy_2024,
	title = {Efficacy of {eSyM}: {Acute} care utilization among patients with cancer who do versus do not report {ePROs}.},
	volume = {42},
	issn = {0732-183X},
	shorttitle = {Efficacy of {eSyM}},
	url = {https://ascopubs.org/doi/abs/10.1200/JCO.2024.42.16_suppl.11001},
	doi = {10.1200/JCO.2024.42.16_suppl.11001},
	abstract = {11001
Background: Patients (pts) receiving cancer treatment frequently experience burdensome symptoms that compromise outcomes and necessitate acute care. Prior clinical trials have demonstrated that electronic patient-reported outcome (ePRO)-based symptom management programs improve outcomes in controlled settings. Deploying these programs in routine care settings remains challenging. With funding from the Cancer Moonshot IMPACT Consortium, we created eSyM – an ePRO-based, EHR-integrated symptom management program – to facilitate the widespread adoption of active symptom management efforts. Methods: eSyM was deployed across six health systems from September 2019-August 2022 via a modified stepped-wedge cluster randomized pragmatic trial. Pts starting chemotherapy (CHEM) or undergoing surgery (SURG) for a suspected or confirmed thoracic, gastrointestinal, or gynecologic cancer were prompted to complete symptom questionnaires regularly; those reporting symptoms were offered additional supports. To assess eSyM efficacy, we studied pts who were eligible to use the program – comparing those who completed at least one symptom questionnaire to those who did not. Outcomes included emergency department (ED) visits and inpatient encounters (INPT) at 30 and 90-days. Odds ratios with 95\% CIs were derived after adjusting to account for the propensity to report ePROs as a function of age, sex, race/ethnicity, employment, marital status, poverty, rurality, insurance, comorbidity, cancer, treatment goal, institution, and calendar time. Results: Among eSyM-eligible pts, 51\% (N = 10,454/20,471) completed at least one symptom questionnaire (median 4 reports/patient) – 47\% (3815/8187) for CHEM and 54\% (6639/12,293) for SURG. Comparing symptom reporters to non-reporters, the proportion of CHEM+SURG pts experiencing an ED event was 5.3\% vs. 7.1\% at 30 days and 10.0\% vs. 12.9\% at 90 days; and the proportion experiencing an INPT event was 6.7\% vs. 11.3\% at 30 days and 14.0\% vs. 19.5\% at 90 days (p{\textless} 0.001 for all). Adjusted ORs appear in the. Conclusions: After accounting for propensity to report symptoms, completing at least one symptom questionnaire was associated with lower odds of experiencing an ED or INPT encounter among CHEM and SURG pts across six diverse health systems. eSyM engagement reduced acute care utilization. This EHR-integrated symptom management solution is broadly available to health systems that use Epic. Clinical trial information: NCT03850912.
Adjusted ORs – symptom reporters vs. non-reporters (95\% CI; P value).
Outcome	Days	CHEM + SURG	CHEM Only	SURG Only
ED	30	0.859
(0.793-0.932; p {\textless} 0.001)	0.833
(0.740-0.938; p = 0.003)	0.881
(0.789-0.984; p = 0.024)
 	90	0.907
(0.853-0.963; p = 0.002)	0.905
(0.829-0.988; p = 0.027)	0.905
(0.832-0.985; p = 0.021)
INPT	30	0.653
(0.609-0.700; p {\textless} 0.001)	0.648
(0.584-0.718; p {\textless} 0.001)	0.656
(0.598-0.721; p {\textless} 0.001)
 	90	0.780
(0.740-0.822; p {\textless} 0.001)	0.813
(0.755-0.876; p {\textless} 0.001)	0.742
(0.689-0.800; p {\textless} 0.001)
EXPAND TABLE
OPEN IN VIEWER},
	number = {16\_suppl},
	urldate = {2025-04-11},
	journal = {JCO},
	author = {Hassett, Michael J. and Uno, Hajime and Tramontano, Angela and Cronin, Christine and Bian, Jessica J and Dizon, Don S. and Hazard-Jenkins, Hannah W. and Osarogiagbon, Raymond U. and Wong, Sandra L. and Schrag, Deb},
	month = jun,
	year = {2024},
	note = {Publisher: Wolters Kluwer},
	keywords = {FLASCO esym},
	pages = {11001--11001},
}

@article{cella_patient-reported_2010,
	title = {The {Patient}-{Reported} {Outcomes} {Measurement} {Information} {System} ({PROMIS}) developed and tested its first wave of adult self-reported health outcome item banks: 2005–2008},
	volume = {63},
	issn = {0895-4356},
	shorttitle = {The {Patient}-{Reported} {Outcomes} {Measurement} {Information} {System} ({PROMIS}) developed and tested its first wave of adult self-reported health outcome item banks},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435610001733},
	doi = {10.1016/j.jclinepi.2010.04.011},
	abstract = {Objectives
Patient-reported outcomes (PROs) are essential when evaluating many new treatments in health care; yet, current measures have been limited by a lack of precision, standardization, and comparability of scores across studies and diseases. The Patient-Reported Outcomes Measurement Information System (PROMIS) provides item banks that offer the potential for efficient (minimizes item number without compromising reliability), flexible (enables optional use of interchangeable items), and precise (has minimal error in estimate) measurement of commonly studied PROs. We report results from the first large-scale testing of PROMIS items.
Study Design and Setting
Fourteen item pools were tested in the U.S. general population and clinical groups using an online panel and clinic recruitment. A scale-setting subsample was created reflecting demographics proportional to the 2000 U.S. census.
Results
Using item-response theory (graded response model), 11 item banks were calibrated on a sample of 21,133, measuring components of self-reported physical, mental, and social health, along with a 10-item Global Health Scale. Short forms from each bank were developed and compared with the overall bank and with other well-validated and widely accepted (“legacy”) measures. All item banks demonstrated good reliability across most of the score distributions. Construct validity was supported by moderate to strong correlations with legacy measures.
Conclusion
PROMIS item banks and their short forms provide evidence that they are reliable and precise measures of generic symptoms and functional reports comparable to legacy instruments. Further testing will continue to validate and test PROMIS items and banks in diverse clinical populations.},
	number = {11},
	urldate = {2025-04-04},
	journal = {Journal of Clinical Epidemiology},
	author = {Cella, David and Riley, William and Stone, Arthur and Rothrock, Nan and Reeve, Bryce and Yount, Susan and Amtmann, Dagmar and Bode, Rita and Buysse, Daniel and Choi, Seung and Cook, Karon and DeVellis, Robert and DeWalt, Darren and Fries, James F. and Gershon, Richard and Hahn, Elizabeth A. and Lai, Jin-Shei and Pilkonis, Paul and Revicki, Dennis and Rose, Matthias and Weinfurt, Kevin and Hays, Ron},
	month = nov,
	year = {2010},
	keywords = {FLASCO esym},
	pages = {1179--1194},
	file = {Accepted Version:C\:\\Users\\anbe6\\Zotero\\storage\\67QALY66\\Cella et al. - 2010 - The Patient-Reported Outcomes Measurement Informat.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\IFSYCWSZ\\S0895435610001733.html:text/html},
}

@article{bartlett_g-formula_2025,
	title = {G-formula with multiple imputation for causal inference with incomplete data},
	issn = {0962-2802},
	url = {https://doi.org/10.1177/09622802251316971},
	doi = {10.1177/09622802251316971},
	abstract = {G-formula is a popular approach for estimating the effects of time-varying treatments or exposures from longitudinal data. G-formula is typically implemented using Monte-Carlo simulation, with non-parametric bootstrapping used for inference. In longitudinal data settings missing data are a common issue, which are often handled using multiple imputation, but it is unclear how G-formula and multiple imputation should be combined. We show how G-formula can be implemented using Bayesian multiple imputation methods for synthetic data, and that by doing so, we can impute missing data and simulate the counterfactuals of interest within a single coherent approach. We describe how this can be achieved using standard multiple imputation software and explore its performance using a simulation study and an application from cystic fibrosis.},
	language = {EN},
	urldate = {2025-04-02},
	journal = {Stat Methods Med Res},
	author = {Bartlett, Jonathan W and Olarte Parra, Camila and Granger, Emily and Keogh, Ruth H and van Zwet, Erik W and Daniel, Rhian M},
	month = mar,
	year = {2025},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {causal inference, Bayesian, g-computation, missing data},
	pages = {09622802251316971},
	file = {SAGE PDF Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\IU3SE5Q3\\Bartlett et al. - 2025 - G-formula with multiple imputation for causal infe.pdf:application/pdf},
}

@book{hefferon_linear_2025,
	edition = {4},
	title = {Linear {Algebra}},
	url = {http://joshua.smcvt.edu/linearalgebra},
	author = {Hefferon, Jim},
	year = {2025},
	file = {Hefferon - 2025 - Linear Algebra.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\WBUJBNSD\\Hefferon - 2025 - Linear Algebra.pdf:application/pdf},
}

@article{kolodziej_epro-based_2022,
	title = {{ePRO}-based digital symptom monitoring in a community oncology practice to reduce emergency room and inpatient utilization.},
	volume = {40},
	issn = {0732-183X, 1527-7755},
	url = {https://ascopubs.org/doi/10.1200/JCO.2022.40.16_suppl.1508},
	doi = {10.1200/JCO.2022.40.16_suppl.1508},
	abstract = {1508
            Background: We have previously reported on the successful implementation of an electronic patient-reported outcomes (ePRO)-based symptom monitoring tool in a community oncology practice. Basch et al reported that use of such a tool in the academic trial setting reduced ER visits and hospitalizations. We have examined the impact of the tool on ER and inpatient utilization in this real world patient population. Methods: Highlands Oncology Group (HOG) is a 21 physician oncology group located in Northwest Arkansas. Beginning in June 2020, HOG offered patients receiving parenteral cancer therapy enrollment onto Expain, an EMR-integrated ePRO system which enables remote symptom monitoring during therapy. EMR data were linked with the Arkansas State Health Alliance for Records Exchange (SHARE), the state’s Health Information Exchange (HIE), to obtain ER visits/hospitalization data. All patients at HOG treated between September 30, 2020 and November 30, 2021 were included in this analysis. Clinical and demographic characteristics were compared in patients who enrolled on Expain versus those who did not, and corresponding p-values were calculated using Mann-Whitney and Chi-square tests. Crude rates for ER visits / hospitalizations were calculated as the total number of events per total person-time. Results: There were 855 patients enrolled on the ePRO system. Concurrently, in the same practice, 1773 patients were treated but not enrolled. Reasons for non-enrollment included patient’s choice to not participate and patient not yet offered enrollment due to rolling enrollment. The non-ePRO cohort was slightly older (66.7 vs 63.3 yrs, p {\textless}.001), more commonly male (47.3\% vs 39.3\%, p {\textless}.001) and less likely to be White (85.3\% vs 89.4\%, p = 0.003). The cohorts were comparable with respect to cancer site distribution and included a diverse and representative distribution of common malignancies receiving systemic therapy in a community practice. The proportion of patients with metastatic disease was comparable (ePRO 52.9\% vs non-ePRO 51.6\%, p = 0.55). Health resource utilization rates were lower for patients in the ePRO cohort: ER visits: 1.72 vs 2.34 per 100 patient-months, rate ratio and 95\% CI = 0.74 (0.60, 0.92), p-value = 0.005; hospitalizations: 4.76 vs 5.41 per 100 patient-months, rate ratio and 95\% CI = 0.87 (0.77, 0.99), p-value = 0.04. Conclusions: Our findings confirm the substantial benefits of using an ePRO tool in reducing health care resource utilization, and extend the initial findings of previous publications in the academic, clinical trial setting to the real world setting. This observational data is subject to confounding factors and we are evaluating the robustness using various methods to address non-comparability of the cohorts. We are further examining the benefits in specific patient subsets and attempting to correlate these benefits with improved survival.},
	language = {en},
	number = {16\_suppl},
	urldate = {2025-04-01},
	journal = {JCO},
	author = {Kolodziej, Michael A. and Kwiatkowsky, Lavi and Parrinello, Christina and Thurow, Tracy and Schaefer, Eric S. and Beck, J. Thaddeus and Cherny, Nathan and Blau, Sibel},
	month = jun,
	year = {2022},
	keywords = {FLASCO esym},
	pages = {1508--1508},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\DZBDPJMF\\Kolodziej et al. - 2022 - ePRO-based digital symptom monitoring in a communi.pdf:application/pdf},
}

@article{denis_two-year_2019,
	title = {Two-{Year} {Survival} {Comparing} {Web}-{Based} {Symptom} {Monitoring} vs {Routine} {Surveillance} {Following} {Treatment} for {Lung} {Cancer}},
	volume = {321},
	issn = {0098-7484},
	url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.2018.18085},
	doi = {10.1001/jama.2018.18085},
	language = {en},
	number = {3},
	urldate = {2025-04-01},
	journal = {JAMA},
	author = {Denis, Fabrice and Basch, Ethan and Septans, Anne-Lise and Bennouna, Jaafar and Urban, Thierry and Dueck, Amylou C. and Letellier, Christophe},
	month = jan,
	year = {2019},
	keywords = {FLASCO esym},
	pages = {306},
	file = {Denis et al. - 2019 - Two-Year Survival Comparing Web-Based Symptom Moni.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\W28XGVKV\\Denis et al. - 2019 - Two-Year Survival Comparing Web-Based Symptom Moni.pdf:application/pdf},
}

@article{basch_overall_2017,
	title = {Overall {Survival} {Results} of a {Trial} {Assessing} {Patient}-{Reported} {Outcomes} for {Symptom} {Monitoring} {During} {Routine} {Cancer} {Treatment}},
	volume = {318},
	issn = {0098-7484},
	url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.2017.7156},
	doi = {10.1001/jama.2017.7156},
	language = {en},
	number = {2},
	urldate = {2025-04-01},
	journal = {JAMA},
	author = {Basch, Ethan and Deal, Allison M. and Dueck, Amylou C. and Scher, Howard I. and Kris, Mark G. and Hudis, Clifford and Schrag, Deborah},
	month = jul,
	year = {2017},
	keywords = {FLASCO esym},
	pages = {197},
	file = {Basch et al. - 2017 - Overall Survival Results of a Trial Assessing Pati.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\27R8D2DD\\Basch et al. - 2017 - Overall Survival Results of a Trial Assessing Pati.pdf:application/pdf},
}

@article{basch_effect_2022,
	title = {Effect of {Electronic} {Symptom} {Monitoring} on {Patient}-{Reported} {Outcomes} {Among} {Patients} {With} {Metastatic} {Cancer}: {A} {Randomized} {Clinical} {Trial}},
	volume = {327},
	issn = {0098-7484},
	shorttitle = {Effect of {Electronic} {Symptom} {Monitoring} on {Patient}-{Reported} {Outcomes} {Among} {Patients} {With} {Metastatic} {Cancer}},
	url = {https://jamanetwork.com/journals/jama/fullarticle/2793279},
	doi = {10.1001/jama.2022.9265},
	abstract = {OBJECTIVE To evaluate whether electronic symptom monitoring during cancer treatment confers benefits on quality-of-life outcomes. DESIGN, SETTING, AND PARTICIPANTS Report of secondary outcomes from the PRO-TECT (Alliance AFT-39) cluster randomized trial in 52 US community oncology practices randomized to electronic symptom monitoring with PRO surveys or usual care. Between October 2017 and March 2020, 1191 adults being treated for metastatic cancer were enrolled, with last follow-up on May 17, 2021. INTERVENTIONS In the PRO group, participants (n = 593) were asked to complete weekly surveys via an internet-based or automated telephone system for up to 1 year. Severe or worsening symptoms triggered care team alerts. The control group (n = 598) received usual care. MAIN OUTCOMES AND MEASURES The 3 prespecified secondary outcomes were physical function, symptom control, and health-related quality of life (HRQOL) at 3 months, measured by the European Organisation for Research and Treatment of Cancer Quality of Life Questionnaire (QLQ-C30; range, 0-100 points; minimum clinically important difference [MCID], 2-7 for physical function; no MCID defined for symptom control or HRQOL). Results on the primary outcome, overall survival, are not yet available.
RESULTS Among 52 practices, 1191 patients were included (mean age, 62.2 years; 694 [58.3\%] women); 1066 (89.5\%) completed 3-month follow-up. Compared with usual care, mean changes on the QLQ-C30 from baseline to 3 months were significantly improved in the PRO group for physical function (PRO, from 74.27 to 75.81 points; control, from 73.54 to 72.61 points; mean difference, 2.47 [95\% CI, 0.41-4.53]; P = .02), symptom control (PRO, from 77.67 to 80.03 points; control, from 76.75 to 76.55 points; mean difference, 2.56 [95\% CI, 0.95-4.17]; P = .002), and HRQOL (PRO, from 78.11 to 80.03 points; control, from 77.00 to 76.50 points; mean difference, 2.43 [95\% CI, 0.90-3.96]; P = .002). Patients in the PRO group had significantly greater odds of experiencing clinically meaningful benefits vs usual care for physical function (7.7\% more with improvements of Ն5 points and 6.1\% fewer with worsening of Ն5 points; odds ratio [OR], 1.35 [95\% CI, 1.08-1.70]; P = .009), symptom control (8.6\% and 7.5\%, respectively; OR, 1.50 [95\% CI, 1.15-1.95]; P = .003), and HRQOL (8.5\% and 4.9\%, respectively; OR, 1.41 [95\% CI, 1.10-1.81]; P = .006).
CONCLUSIONS AND RELEVANCE In this report of secondary outcomes from a randomized clinical trial of adults receiving cancer treatment, use of weekly electronic PRO surveys to monitor symptoms, compared with usual care, resulted in statistically significant improvements in physical function, symptom control, and HRQOL at 3 months, with mean improvements of approximately 2.5 points on a 0- to 100-point scale. These findings should be interpreted provisionally pending results of the primary outcome of overall survival.},
	language = {en},
	number = {24},
	urldate = {2025-04-01},
	journal = {JAMA},
	author = {Basch, Ethan and Schrag, Deborah and Henson, Sydney and Jansen, Jennifer and Ginos, Brenda and Stover, Angela M. and Carr, Philip and Spears, Patricia A. and Jonsson, Mattias and Deal, Allison M. and Bennett, Antonia V. and Thanarajasingam, Gita and Rogak, Lauren J. and Reeve, Bryce B. and Snyder, Claire and Bruner, Deborah and Cella, David and Kottschade, Lisa A. and Perlmutter, Jane and Geoghegan, Cindy and Samuel-Ryals, Cleo A. and Given, Barbara and Mazza, Gina L. and Miller, Robert and Strasser, Jon F. and Zylla, Dylan M. and Weiss, Anna and Blinder, Victoria S. and Dueck, Amylou C.},
	month = jun,
	year = {2022},
	keywords = {FLASCO esym},
	pages = {2413},
	file = {Basch et al. - 2022 - Effect of Electronic Symptom Monitoring on Patient.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\87662V8Y\\Basch et al. - 2022 - Effect of Electronic Symptom Monitoring on Patient.pdf:application/pdf},
}

@article{laughlin_accelerating_2020,
	title = {Accelerating the {Delivery} of {Cancer} {Care} at {Home} {During} the {Covid}-19 {Pandemic}},
	volume = {1},
	url = {https://catalyst.nejm.org/doi/full/10.1056/cat.20.0258},
	doi = {10.1056/CAT.20.0258},
	number = {4},
	urldate = {2025-04-01},
	journal = {Catalyst non-issue content},
	author = {Laughlin, Amy I. and Begley, Michael and Delaney, Timothy and Zinck, Lindsey and Schuchter, Lynn M. and Doyle, Joan and Mehta, Shivan and Bekelman, Justin E. and Scott, Callie A.},
	month = jul,
	year = {2020},
	note = {Publisher: Massachusetts Medical Society},
	keywords = {FLASCO home care},
}

@article{tennant_analyses_2022,
	title = {Analyses of ‘change scores’ do not estimate causal effects in observational data},
	volume = {51},
	issn = {0300-5771},
	url = {https://doi.org/10.1093/ije/dyab050},
	doi = {10.1093/ije/dyab050},
	abstract = {In longitudinal data, it is common to create ‘change scores’ by subtracting measurements taken at baseline from those taken at follow-up, and then to analyse the resulting ‘change’ as the outcome variable. In observational data, this approach can produce misleading causal-effect estimates. The present article uses directed acyclic graphs (DAGs) and simple simulations to provide an accessible explanation for why change scores do not estimate causal effects in observational data.Data were simulated to match three general scenarios in which the outcome variable at baseline was a (i) ‘competing exposure’ (i.e. a cause of the outcome that is neither caused by nor causes the exposure), (ii) confounder or (iii) mediator for the total causal effect of the exposure variable at baseline on the outcome variable at follow-up. Regression coefficients were compared between change-score analyses and the appropriate estimator(s) for the total and/or direct causal effect(s).Change-score analyses do not provide meaningful causal-effect estimates unless the baseline outcome variable is a ‘competing exposure’ for the effect of the exposure on the outcome at follow-up. Where the baseline outcome is a confounder or mediator, change-score analyses evaluate obscure estimands, which may diverge substantially in magnitude and direction from the total and direct causal effects.Future observational studies that seek causal-effect estimates should avoid analysing change scores and adopt alternative analytical strategies.},
	number = {5},
	urldate = {2025-03-24},
	journal = {International Journal of Epidemiology},
	author = {Tennant, Peter W G and Arnold, Kellyn F and Ellison, George T H and Gilthorpe, Mark S},
	month = oct,
	year = {2022},
	keywords = {causal inference},
	pages = {1604--1615},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\H9RGV4N5\\Tennant et al. - 2022 - Analyses of ‘change scores’ do not estimate causal.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\Q7MYSMVL\\6294759.html:text/html},
}

@article{berrie_depicting_2025,
	title = {Depicting deterministic variables within directed acyclic graphs: an aid for identifying and interpreting causal effects involving derived variables and compositional data},
	volume = {194},
	issn = {0002-9262},
	shorttitle = {Depicting deterministic variables within directed acyclic graphs},
	url = {https://doi.org/10.1093/aje/kwae153},
	doi = {10.1093/aje/kwae153},
	abstract = {Deterministic variables are variables that are functionally determined by one or more parent variables. They commonly arise when a variable has been functionally created from one or more parent variables, as with derived variables, and in compositional data, where the “whole” variable is determined from its “parts.” This article introduces how deterministic variables may be depicted within directed acyclic graphs (DAGs) to help with identifying and interpreting causal effects involving derived variables and/or compositional data. We propose a 2-step approach in which all variables are initially considered, and a choice is made as to whether to focus on the deterministic variable or its determining parents. Depicting deterministic variables within DAGs brings several benefits. It is easier to identify and avoid misinterpreting tautological associations, that is, self-fulfilling associations between deterministic variables and their parents, or between sibling variables with shared parents. In compositional data, it is easier to understand the consequences of conditioning on the “whole” variable and to correctly identify total and relative causal effects. For derived variables, it encourages greater consideration of the target estimand and greater scrutiny of the consistency and exchangeability assumptions. DAGs with deterministic variables are a useful aid for planning and interpreting analyses involving derived variables and/or compositional data.},
	number = {2},
	urldate = {2025-03-24},
	journal = {American Journal of Epidemiology},
	author = {Berrie, Laurie and Arnold, Kellyn F and Tomova, Georgia D and Gilthorpe, Mark S and Tennant, Peter W G},
	month = feb,
	year = {2025},
	keywords = {causal inference, obesity},
	pages = {469--479},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\LSKNMHTG\\Berrie et al. - 2025 - Depicting deterministic variables within directed .pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\D2YGZF8I\\7698093.html:text/html},
}

@article{ali_composite_2025,
	title = {Composite variable bias: causal analysis of weight outcomes},
	copyright = {2025 The Author(s)},
	issn = {1476-5497},
	shorttitle = {Composite variable bias},
	url = {https://www.nature.com/articles/s41366-025-01732-6},
	doi = {10.1038/s41366-025-01732-6},
	abstract = {Researchers often use composite variables (e.g., BMI and change scores). By combining multiple variables (e.g., height and weight or follow-up weight and baseline weight) into a single variable it becomes challenging to untangle the causal roles of each component variable. Composite variable bias—an issue previously identified for exposure variables that may yield misleading causal inferences—is illustrated as a similar concern for composite outcomes. We explain how this occurs for composite weight outcomes: BMI, ‘weight change’, their combination ‘BMI change’, and variations involving relative change.},
	language = {en},
	urldate = {2025-03-24},
	journal = {Int J Obes},
	author = {Ali, Ridda and Prestwich, Andrew and Ge, Jiaqi and Griffiths, Claire and Allmendinger, Richard and Shahgholian, Azar and Chen, Yu-wang and Mansournia, Mohammad Ali and Gilthorpe, Mark S.},
	month = mar,
	year = {2025},
	note = {Publisher: Nature Publishing Group},
	keywords = {causal inference, obesity},
	pages = {1--8},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\2N54QEUT\\Ali et al. - 2025 - Composite variable bias causal analysis of weight.pdf:application/pdf},
}

@misc{golchi_estimating_2024,
	title = {Estimating the {Sampling} {Distribution} of {Posterior} {Decision} {Summaries} in {Bayesian} {Clinical} {Trials}},
	url = {http://arxiv.org/abs/2306.09151},
	doi = {10.48550/arXiv.2306.09151},
	abstract = {Bayesian inference and the use of posterior or posterior predictive probabilities for decision making have become increasingly popular in clinical trials. The current practice in Bayesian clinical trials relies on a hybrid Bayesian-frequentist approach where the design and decision criteria are assessed with respect to frequentist operating characteristics such as power and type I error rate conditioning on a given set of parameters. These operating characteristics are commonly obtained via simulation studies. The utility of Bayesian measures, such as ``assurance", that incorporate uncertainty about model parameters in estimating the probabilities of various decisions in trials has been demonstrated recently. However, the computational burden remains an obstacle toward wider use of such criteria. In this article, we propose methodology which utilizes large sample theory of the posterior distribution to define parametric models for the sampling distribution of the posterior summaries used for decision making. The parameters of these models are estimated using a small number of simulation scenarios, thereby refining these models to capture the sampling distribution for small to moderate sample size. The proposed approach toward the assessment of conditional and marginal operating characteristics and sample size determination can be considered as simulation-assisted rather than simulation-based. It enables formal incorporation of uncertainty about the trial assumptions via a design prior and significantly reduces the computational burden for the design of Bayesian trials in general.},
	urldate = {2025-03-23},
	publisher = {arXiv},
	author = {Golchi, Shirin and Willard, James},
	month = apr,
	year = {2024},
	note = {arXiv:2306.09151 [stat]
version: 2},
	keywords = {Statistics - Methodology},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\K9LI77LB\\Golchi and Willard - 2024 - Estimating the Sampling Distribution of Posterior .pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\WMJM8D76\\2306.html:text/html},
}

@article{talbot_guidelines_2025,
	title = {Guidelines and {Best} {Practices} for the {Use} of {Targeted} {Maximum} {Likelihood} and {Machine} {Learning} {When} {Estimating} {Causal} {Effects} of {Exposures} on {Time}-{To}-{Event} {Outcomes}},
	volume = {44},
	copyright = {© 2025 The Author(s). Statistics in Medicine published by John Wiley \& Sons Ltd.},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.70034},
	doi = {10.1002/sim.70034},
	abstract = {Targeted maximum likelihood estimation (TMLE) is an increasingly popular framework for the estimation of causal effects. It requires modeling both the exposure and outcome but is doubly robust in the sense that it is valid if at least one of these models is correctly specified. In addition, TMLE allows for flexible modeling of both the exposure and outcome with machine learning methods. This provides better control for measured confounders since the model specification automatically adapts to the data, instead of needing to be specified by the analyst a priori. Despite these methodological advantages, TMLE remains less popular than alternatives in part because of its less accessible theory and implementation. While some tutorials have been proposed, none address the case of a time-to-event outcome. This tutorial provides a detailed step-by-step explanation of the implementation of TMLE for estimating the effect of a point binary or multilevel exposure on a time-to-event outcome, modeled as counterfactual survival curves and causal hazard ratios. The tutorial also provides guidelines on how best to use TMLE in practice, including aspects related to study design, choice of covariates, controlling biases and use of machine learning. R-code is provided to illustrate each step using simulated data ( https://github.com/detal9/SurvTMLE). To facilitate implementation, a general R function implementing TMLE with options to use machine learning is also provided. The method is illustrated in a real-data analysis concerning the effectiveness of statins for the prevention of a first cardiovascular disease among older adults in Québec, Canada, between 2013 and 2018.},
	language = {en},
	number = {6},
	urldate = {2025-03-14},
	journal = {Statistics in Medicine},
	author = {Talbot, Denis and Diop, Awa and Mésidor, Miceline and Chiu, Yohann and Sirois, Caroline and Spieker, Andrew J. and Pariente, Antoine and Noize, Pernelle and Simard, Marc and Luque Fernandez, Miguel Angel and Schomaker, Michael and Fujita, Kenji and Gnjidic, Danijela and Schnitzer, Mireille E.},
	year = {2025},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.70034},
	keywords = {causal inference},
	pages = {e70034},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\J5JPZW8F\\Talbot et al. - 2025 - Guidelines and Best Practices for the Use of Targe.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\H2TFIJFS\\sim.html:text/html},
}

@book{harrell_regression_2001,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Regression {Modeling} {Strategies}: {With} {Applications} to {Linear} {Models}, {Logistic} {Regression}, and {Survival} {Analysis}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4419-2918-1 978-1-4757-3462-1},
	shorttitle = {Regression {Modeling} {Strategies}},
	url = {http://link.springer.com/10.1007/978-1-4757-3462-1},
	language = {en},
	urldate = {2025-03-13},
	publisher = {Springer New York},
	author = {Harrell, Frank E.},
	year = {2001},
	doi = {10.1007/978-1-4757-3462-1},
	file = {Harrell - 2001 - Regression Modeling Strategies With Applications .pdf:C\:\\Users\\anbe6\\Zotero\\storage\\9WAEBJWP\\Harrell - 2001 - Regression Modeling Strategies With Applications .pdf:application/pdf},
}

@article{rubin_bayesian_1981,
	title = {The {Bayesian} {Bootstrap}},
	volume = {9},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-9/issue-1/The-Bayesian-Bootstrap/10.1214/aos/1176345338.full},
	doi = {10.1214/aos/1176345338},
	abstract = {The Bayesian bootstrap is the Bayesian analogue of the bootstrap. Instead of simulating the sampling distribution of a statistic estimating a parameter, the Bayesian bootstrap simulates the posterior distribution of the parameter; operationally and inferentially the methods are quite similar. Because both methods of drawing inferences are based on somewhat peculiar model assumptions and the resulting inferences are generally sensitive to these assumptions, neither method should be applied without some consideration of the reasonableness of these model assumptions. In this sense, neither method is a true bootstrap procedure yielding inferences unaided by external assumptions.},
	number = {1},
	urldate = {2025-03-10},
	journal = {The Annals of Statistics},
	author = {Rubin, Donald B.},
	month = jan,
	year = {1981},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Bayesian},
	pages = {130--134},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\HXPMX78I\\Rubin - 1981 - The Bayesian Bootstrap.pdf:application/pdf},
}

@article{saarela_bayesian_2016,
	title = {A {Bayesian} view of doubly robust causal inference},
	volume = {103},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/asw025},
	doi = {10.1093/biomet/asw025},
	abstract = {In causal inference the effect of confounding may be controlled using regression adjustment in an outcome model, propensity score adjustment, inverse probability of treatment weighting or a combination of these. Approaches based on modelling the treatment assignment mechanism, along with their doubly robust extensions, have been difficult to motivate using formal likelihood-based or Bayesian arguments, as the treatment assignment model plays no part in inferences concerning the expected outcomes. On the other hand, forcing dependency between the outcome and treatment assignment models by allowing the former to be misspecified results in loss of the balancing property of the propensity scores and the loss of any double robustness. In this paper, we explain in the framework of misspecified models why doubly robust inferences cannot arise from purely likelihood-based arguments. As an alternative to Bayesian propensity score analysis, we propose a Bayesian posterior predictive method for constructing doubly robust estimation procedures by incorporating the inverse treatment assignment probabilities as importance sampling weights in Monte Carlo integration.},
	number = {3},
	urldate = {2025-03-10},
	journal = {Biometrika},
	author = {Saarela, O. and Belzile, L. R. and Stephens, D. A.},
	month = sep,
	year = {2016},
	keywords = {causal inference, Bayesian, doubly robust},
	pages = {667--681},
	file = {Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\AM5A6MRB\\1743939.html:text/html;Submitted Version:C\:\\Users\\anbe6\\Zotero\\storage\\P4V5UQ9F\\Saarela et al. - 2016 - A Bayesian view of doubly robust causal inference.pdf:application/pdf},
}

@article{anderson-bergman_bayesian_2017,
	title = {Bayesian {Regression} {Models} for {Interval}-censored {Data} in {R}},
	volume = {9},
	issn = {2073-4859},
	url = {https://journal.r-project.org/archive/2017/RJ-2017-050/index.html},
	doi = {10.32614/RJ-2017-050},
	abstract = {The package icenReg provides classic survival regression models for interval-censored data. We present an update to the package that extends the parametric models into the Bayesian framework. Core additions include functionality to deﬁne the regression model with the standard regression syntax while providing a custom prior function. Several other utility functions are presented that allow for simpliﬁed examination of the posterior distribution.},
	language = {en},
	number = {2},
	urldate = {2025-03-07},
	journal = {The R Journal},
	author = {Anderson-Bergman, Clifford},
	year = {2017},
	keywords = {Bayesian},
	pages = {487},
	file = {Anderson-Bergman - 2017 - Bayesian Regression Models for Interval-censored D.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\TVZQL3TA\\Anderson-Bergman - 2017 - Bayesian Regression Models for Interval-censored D.pdf:application/pdf},
}

@article{jackson_flexible_nodate,
	title = {Flexible parametric multi-state modelling with flexsurv},
	abstract = {A multi-state model represents how an individual moves between multiple states in continuous time. Survival analysis is a special case with two states, “alive” and “dead”. Competing risks are a further special case, where there are multiple causes of death, that is, one starting state and multiple possible destination states.},
	language = {en},
	author = {Jackson, Christopher and Unit, MRC Biostatistics},
	keywords = {multi-state},
	file = {Jackson and Unit - Flexible parametric multi-state modelling with fle.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\J9UBXLBZ\\Jackson and Unit - Flexible parametric multi-state modelling with fle.pdf:application/pdf},
}

@article{barton_optimal_2008,
	title = {Optimal {Cost}-{Effectiveness} {Decisions}: {The} {Role} of the {Cost}-{Effectiveness} {Acceptability} {Curve} ({CEAC}), the {Cost}-{Effectiveness} {Acceptability} {Frontier} ({CEAF}), and the {Expected} {Value} of {Perfection} {Information} ({EVPI})},
	volume = {11},
	issn = {1524-4733},
	shorttitle = {Optimal {Cost}-{Effectiveness} {Decisions}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1524-4733.2008.00358.x},
	doi = {10.1111/j.1524-4733.2008.00358.x},
	abstract = {Objective: To demonstrate how the optimal decision and level of uncertainty associated with that decision, can be presented when assessing the cost-effectiveness of multiple options. To explore and explain potentially counterintuitive results that can arise when analyzing multiple options. Methods: A template was created, based on the assumption of multivariate normality, in order to replicate a previous analysis that compared the cost-effectiveness of multiple options. We used this template to explain some of the different shapes that the cost-effectiveness acceptability curve (CEAC), cost-effectiveness acceptability frontier (CEAF), and expected value of perfection information (EVPI) may take, with changing correlation structure and variance between the multiple options. Results: We show that it is possible for 1) an option that is subject to extended dominance to have the highest probability of being cost-effective for some values of the cost-effectiveness threshold; 2) the most cost-effective (optimal) option to never have the highest probability of being cost-effective; and 3) the EVPI to increase when the probability of making the wrong decision decreases. Changing the correlation structure between multiple options did not change the presentation of results on the cost-effectiveness plane. Conclusion: The cost-effectiveness plane has limited use in representing the uncertainty surrounding multiple options as it cannot represent correlation between the options. CEACs can represent decision uncertainty, but should not be used to determine the optimal decision. Instead, the CEAF shows the decision uncertainty surrounding the optimal choice and this can be augmented by the EVPI to show the potential gains to further research.},
	language = {en},
	number = {5},
	urldate = {2025-03-04},
	journal = {Value in Health},
	author = {Barton, Garry R. and Briggs, Andrew H. and Fenwick, Elisabeth A. L.},
	year = {2008},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1524-4733.2008.00358.x},
	keywords = {cost-effectiveness acceptability curve (CEAC), cost-effectiveness acceptability frontier (CEAF), cost-effectiveness plane, expected value of perfect information (EVPI), uncertainty},
	pages = {886--897},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\8TLQXEKU\\Barton et al. - 2008 - Optimal Cost-Effectiveness Decisions The Role of .pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\XTF3HUJM\\j.1524-4733.2008.00358.html:text/html},
}

@article{vermeulen_bias-reduced_2015,
	title = {Bias-{Reduced} {Doubly} {Robust} {Estimation}},
	volume = {110},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/full/10.1080/01621459.2014.958155},
	doi = {10.1080/01621459.2014.958155},
	language = {en},
	number = {511},
	urldate = {2025-03-03},
	journal = {Journal of the American Statistical Association},
	author = {Vermeulen, Karel and Vansteelandt, Stijn},
	month = jul,
	year = {2015},
	keywords = {causal inference, doubly robust},
	pages = {1024--1036},
	file = {Vermeulen and Vansteelandt - 2015 - Bias-Reduced Doubly Robust Estimation.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\66IDMCZM\\Vermeulen and Vansteelandt - 2015 - Bias-Reduced Doubly Robust Estimation.pdf:application/pdf},
}

@article{funk_doubly_2011,
	title = {Doubly {Robust} {Estimation} of {Causal} {Effects}},
	volume = {173},
	issn = {1476-6256, 0002-9262},
	url = {https://academic.oup.com/aje/article-lookup/doi/10.1093/aje/kwq439},
	doi = {10.1093/aje/kwq439},
	language = {en},
	number = {7},
	urldate = {2025-03-03},
	journal = {American Journal of Epidemiology},
	author = {Funk, Michele Jonsson and Westreich, Daniel and Wiesen, Chris and Stürmer, Til and Brookhart, M. Alan and Davidian, Marie},
	month = apr,
	year = {2011},
	keywords = {causal inference},
	pages = {761--767},
	file = {Funk et al. - 2011 - Doubly Robust Estimation of Causal Effects.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\8GH7MSVM\\Funk et al. - 2011 - Doubly Robust Estimation of Causal Effects.pdf:application/pdf},
}

@misc{deng_separable_2024,
	title = {Separable pathway effects of semi-competing risks using multi-state models},
	url = {http://arxiv.org/abs/2306.15947},
	doi = {10.48550/arXiv.2306.15947},
	abstract = {Semi-competing risks refer to the phenomenon where a primary event (such as mortality) can ``censor'' an intermediate event (such as relapse of a disease), but not vice versa. Under the multi-state model, the primary event consists of two specific types: the direct outcome event and an indirect outcome event developed from intermediate events. Within this framework, we show that the total treatment effect on the cumulative incidence of the primary event can be decomposed into three separable pathway effects, capturing treatment effects on population-level transition rates between states. We next propose two estimators for the counterfactual cumulative incidences of the primary event under hypothetical treatment components. One estimator is given by the generalized Nelson--Aalen estimator with inverse probability weighting under covariates isolation, and the other is given based on the efficient influence function. The asymptotic normality of these estimators is established. The first estimator only involves a propensity score model and avoid modeling the cause-specific hazards. The second estimator has robustness against the misspecification of submodels. As an illustration of its potential usefulness, the proposed method is applied to compare effects of different allogeneic stem cell transplantation types on overall survival after transplantation.},
	urldate = {2025-03-01},
	publisher = {arXiv},
	author = {Deng, Yuhao and Wang, Yi and Zhan, Xiang and Zhou, Xiao-Hua},
	month = oct,
	year = {2024},
	note = {arXiv:2306.15947 [stat]},
	keywords = {causal inference, semicompeting risks},
	file = {Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\Z4WH9C3J\\2306.html:text/html},
}

@article{alvares_semicomprisks_2019,
	title = {{SemiCompRisks}: {An} {R} {Package} for the {Analysis} of {Independent} and {Cluster}-correlated {Semi}-competing {Risks} {Data}},
	volume = {11},
	issn = {2073-4859},
	shorttitle = {{SemiCompRisks}},
	url = {https://journal.r-project.org/archive/2019/RJ-2019-038/index.html},
	doi = {10.32614/RJ-2019-038},
	abstract = {Semi-competing risks refer to the setting where primary scientiﬁc interest lies in estimation and inference with respect to a non-terminal event, the occurrence of which is subject to a terminal event. In this paper, we present the R package SemiCompRisks that provides functions to perform the analysis of independent/clustered semi-competing risks data under the illness-death multi-state model. The package allows the user to choose the speciﬁcation for model components from a range of options giving users substantial ﬂexibility, including: accelerated failure time or proportional hazards regression models; parametric or non-parametric speciﬁcations for baseline survival functions; parametric or non-parametric speciﬁcations for random effects distributions when the data are clustercorrelated; and, a Markov or semi-Markov speciﬁcation for terminal event following non-terminal event. While estimation is mainly performed within the Bayesian paradigm, the package also provides the maximum likelihood estimation for select parametric models. The package also includes functions for univariate survival analysis as complementary analysis tools.},
	language = {en},
	number = {1},
	urldate = {2025-02-28},
	journal = {The R Journal},
	author = {Alvares, Danilo and Haneuse, Sebastien and Lee, Catherine and Lee, Ha, Kyu},
	year = {2019},
	keywords = {competing risks, semi-Markov, semicompeting risks},
	pages = {376},
	file = {Alvares et al. - 2019 - SemiCompRisks An R Package for the Analysis of In.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\ISLIZ3NK\\Alvares et al. - 2019 - SemiCompRisks An R Package for the Analysis of In.pdf:application/pdf},
}

@article{heuts_bayesian_2025,
	title = {Bayesian {Analytical} {Methods} in {Cardiovascular} {Clinical} {Trials}: {Why}, {When}, and {How}},
	volume = {41},
	issn = {0828282X},
	shorttitle = {Bayesian {Analytical} {Methods} in {Cardiovascular} {Clinical} {Trials}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0828282X24011309},
	doi = {10.1016/j.cjca.2024.11.002},
	language = {en},
	number = {1},
	urldate = {2025-02-06},
	journal = {Canadian Journal of Cardiology},
	author = {Heuts, Samuel and Kawczynski, Michal J. and Sayed, Ahmed and Urbut, Sarah M. and Albuquerque, Arthur M. and Mandrola, John M. and Kaul, Sanjay and Harrell, Frank E. and Gabrio, Andrea and Brophy, James M.},
	month = jan,
	year = {2025},
	keywords = {Bayesian, trial design},
	pages = {30--44},
	file = {Heuts et al. - 2025 - Bayesian Analytical Methods in Cardiovascular Clin.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\8TJYCMVV\\Heuts et al. - 2025 - Bayesian Analytical Methods in Cardiovascular Clin.pdf:application/pdf},
}

@article{hobbs_practical_2007,
	title = {Practical {Bayesian} {Design} and {Analysis} for {Drug} and {Device} {Clinical} {Trials}},
	volume = {18},
	issn = {1054-3406, 1520-5711},
	url = {https://www.tandfonline.com/doi/full/10.1080/10543400701668266},
	doi = {10.1080/10543400701668266},
	language = {en},
	number = {1},
	urldate = {2025-02-27},
	journal = {Journal of Biopharmaceutical Statistics},
	author = {Hobbs, Brian P. and Carlin, Bradley P.},
	month = dec,
	year = {2007},
	keywords = {Bayesian, trial design},
	pages = {54--80},
	file = {Hobbs and Carlin - 2007 - Practical Bayesian Design and Analysis for Drug an.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\87WPLPLC\\Hobbs and Carlin - 2007 - Practical Bayesian Design and Analysis for Drug an.pdf:application/pdf},
}

@article{alinia_exploring_2024,
	title = {Exploring the impact of stage and tumor site on colorectal cancer survival: {Bayesian} survival modeling},
	volume = {14},
	issn = {2045-2322},
	shorttitle = {Exploring the impact of stage and tumor site on colorectal cancer survival},
	url = {https://www.nature.com/articles/s41598-024-54943-8},
	doi = {10.1038/s41598-024-54943-8},
	abstract = {Abstract
            Colorectal cancer is a prevalent malignancy with global significance. This retrospective study aimed to investigate the influence of stage and tumor site on survival outcomes in 284 colorectal cancer patients diagnosed between 2001 and 2017. Patients were categorized into four groups based on tumor site (colon and rectum) and disease stage (early stage and advanced stage). Demographic characteristics, treatment modalities, and survival outcomes were recorded. Bayesian survival modeling was performed using semi-competing risks illness-death models with an accelerated failure time (AFT) approach, utilizing R 4.1 software. Results demonstrated significantly higher time ratios for disease recurrence (TR = 1.712, 95\% CI 1.489–2.197), mortality without recurrence (TR = 1.933, 1.480–2.510), and mortality after recurrence (TR = 1.847, 1.147–2.178) in early-stage colon cancer compared to early-stage rectal cancer. Furthermore, patients with advanced-stage rectal cancer exhibited shorter survival times for disease recurrence than patients with early-stage colon cancer. The interaction effect between the disease site and cancer stage was not significant. These findings, derived from the optimal Bayesian log-normal model for terminal and non-terminal events, highlight the importance of early detection and effective management strategies for colon cancer. Early-stage colon cancer demonstrated improved survival rates for disease recurrence, mortality without recurrence, and mortality after recurrence compared to other stages. Early intervention and comprehensive care are crucial to enhance prognosis and minimize adverse events in colon cancer patients.},
	language = {en},
	number = {1},
	urldate = {2025-02-27},
	journal = {Sci Rep},
	author = {Alinia, Shayesteh and Ahmadi, Samira and Mohammadi, Zahra and Rastkar Shirvandeh, Farzaneh and Asghari-Jafarabadi, Mohammad and Mahmoudi, Leila and Safari, Malihe and Roshanaei, Ghodratollah},
	month = feb,
	year = {2024},
	keywords = {survival analysis, Bayesian},
	pages = {4270},
	file = {Alinia et al. - 2024 - Exploring the impact of stage and tumor site on co.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\JSST4DDY\\Alinia et al. - 2024 - Exploring the impact of stage and tumor site on co.pdf:application/pdf},
}

@article{gran_causal_2015,
	title = {Causal inference in multi-state models–sickness absence and work for 1145 participants after work rehabilitation},
	volume = {15},
	issn = {1471-2458},
	url = {https://doi.org/10.1186/s12889-015-2408-8},
	doi = {10.1186/s12889-015-2408-8},
	abstract = {Multi-state models, as an extension of traditional models in survival analysis, have proved to be a flexible framework for analysing the transitions between various states of sickness absence and work over time. In this paper we study a cohort of work rehabilitation participants and analyse their subsequent sickness absence using Norwegian registry data on sickness benefits. Our aim is to study how detailed individual covariate information from questionnaires explain differences in sickness absence and work, and to use methods from causal inference to assess the effect of interventions to reduce sickness absence. Examples of the latter are to evaluate the use of partial versus full time sick leave and to estimate the effect of a cooperation agreement on a more inclusive working life.},
	language = {en},
	number = {1},
	urldate = {2025-02-27},
	journal = {BMC Public Health},
	author = {Gran, Jon Michael and Lie, Stein Atle and Øyeflaten, Irene and Borgan, Ørnulf and Aalen, Odd O.},
	month = oct,
	year = {2015},
	keywords = {causal inference, multi-state},
	pages = {1082},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\5K5Z2Q6Q\\Gran et al. - 2015 - Causal inference in multi-state models–sickness ab.pdf:application/pdf},
}

@article{kneib_bayesian_2008,
	title = {Bayesian semi parametric multi-state models},
	volume = {8},
	copyright = {https://journals.sagepub.com/page/policies/text-and-data-mining-license},
	issn = {1471-082X, 1477-0342},
	url = {https://journals.sagepub.com/doi/10.1177/1471082X0800800203},
	doi = {10.1177/1471082X0800800203},
	abstract = {Multi-state models provide a unified framework for the description of the evolution of discrete phenomena in continuous time. One particular example is Markov processes which can be characterised by a set of time-constant transition intensities between the states. In this paper, we will extend such parametric approaches to semiparametric models with flexible transition intensities based on Bayesian versions of penalised splines. The transition intensities will be modelled as smooth functions of time and can further be related to parametric as well as nonparametric covariate effects. Covariates with time-varying effects and frailty terms can be included in addition. Inference will be conducted either fully Bayesian (using Markov chain Monte Carlo simulation techniques) or empirically Bayesian (based on a mixed model representation). A counting process representation of semiparametric multi-state models provides the likelihood formula and also forms the basis for model validation via martingale residual processes. As an application, we will consider human sleep data with a discrete set of sleep states such as REM and non-REM phases. In this case, simple parametric approaches are inappropriate since the dynamics underlying human sleep are strongly varying throughout the night and individual-specific variation has to be accounted for using covariate information and frailty terms.},
	language = {en},
	number = {2},
	urldate = {2025-02-27},
	journal = {Statistical Modelling},
	author = {Kneib, Thomas and Hennerfeind, Andrea},
	month = jul,
	year = {2008},
	keywords = {Bayesian, multi-state},
	pages = {169--198},
	file = {Accepted Version:C\:\\Users\\anbe6\\Zotero\\storage\\I2P76SL7\\Kneib and Hennerfeind - 2008 - Bayesian semi parametric multi-state models.pdf:application/pdf},
}

@article{shen_bayesian_2017,
	title = {A {Bayesian} model for estimating multi-state disease progression},
	volume = {81},
	issn = {00104825},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010482516303316},
	doi = {10.1016/j.compbiomed.2016.12.011},
	language = {en},
	urldate = {2025-02-27},
	journal = {Computers in Biology and Medicine},
	author = {Shen, Shiwen and Han, Simon X. and Petousis, Panayiotis and Weiss, Robert E. and Meng, Frank and Bui, Alex A.T. and Hsu, William},
	month = feb,
	year = {2017},
	keywords = {Bayesian, multi-state},
	pages = {111--120},
	file = {Bayesian-1-s2.0-S0010482516303316.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\T9R4UE4K\\Bayesian-1-s2.0-S0010482516303316.pdf:application/pdf},
}

@article{parikh_algorithm-based_2025,
	title = {Algorithm-{Based} {Palliative} {Care} in {Patients} {With} {Cancer}: {A} {Cluster} {Randomized} {Clinical} {Trial}},
	volume = {8},
	issn = {2574-3805},
	shorttitle = {Algorithm-{Based} {Palliative} {Care} in {Patients} {With} {Cancer}},
	url = {https://doi.org/10.1001/jamanetworkopen.2024.58576},
	doi = {10.1001/jamanetworkopen.2024.58576},
	abstract = {Among patients with advanced solid malignant tumors, early specialty palliative care (PC) is guideline recommended, but strategies to increase PC access and effectiveness in community oncology are lacking.To test whether algorithm-based defaults with opting out and accountable justification embedded in the electronic health record (EHR) increase completed PC visits.This 2-arm cluster randomized clinical trial was conducted from November 1, 2022, to December 31, 2023. Eligible patients from 15 urban or rural clinics within a large community oncology network in Tennessee had advanced lung or noncolorectal gastrointestinal cancer and were identified by an automated EHR algorithm adapted from national guidelines. Data were analyzed between November 1, 2023, and March 4, 2024.At sites randomized to control, clinicians received weekly reports detailing PC referral rates compared with peer clinicians (peer comparison) and referred patients to PC at their discretion. At sites randomized to intervention, clinicians also received default PC orders using the EHR. Clinicians who opted out of PC consultation were asked to provide justification (accountable justification). If clinicians did not opt out, a study coordinator contacted patients to introduce and schedule PC visits using a standardized, predefined script.The primary outcome was a completed PC consultation within 12 weeks of enrollment. Exploratory outcomes included quality of life, feeling heard and understood, and intensive end-of-life care. Outcomes were analyzed using clustered generalized linear and logistic regression models.The trial enrolled 562 patients (mean [SD] age, 68.5 [10.1] years; 288 male [51.2\%]), of whom 433 (77.0\%) had lung cancer. There were 130 of 296 patients (43.9\%) randomized to the intervention group and 22 of 266 (8.3\%) randomized to the control group who completed PC visits (adjusted odds ratio, 8.9 [95\% CI, 5.5-14.6]; P \&lt; .001). Among 179 patients who died at the 24-week follow-up, 6 of 92 (6.5\%) in the intervention group compared with 14 of 87 (16.1\%) in the control group received systemic therapy within 14 days of death (adjusted odds ratio, 0.3 [95\% CI, 0.1-0.7]; P = .05). There were no differences in quality of life, feeling heard and understood, or late hospice referral.In this randomized clinical trial of algorithm-based EHR defaults, the intervention increased PC consultations and decreased end-of-life systemic therapy. The intervention provides a scalable implementation strategy to increase specialty PC referrals in the community oncology setting.ClinicalTrials.gov Identifier: NCT05590962},
	number = {2},
	urldate = {2025-02-26},
	journal = {JAMA Network Open},
	author = {Parikh, Ravi B. and Ferrell, William J. and Li, Yang and Chen, Jinbo and Bilbrey, Larry and Johnson, Nicole and White, Jenna and Sedhom, Ramy and Dickson, Natalie R. and Schleicher, Stephen and Bekelman, Justin E. and Mudumbi, Sandhya},
	month = feb,
	year = {2025},
	keywords = {palliative care},
	pages = {e2458576},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\FWN9VS6M\\Parikh et al. - 2025 - Algorithm-Based Palliative Care in Patients With C.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\UN3QYH6P\\2830599.html:text/html},
}

@article{josefsson_bayesian_2021,
	title = {Bayesian {Semi}-parametric {G}-computation {For} {Causal} {Inference} in a {Cohort} {Study} with {Mnar} {Dropout} and {Death}},
	volume = {70},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	issn = {0035-9254, 1467-9876},
	url = {https://academic.oup.com/jrsssc/article/70/2/398/7033955},
	doi = {10.1111/rssc.12464},
	abstract = {Abstract
            Causal inference with observational longitudinal data and time-varying exposures is often complicated by time-dependent confounding and attrition. The G-computation formula is one approach for estimating a causal effect in this setting. The parametric modelling approach typically used in practice relies on strong modelling assumptions for valid inference and moreover depends on an assumption of missing at random, which is not appropriate when the missingness is missing not at random (MNAR) or due to death. In this work we develop a flexible Bayesian semi-parametric G-computation approach for assessing the causal effect on the subpopulation that would survive irrespective of exposure, in a setting with MNAR dropout. The approach is to specify models for the observed data using Bayesian additive regression trees, and then, use assumptions with embedded sensitivity parameters to identify and estimate the causal effect. The proposed approach is motivated by a longitudinal cohort study on cognition, health and ageing and we apply our approach to study the effect of becoming a widow on memory. We also compare our approach to several standard methods.},
	language = {en},
	number = {2},
	urldate = {2025-02-25},
	journal = {Journal of the Royal Statistical Society Series C: Applied Statistics},
	author = {Josefsson, Maria and Daniels, Michael J.},
	month = mar,
	year = {2021},
	keywords = {causal inference, Bayesian, g-computation, missing data},
	pages = {398--414},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\FPFTRT9F\\Josefsson and Daniels - 2021 - Bayesian Semi-parametric G-computation For Causal .pdf:application/pdf;rssc_70_2_398.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\9CVWNZ3V\\rssc_70_2_398.pdf:application/pdf},
}

@article{taubman_intervening_2009,
	title = {Intervening on risk factors for coronary heart disease: an application of the parametric g-formula},
	volume = {38},
	issn = {0300-5771},
	shorttitle = {Intervening on risk factors for coronary heart disease},
	url = {https://doi.org/10.1093/ije/dyp192},
	doi = {10.1093/ije/dyp192},
	abstract = {Estimating the population risk of disease under hypothetical interventions—such as the population risk of coronary heart disease (CHD) were everyone to quit smoking and start exercising or to start exercising if diagnosed with diabetes—may not be possible using standard analytic techniques. The parametric g-formula, which appropriately adjusts for time-varying confounders affected by prior exposures, is especially well suited to estimating effects when the intervention involves multiple factors (joint interventions) or when the intervention involves decisions that depend on the value of evolving time-dependent factors (dynamic interventions). We describe the parametric g-formula, and use it to estimate the effect of various hypothetical lifestyle interventions on the risk of CHD using data from the Nurses’ Health Study. Over the period 1982–2002, the 20-year risk of CHD in this cohort was 3.50\%. Under a joint intervention of no smoking, increased exercise, improved diet, moderate alcohol consumption and reduced body mass index, the estimated risk was 1.89\% (95\% confidence interval: 1.46–2.41). We discuss whether the assumptions required for the validity of the parametric g-formula hold in the Nurses’ Health Study data. This work represents the first large-scale application of the parametric g-formula in an epidemiologic cohort study.},
	number = {6},
	urldate = {2025-02-25},
	journal = {International Journal of Epidemiology},
	author = {Taubman, Sarah L and Robins, James M and Mittleman, Murray A and Hernán, Miguel A},
	month = dec,
	year = {2009},
	keywords = {g-computation},
	pages = {1599--1611},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\RSQSRBQ9\\Taubman et al. - 2009 - Intervening on risk factors for coronary heart dis.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\KP5WQGA7\\669228.html:text/html},
}

@misc{ji_causalbeta_2023,
	title = {{causalBETA}: {An} {R} {Package} for {Bayesian} {Semiparametric} {Causal} {Inference} with {Event}-{Time} {Outcomes}},
	shorttitle = {{causalBETA}},
	url = {http://arxiv.org/abs/2310.12358},
	abstract = {Observational studies are often conducted to estimate causal effects of treatments or exposures on event-time outcomes. Since treatments are not randomized in observational studies, techniques from causal inference are required to adjust for confounding. Bayesian approaches to causal estimates are desirable because they provide 1) prior smoothing provides useful regularization of causal effect estimates, 2) flexible models that are robust to misspecification, 3) full inference (i.e. both point and uncertainty estimates) for causal estimands. However, Bayesian causal inference is difficult to implement manually and there is a lack of user-friendly software, presenting a significant barrier to wide-spread use. We address this gap by developing causalBETA (Bayesian Event Time Analysis) - an open-source R package for estimating causal effects on event-time outcomes using Bayesian semiparametric models. The package provides a familiar front-end to users, with syntax identical to existing survival analysis R packages such as survival. At the same time, it back-ends to Stan - a popular platform for Bayesian modeling and high performance statistical computing - for efficient posterior computation. To improve user experience, the package is built using customized S3 class objects and methods to facilitate visualizations and summaries of results using familiar generic functions like plot() and summary(). In this paper, we provide the methodological details of the package, a demonstration using publicly-available data, and computational guidance.},
	urldate = {2024-02-08},
	publisher = {arXiv},
	author = {Ji, Han and Oganisian, Arman},
	month = oct,
	year = {2023},
	note = {arXiv:2310.12358 [stat]},
	keywords = {causal inference, survival analysis, Bayesian, g-computation},
	file = {arXiv.org Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\92NHLEPG\\2310.html:text/html;Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\JCUA9Z64\\Ji and Oganisian - 2023 - causalBETA An R Package for Bayesian Semiparametr.pdf:application/pdf},
}

@article{hernan_observational_2008,
	title = {Observational {Studies} {Analyzed} {Like} {Randomized} {Experiments}: {An} {Application} to {Postmenopausal} {Hormone} {Therapy} and {Coronary} {Heart} {Disease}},
	volume = {19},
	issn = {1044-3983},
	shorttitle = {Observational {Studies} {Analyzed} {Like} {Randomized} {Experiments}},
	url = {https://journals.lww.com/epidem/fulltext/2008/11000/observational_studies_analyzed_like_randomized.2.aspx},
	doi = {10.1097/EDE.0b013e3181875e61},
	abstract = {Background: 
          The Women's Health Initiative randomized trial found greater coronary heart disease (CHD) risk in women assigned to estrogen/progestin therapy than in those assigned to placebo. Observational studies had previously suggested reduced CHD risk in hormone users.
          Methods: 
          Using data from the observational Nurses’ Health Study, we emulated the design and intention-to-treat (ITT) analysis of the randomized trial. The observational study was conceptualized as a sequence of “trials,” in which eligible women were classified as initiators or noninitiators of estrogen/progestin therapy.
          Results: 
          The ITT hazard ratios (HRs) (95\% confidence intervals) of CHD for initiators versus noninitiators were 1.42 (0.92–2.20) for the first 2 years, and 0.96 (0.78–1.18) for the entire follow-up. The ITT HRs were 0.84 (0.61–1.14) in women within 10 years of menopause, and 1.12 (0.84–1.48) in the others (P value for interaction = 0.08). These ITT estimates are similar to those from the Women's Health Initiative. Because the ITT approach causes severe treatment misclassification, we also estimated adherence-adjusted effects by inverse probability weighting. The HRs were 1.61 (0.97–2.66) for the first 2 years, and 0.98 (0.66–1.49) for the entire follow-up. The HRs were 0.54 (0.19–1.51) in women within 10 years after menopause, and 1.20 (0.78–1.84) in others (P value for interaction = 0.01). We also present comparisons between these estimates and previously reported Nurses’ Health Study estimates.
          Conclusions: 
          Our findings suggest that the discrepancies between the Women's Health Initiative and Nurses’ Health Study ITT estimates could be largely explained by differences in the distribution of time since menopause and length of follow-up.},
	language = {en-US},
	number = {6},
	urldate = {2025-02-25},
	journal = {Epidemiology},
	author = {Hernán, Miguel A. and Alonso, Alvaro and Logan, Roger and Grodstein, Francine and Michels, Karin B. and Willett, Walter C. and Manson, JoAnn E. and Robins, James M.},
	month = nov,
	year = {2008},
	pages = {766},
	file = {Accepted Version:C\:\\Users\\anbe6\\Zotero\\storage\\MY6WDA5E\\Hernán et al. - 2008 - Observational Studies Analyzed Like Randomized Exp.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\V893HXMY\\observational_studies_analyzed_like_randomized.2.html:text/html},
}

@article{sharma_comparing_2021,
	title = {Comparing {Charlson} and {Elixhauser} comorbidity indices with different weightings to predict in-hospital mortality: an analysis of national inpatient data},
	volume = {21},
	issn = {1472-6963},
	shorttitle = {Comparing {Charlson} and {Elixhauser} comorbidity indices with different weightings to predict in-hospital mortality},
	url = {https://doi.org/10.1186/s12913-020-05999-5},
	doi = {10.1186/s12913-020-05999-5},
	abstract = {Understanding how comorbidity measures contribute to patient mortality is essential both to describe patient health status and to adjust for risks and potential confounding. The Charlson and Elixhauser comorbidity indices are well-established for risk adjustment and mortality prediction. Still, a different set of comorbidity weights might improve the prediction of in-hospital mortality. The present study, therefore, aimed to derive a set of new Swiss Elixhauser comorbidity weightings, to validate and compare them against those of the Charlson and Elixhauser-based van Walraven weights in an adult in-patient population-based cohort of general hospitals.},
	number = {1},
	urldate = {2025-02-24},
	journal = {BMC Health Services Research},
	author = {Sharma, Narayan and Schwendimann, René and Endrich, Olga and Ausserhofer, Dietmar and Simon, Michael},
	month = jan,
	year = {2021},
	keywords = {Comorbidity indices, In-hospital mortality, Inpatient data, Risk adjustment, Weights},
	pages = {13},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\F4Z4TN2B\\Sharma et al. - 2021 - Comparing Charlson and Elixhauser comorbidity indi.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\KSKXN8EF\\s12913-020-05999-5.html:text/html},
}

@article{van_walraven_modification_2009,
	title = {A {Modification} of the {Elixhauser} {Comorbidity} {Measures} {Into} a {Point} {System} for {Hospital} {Death} {Using} {Administrative} {Data}},
	volume = {47},
	issn = {0025-7079},
	url = {https://journals.lww.com/00005650-200906000-00004},
	doi = {10.1097/MLR.0b013e31819432e5},
	language = {en},
	number = {6},
	urldate = {2025-02-24},
	journal = {Medical Care},
	author = {Van Walraven, Carl and Austin, Peter C. and Jennings, Alison and Quan, Hude and Forster, Alan J.},
	month = jun,
	year = {2009},
	keywords = {co-morbidity},
	pages = {626--633},
	file = {Van Walraven et al. - 2009 - A Modification of the Elixhauser Comorbidity Measu.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\XH8DNSV8\\Van Walraven et al. - 2009 - A Modification of the Elixhauser Comorbidity Measu.pdf:application/pdf},
}

@article{quan_updating_2011,
	title = {Updating and {Validating} the {Charlson} {Comorbidity} {Index} and {Score} for {Risk} {Adjustment} in {Hospital} {Discharge} {Abstracts} {Using} {Data} {From} 6 {Countries}},
	volume = {173},
	issn = {0002-9262, 1476-6256},
	url = {https://academic.oup.com/aje/article-lookup/doi/10.1093/aje/kwq433},
	doi = {10.1093/aje/kwq433},
	language = {en},
	number = {6},
	urldate = {2025-02-24},
	journal = {American Journal of Epidemiology},
	author = {Quan, H. and Li, B. and Couris, C. M. and Fushimi, K. and Graham, P. and Hider, P. and Januel, J.-M. and Sundararajan, V.},
	month = mar,
	year = {2011},
	keywords = {co-morbidity},
	pages = {676--682},
	file = {Quan et al. - 2011 - Updating and Validating the Charlson Comorbidity I.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\M9SFD8G8\\Quan et al. - 2011 - Updating and Validating the Charlson Comorbidity I.pdf:application/pdf},
}

@article{kurian_germline_2023,
	title = {Germline {Genetic} {Testing} {After} {Cancer} {Diagnosis}},
	volume = {330},
	issn = {0098-7484},
	url = {https://jamanetwork.com/journals/jama/fullarticle/2805796},
	doi = {10.1001/jama.2023.9526},
	abstract = {Importance
              Germline genetic testing is recommended by practice guidelines for patients diagnosed with cancer to enable genetically targeted treatment and identify relatives who may benefit from personalized cancer screening and prevention.


              Objective
              To describe the prevalence of germline genetic testing among patients diagnosed with cancer in California and Georgia between 2013 and 2019.


              Design, Setting, and Participants
              Observational study including patients aged 20 years or older who had been diagnosed with any type of cancer between January 1, 2013, and March 31, 2019, that was reported to statewide Surveillance, Epidemiology, and End Results registries in California and Georgia. These patients were linked to genetic testing results from 4 laboratories that performed most germline testing for California and Georgia.


              Main Outcomes and Measures
              The primary outcome was germline genetic testing within 2 years of a cancer diagnosis. Testing trends were analyzed with logistic regression modeling. The results of sequencing each gene, including variants associated with increased cancer risk (pathogenic results) and variants whose cancer risk association was unknown (uncertain results), were evaluated. The genes were categorized according to their primary cancer association, including breast or ovarian, gastrointestinal, and other, and whether practice guidelines recommended germline testing.


              Results

                Among 1 369 602 patients diagnosed with cancer between 2013 and 2019 in California and Georgia, 93 052 (6.8\%) underwent germline testing through March 31, 2021. The proportion of patients tested varied by cancer type: male breast (50\%), ovarian (38.6\%), female breast (26\%), multiple (7.5\%), endometrial (6.4\%), pancreatic (5.6\%), colorectal (5.6\%), prostate (1.1\%), and lung (0.3\%). In a logistic regression model, compared with the 31\% (95\% CI, 30\%-31\%) of non-Hispanic White patients with male breast cancer, female breast cancer, or ovarian cancer who underwent testing, patients of other races and ethnicities underwent testing less often: 22\% (95\% CI, 21\%-22\%) of Asian patients, 25\% (95\% CI, 24\%-25\%) of Black patients, and 23\% (95\% CI, 23\%-23\%) of Hispanic patients (
                P
                 \&amp;lt; .001 using the χ
                2
                test). Of all pathogenic results, 67.5\% to 94.9\% of variants were identified in genes for which practice guidelines recommend testing and 68.3\% to 83.8\% of variants were identified in genes associated with the diagnosed cancer type.



              Conclusions and Relevance
              Among patients diagnosed with cancer in California and Georgia between 2013 and 2019, only 6.8\% underwent germline genetic testing. Compared with non-Hispanic White patients, rates of testing were lower among Asian, Black, and Hispanic patients.},
	language = {en},
	number = {1},
	urldate = {2025-02-21},
	journal = {JAMA},
	author = {Kurian, Allison W. and Abrahamse, Paul and Furgal, Allison and Ward, Kevin C. and Hamilton, Ann S. and Hodan, Rachel and Tocco, Rachel and Liu, Lihua and Berek, Jonathan S. and Hoang, Lily and Yussuf, Amal and Susswein, Lisa and Esplin, Edward D. and Slavin, Thomas P. and Gomez, Scarlett L. and Hofer, Timothy P. and Katz, Steven J.},
	month = jul,
	year = {2023},
	keywords = {Genetic Testing, GYN Oncology},
	pages = {43},
}

@article{liu_timing_2023,
	title = {Timing of interval debulking surgery and postoperative chemotherapy after neoadjuvant chemotherapy in advanced epithelial ovarian cancer: a multicenter real-world study},
	volume = {16},
	issn = {1757-2215},
	shorttitle = {Timing of interval debulking surgery and postoperative chemotherapy after neoadjuvant chemotherapy in advanced epithelial ovarian cancer},
	url = {https://ovarianresearch.biomedcentral.com/articles/10.1186/s13048-023-01164-8},
	doi = {10.1186/s13048-023-01164-8},
	abstract = {Abstract

              Background
              To investigate the prognostic relevance of the time to interval debulking surgery (TTS) and the time to postoperative adjuvant chemotherapy (TTC) after the completion of neoadjuvant chemotherapy (NACT).


              Methods
              A retrospective real-word study included 658 patients with histologically confirmed advanced epithelial ovarian cancer who received NACT at seven tertiary hospitals in China from June 2008 to June 2020. TTS was defined as the time interval from the completion of NACT to the time of interval debulking surgery (IDS). TTC was defined as the time interval from the completion of NACT to the initiation of postoperative adjuvant chemotherapy (PACT).


              Results

                The median TTS and TTC were 25 (IQR, 20–29) and 40 (IQR, 33–49) days, respectively. Patients with TTS {\textgreater} 25 days were older (55 vs. 53 years,
                P
                 = 0.012) and received more NACT cycles (median, 3 vs. 2,
                P
                 = 0.002). Similar results were observed in patients with TTC {\textgreater} 40 days. In the multivariate analyses, TTS and TTC were not associated with PFS when stratified by median, quartile, or integrated as continuous variables (all
                P
                 {\textgreater} 0.05). However, TTS and TTC were significantly associated with worse OS when stratified by median (
                P
                 = 0.018 and 0.018, respectively), quartile (
                P
                 = 0.169, 0.014, 0.027 and 0.012, 0.001, 0.033, respectively), or integrated as continuous variables (
                P
                 = 0.018 and 0.011, respectively). Similarly, increasing TTS and TTC intervals were associated with a higher risk of death (
                P

                  trend

                = 0.016 and 0.031, respectively) but not with recurrence (
                P

                  trend

                = 0.103 and 0.381, respectively).



              Conclusion
              The delays of IDS and PACT after the completion of NACT have adverse impacts on OS but no impacts on PFS, which indicates that reducing delays of IDS and PACT might ameliorate the outcomes of ovarian cancer patients treated with NACT.},
	language = {en},
	number = {1},
	urldate = {2025-02-21},
	journal = {J Ovarian Res},
	author = {Liu, Xingyu and Zhao, Yingjun and Jiao, Xiaofei and Yu, Yang and Li, Ruyuan and Zeng, Shaoqing and Chi, Jianhua and Ma, Guanchen and Huo, Yabing and Li, Ming and Peng, Zikun and Liu, Jiahao and Zhou, Qi and Zou, Dongling and Wang, Li and Li, Qingshui and Wang, Jing and Yao, Shuzhong and Chen, Youguo and Ma, Ding and Hu, Ting and Gao, Qinglei},
	month = jun,
	year = {2023},
	keywords = {GYN Oncology},
	pages = {121},
}

@article{chan_association_2016,
	title = {The association between timing of initiation of adjuvant therapy and the survival of early stage ovarian cancer patients – {An} analysis of {NRG} {Oncology}/{Gynecologic} {Oncology} {Group} trials},
	volume = {143},
	issn = {00908258},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0090825816314214},
	doi = {10.1016/j.ygyno.2016.09.015},
	language = {en},
	number = {3},
	urldate = {2025-02-21},
	journal = {Gynecologic Oncology},
	author = {Chan, John K. and Java, James J. and Fuh, Katherine and Monk, Bradley J. and Kapp, Daniel S. and Herzog, Thomas and Bell, Jeffrey and Young, Robert},
	month = dec,
	year = {2016},
	keywords = {GYN Oncology},
	pages = {490--495},
}

@article{chan_jk_java_jj_fuh_k_monk_bj_kapp_ds_herzog_t_bell_j_young_r_association_nodate,
	title = {The association between timing of initiation of adjuvant therapy and the survival of early stage ovarian cancer patients - {An} analysis of {NRG} {Oncology}/{Gynecologic} {Oncology} {Group} trials.},
	doi = {10.1016/j.ygyno.2016.09.015},
	journal = {Gynecological Oncology},
	author = {Chan JK, Java JJ, Fuh K, Monk BJ, Kapp DS, Herzog T, Bell J, Young R.},
}

@article{zhou_note_2010,
	title = {A {Note} on {Bayesian} {Inference} {After} {Multiple} {Imputation}},
	volume = {64},
	issn = {0003-1305},
	url = {https://www.jstor.org/stable/20799887},
	abstract = {This article is aimed at practitioners who plan to use Bayesian inference on multiply-imputed datasets in settings where posterior distributions of the parameters of interest are not approximately Gaussian. We seek to steer practitioners away from a naive approach to Bayesian inference, namely estimating the posterior distribution in each completed dataset and averaging functionals of these distributions. We demonstrate that this approach results in unreliable inferences. A better approach is to mix draws from the posterior distributions from each completed dataset, and use the mixed draws to summarize the posterior distribution. Using simulations, we show that for this second approach to work well, the number of imputed datasets should be large. In particular, five to ten imputed datasets—which is the standard recommendation for multiple imputation—is generally not enough to result in reliable Bayesian inferences.},
	number = {2},
	urldate = {2025-02-20},
	journal = {The American Statistician},
	author = {Zhou, Xiang and Reiter, Jerome P.},
	year = {2010},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	keywords = {Bayesian, missing data},
	pages = {159--163},
}

@book{boyd_introduction_2018,
	edition = {1},
	title = {Introduction to {Applied} {Linear} {Algebra}: {Vectors}, {Matrices}, and {Least} {Squares}},
	copyright = {https://www.cambridge.org/core/terms},
	isbn = {978-1-108-58366-4 978-1-316-51896-0},
	shorttitle = {Introduction to {Applied} {Linear} {Algebra}},
	url = {https://www.cambridge.org/highereducation/product/9781108583664/book},
	abstract = {This groundbreaking textbook combines straightforward explanations with a wealth of practical examples to offer an innovative approach to teaching linear algebra. Requiring no prior knowledge of the subject, it covers the aspects of linear algebra - vectors, matrices, and least squares - that are needed for engineering applications, discussing examples across data science, machine learning and artificial intelligence, signal and image processing, tomography, navigation, control, and finance. The numerous practical exercises throughout allow students to test their understanding and translate their knowledge into solving real-world problems, with lecture slides, additional computational exercises in Julia and MATLAB®, and data sets accompanying the book online. Suitable for both one-semester and one-quarter courses, as well as self-study, this self-contained text provides beginning students with the foundation they need to progress to more advanced study.},
	urldate = {2025-02-18},
	publisher = {Cambridge University Press \& Assessment},
	author = {Boyd, Stephen and Vandenberghe, Lieven},
	month = jun,
	year = {2018},
	doi = {10.1017/9781108583664},
	file = {Boyd and Vandenberghe - 2018 - Introduction to Applied Linear Algebra Vectors, M.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\D4YYPCYC\\Boyd and Vandenberghe - 2018 - Introduction to Applied Linear Algebra Vectors, M.pdf:application/pdf},
}

@book{agresti_foundations_2015,
	title = {Foundations of linear and generalized linear models},
	publisher = {Wiley},
	author = {Agresti, Alan},
	year = {2015},
	file = {Agresti - Foundations of linear and generalized linear model.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\RA47Z49G\\Agresti - Foundations of linear and generalized linear model.pdf:application/pdf},
}

@book{agresti_introduction_2007,
	edition = {2nd},
	title = {Introduction to {Categorical} {Data} {Analysis}},
	language = {en},
	author = {Agresti, Alan},
	year = {2007},
	file = {Agresti - Introduction to Categorical Data Analysis.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\ML4RPFS5\\Agresti - Introduction to Categorical Data Analysis.pdf:application/pdf},
}

@article{peltrini_efficacy_2020,
	title = {Efficacy of transversus abdominis plane ({TAP}) block in colorectal surgery: a systematic review and meta-analysis},
	volume = {24},
	issn = {1128-045X},
	shorttitle = {Efficacy of transversus abdominis plane ({TAP}) block in colorectal surgery},
	url = {https://doi.org/10.1007/s10151-020-02206-9},
	doi = {10.1007/s10151-020-02206-9},
	abstract = {Multimodal opioid-sparing analgesia is a key component of the enhanced recovery after surgery (ERAS) protocol for postoperative pain management. Transversus abdominis plane (TAP) block has contributed to the implementation of this approach in different kinds of surgical procedures. The aim of this study was to evaluate the efficacy of TAP block and its impact on recovery in colorectal surgery.},
	language = {en},
	number = {8},
	urldate = {2025-02-06},
	journal = {Tech Coloproctol},
	author = {Peltrini, R. and Cantoni, V. and Green, R. and Greco, P. A. and Calabria, M. and Bucci, L. and Corcione, F.},
	month = aug,
	year = {2020},
	keywords = {Colorectal surgery, Postoperative analgesia, Postoperative pain, TAP block, Transversus abdominis plane block},
	pages = {787--802},
}

@article{truong_prospective_2021,
	title = {A {Prospective} {Randomized} {Trial} of {Surgeon}-{Administered} {Intraoperative} {Transversus} {Abdominis} {Plane} {Block} {With} {Bupivacaine} {Against} {Liposomal} {Bupivacaine}: {The} {TINGLE} {Trial}},
	volume = {64},
	issn = {0012-3706},
	shorttitle = {A {Prospective} {Randomized} {Trial} of {Surgeon}-{Administered} {Intraoperative} {Transversus} {Abdominis} {Plane} {Block} {With} {Bupivacaine} {Against} {Liposomal} {Bupivacaine}},
	url = {https://journals.lww.com/dcrjournal/fulltext/2021/07000/a_prospective_randomized_trial_of.14.aspx},
	doi = {10.1097/DCR.0000000000002008},
	abstract = {BACKGROUND: 
          Transversus abdominis plane blocks are increasingly used to achieve opioid-sparing analgesia after colorectal surgery. Traditionally, bupivacaine was the long-acting analgesic of choice, but the addition of dexamethasone and/or epinephrine to bupivacaine may extend block duration. Liposomal bupivacaine has also been suggested to achieve an extended analgesia duration of 72 hours but is significantly more expensive.
          OBJECTIVE: 
          The purpose of this study was to compare pain control between laparoscopic transversus abdominis plane blocks using liposomal bupivacaine versus bupivacaine with epinephrine and dexamethasone.
          DESIGN: 
          This was a parallel-group, single-institution, randomized clinical trial.
          SETTINGS: 
          The study was conducted at a single tertiary medical center.
          PATIENTS: 
          Consecutive patients between October 2018 to October 2019, ages 18 to 90 years, undergoing minimally invasive colorectal surgery with multimodal analgesia were included.
          INTERVENTIONS: 
          Patients were randomly assigned 1:1 to receive a laparoscopic transversus abdominis plane block with liposomal bupivacaine or bupivacaine with epinephrine and dexamethasone.
          MAIN OUTCOME MEASURES: 
          The primary outcome was total oral morphine equivalents administered in the first 48 hours postoperatively. Secondary outcomes included pain scores, time to ambulation and solid diet, hospital length of stay, and complications.
          RESULTS: 
          A total of 102 patients (50 men) with a median age of 42 years (interquartile range, 29–60 y) consented and were randomly assigned. The primary end point, total oral morphine equivalents administered in the first 48 hours, was not significantly different between the liposomal bupivacaine group (median = 69 mg) and the bupivacaine with epinephrine and dexamethasone group (median = 47 mg; difference in medians = 22 mg, (95\% CI, –17 to 49 mg); p = 0.60). There were no significant differences in pain scores, time to ambulation, time to diet tolerance, time to bowel movement, length of stay, overall complications, or readmission rate between groups. There were no treatment-related adverse outcomes.
          LIMITATIONS: 
          This study was not placebo controlled or blinded.
          CONCLUSIONS: 
          This first randomized trial comparing laparoscopic transversus abdominis plane block with liposomal bupivacaine or bupivacaine with epinephrine and dexamethasone showed that a liposomal bupivacaine block does not provide superior or extended analgesia in the era of standardized multimodal analgesia protocols.
          See Video Abstract at https://links.lww.com/DCR/B533.
          ESTUDIO PROSPECTIVO Y RANDOMIZADO DE BLOQUEO DEL PLANO MUSCULAR TRANSVERSO DEL ABDOMEN REALIZADO POR EL CIRUJANO CON BUPIVACAÍNA VERSUS BUPIVACAÍNA LIPOSOMAL: ESTUDIO TINGLE 
          ANTECEDENTES:
          El bloqueo anestésico del plano muscular transverso del abdomen se utiliza cada vez más para lograr una analgesia con menos consumo de opioides después de cirugía colorrectal. Tradicionalmente, la Bupivacaína era el analgésico de acción prolongada de elección, pero al agregarse Dexametasona y/o Adrenalina a la Bupivacaína se puede prolongar la duración del bloqueo. También se ha propuesto que la Bupivacaína liposomal logra una duración prolongada de la analgesia de 72 horas, pero es significativamente más cara.
          OBJETIVO:
          Comparar el control del dolor entre bloqueo laparoscópico del plano de los transversos del abdomen usando Bupivacaína liposomal versus Bupivacaína con Adrenalina y Dexametasona.
          DISEÑO:
          Estudio clínico prospectivo y randomizado de una sola institución en grupos paralelos.
          AJUSTE:
          Centro médico terciario único.
          PACIENTES:
          Todos aquellos pacientes entre 18 y 90 años sometidos a cirugía colorrectal mínimamente invasiva con analgesia multimodal, entre octubre de 2018 a octubre de 2019 incluidos de manera consecutiva.
          INTERVENCIONES:
          Los pacientes fueron seleccionados aleatoriamente 1:1 para recibir un bloqueo laparoscópico del plano de los transversos del abdomen con Bupivacaína liposomal o Bupivacaína con Adrenalina y Dexametasona.
          PRINCIPALES MEDIDAS DE RESULTADO:
          El resultado primario fue el total de equivalentes de morfina oral administradas en las primeras 48 horas después de la operación. Los resultados secundarios incluyeron puntuaciones de dolor, inicio de dieta sólida, tiempo de inicio a la deambulación, la estadía hospitalaria y las complicaciones.
          RESULTADOS:
          Un total de 102 pacientes (50 hombres) con una mediana de edad de 42 años (IQR 29-60) fueron incluidos aleatoriamente. El criterio de valoración principal, equivalentes de morfina oral total administrada en las primeras 48 horas, no fue significativamente diferente entre el grupo de Bupivacaína liposomal (mediana = 69 mg) y el grupo de Bupivacaína con Adrenalina y Dexametasona (mediana = 47 mg; diferencia en medianas = 22 mg, IC del 95\% [-17] - 49 mg, p = 0,60). No hubo diferencias significativas en las puntuaciones de dolor, tiempo de inicio a la deambulación, el tiempo de tolerancia a la dieta sólida, el tiempo hasta el primer evacuado intestinal, la duración de la estadía hospitalaria, las complicaciones generales o la tasa de readmisión entre los grupos. No hubo resultados adversos relacionados con el tratamiento.
          LIMITACIONES:
          Este estudio no fue controlado con placebo ni de manera cegada.
          CONCLUSIONES:
          Este primer estudio prospectivo y randomizado que comparó el bloqueo del plano de los músculos transversos del abdomen por vía laparoscópica, utilizando Bupivacaína liposomal o Bupivacaína con Adrenalina y Dexametasona, demostró que el bloqueo de Bupivacaína liposomal no proporciona ni mejor analgesia ni un efecto mas prolongado.
          Consulte Video Resumen en https://links.lww.com/DCR/B533.},
	language = {en-US},
	number = {7},
	urldate = {2025-02-06},
	journal = {Diseases of the Colon \& Rectum},
	author = {Truong, Adam and Fleshner, Phillip R. and Mirocha, James M. and Tran, Hai P. and Shane, Rita and Zaghiyan, Karen N.},
	month = jul,
	year = {2021},
	pages = {888},
	file = {Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\L9697VN2\\a_prospective_randomized_trial_of.14.html:text/html},
}

@article{andersen_causal_2017-1,
	title = {Causal inference in survival analysis using pseudo-observations},
	volume = {36},
	copyright = {Copyright © 2017 John Wiley \& Sons, Ltd.},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7297},
	doi = {10.1002/sim.7297},
	abstract = {Causal inference for non-censored response variables, such as binary or quantitative outcomes, is often based on either (1) direct standardization (‘G-formula’) or (2) inverse probability of treatment assignment weights (‘propensity score’). To do causal inference in survival analysis, one needs to address right-censoring, and often, special techniques are required for that purpose. We will show how censoring can be dealt with ‘once and for all’ by means of so-called pseudo-observations when doing causal inference in survival analysis. The pseudo-observations can be used as a replacement of the outcomes without censoring when applying ‘standard’ causal inference methods, such as (1) or (2) earlier. We study this idea for estimating the average causal effect of a binary treatment on the survival probability, the restricted mean lifetime, and the cumulative incidence in a competing risks situation. The methods will be illustrated in a small simulation study and via a study of patients with acute myeloid leukemia who received either myeloablative or non-myeloablative conditioning before allogeneic hematopoetic cell transplantation. We will estimate the average causal effect of the conditioning regime on outcomes such as the 3-year overall survival probability and the 3-year risk of chronic graft-versus-host disease. Copyright © 2017 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {17},
	urldate = {2025-01-31},
	journal = {Statistics in Medicine},
	author = {Andersen, Per K. and Syriopoulou, Elisavet and Parner, Erik T.},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.7297},
	keywords = {causal inference, survival analysis},
	pages = {2669--2681},
	file = {Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\X7673CD5\\sim.html:text/html},
}

@misc{schenk_modeling_2024,
	title = {Modeling the restricted mean survival time using pseudo-value random forests},
	url = {http://arxiv.org/abs/2411.01381},
	doi = {10.48550/arXiv.2411.01381},
	abstract = {The restricted mean survival time (RMST) has become a popular measure to summarize event times in longitudinal studies. Defined as the area under the survival function up to a time horizon \${\textbackslash}tau\$ {\textgreater} 0, the RMST can be interpreted as the life expectancy within the time interval [0, \${\textbackslash}tau\$]. In addition to its straightforward interpretation, the RMST also allows for the definition of valid estimands for the causal analysis of treatment contrasts in medical studies. In this work, we introduce a non-parametric approach to model the RMST conditional on a set of baseline variables (including, e.g., treatment variables and confounders). Our method is based on a direct modeling strategy for the RMST, using leave-one-out jackknife pseudo-values within a random forest regression framework. In this way, it can be employed to obtain precise estimates of both patient-specific RMST values and confounder-adjusted treatment contrasts. Since our method (termed "pseudo-value random forest", PVRF) is model-free, RMST estimates are not affected by restrictive assumptions like the proportional hazards assumption. Particularly, PVRF offers a high flexibility in detecting relevant covariate effects from higher-dimensional data, thereby expanding the range of existing pseudo-value modeling techniques for RMST estimation. We investigate the properties of our method using simulations and illustrate its use by an application to data from the SUCCESS-A breast cancer trial. Our numerical experiments demonstrate that PVRF yields accurate estimates of both patient-specific RMST values and RMST-based treatment contrasts.},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Schenk, Alina and Basten, Vanessa and Schmid, Matthias},
	month = nov,
	year = {2024},
	note = {arXiv:2411.01381 [stat]},
	keywords = {causal inference, rmst},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\BUPKXCHM\\Schenk et al. - 2024 - Modeling the restricted mean survival time using p.pdf:application/pdf;Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\A4X35F8R\\Schenk et al. - 2024 - Modeling the restricted mean survival time using p.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\QXJIHCWW\\2411.html:text/html},
}

@article{brodersen_inferring_2015,
	title = {Inferring causal impact using {Bayesian} structural time-series models},
	volume = {9},
	issn = {1932-6157, 1941-7330},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-9/issue-1/Inferring-causal-impact-using-Bayesian-structural-time-series-models/10.1214/14-AOAS788.full},
	doi = {10.1214/14-AOAS788},
	abstract = {An important problem in econometrics and marketing is to infer the causal impact that a designed market intervention has exerted on an outcome metric over time. This paper proposes to infer causal impact on the basis of a diffusion-regression state-space model that predicts the counterfactual market response in a synthetic control that would have occurred had no intervention taken place. In contrast to classical difference-in-differences schemes, state-space models make it possible to (i) infer the temporal evolution of attributable impact, (ii) incorporate empirical priors on the parameters in a fully Bayesian treatment, and (iii) flexibly accommodate multiple sources of variation, including local trends, seasonality and the time-varying influence of contemporaneous covariates. Using a Markov chain Monte Carlo algorithm for posterior inference, we illustrate the statistical properties of our approach on simulated data. We then demonstrate its practical utility by estimating the causal effect of an online advertising campaign on search-related site visits. We discuss the strengths and limitations of state-space models in enabling causal attribution in those settings where a randomised experiment is unavailable. The CausalImpact R package provides an implementation of our approach.},
	number = {1},
	urldate = {2025-01-23},
	journal = {The Annals of Applied Statistics},
	author = {Brodersen, Kay H. and Gallusser, Fabian and Koehler, Jim and Remy, Nicolas and Scott, Steven L.},
	month = mar,
	year = {2015},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {causal inference, Bayesian, synthetic controls},
	pages = {247--274},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\VG7XVGK6\\Brodersen et al. - 2015 - Inferring causal impact using Bayesian structural .pdf:application/pdf},
}

@book{imbens_causal_2015,
	title = {Causal {Inference} in {Statistics}, {Social}, and {Biomedical} {Sciences}},
	isbn = {978-0-521-88588-1},
	abstract = {Most questions in social and biomedical sciences are causal in nature: what would happen to individuals, or to groups, if part of their environment were changed? In this groundbreaking text, two world-renowned experts present statistical methods for studying such questions. This book starts with the notion of potential outcomes, each corresponding to the outcome that would be realized if a subject were exposed to a particular treatment or regime. In this approach, causal effects are comparisons of such potential outcomes. The fundamental problem of causal inference is that we can only observe one of the potential outcomes for a particular subject. The authors discuss how randomized experiments allow us to assess causal effects and then turn to observational studies. They lay out the assumptions needed for causal inference and describe the leading analysis methods, including, matching, propensity-score methods, and instrumental variables. Many detailed applications are included, with special focus on practical aspects for the empirical researcher.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Imbens, Guido W. and Rubin, Donald B.},
	month = apr,
	year = {2015},
	note = {Google-Books-ID: Bf1tBwAAQBAJ},
}

@article{yu_bayesian_2023,
	title = {Bayesian mediation analysis methods to explore racial/ethnic disparities in anxiety among cancer survivors},
	volume = {50},
	issn = {1349-6964},
	url = {https://doi.org/10.1007/s41237-022-00185-9},
	doi = {10.1007/s41237-022-00185-9},
	abstract = {Third-variables refer to the middle variables that are positioned in the pathway between an exposure and an outcome variable. Mediation analysis is a statistical approach to identify third variables, and to estimate and test third-variable effects that explain the exposure – outcome association. In this paper, we propose three methods for mediation analysis in Bayesian settings: (1) the function of coefficients method, (2) the product of partial differences method, and (3) the resampling method. The explicit benefit of the Bayesian mediation analysis is that the hierarchical relationships between the exposure variable and third variables, and between third variables and the outcome are naturally built into the Bayesian models. We performed sensitivity analysis to assess the impact of the choice of prior distributions in the three Bayesian inference methods. We found that the proposed methods are robust across a range of priors. Finally, we illustrate the proposed methods using real data from the MY-Health Study to explore racial/ethnic disparities in anxiety among cancer survivors. The results are comparable to those from the Frequentist’s general mediation analysis but request shorter computing time.},
	language = {en},
	number = {1},
	urldate = {2025-01-23},
	journal = {Behaviormetrika},
	author = {Yu, Qingzhao and Cao, Wentao and Mercante, Donald and Wu, Xiaocheng and Li, Bin},
	month = jan,
	year = {2023},
	keywords = {causal inference, Bayesian, mediation},
	pages = {361--383},
}

@article{gao_bayesian_2019,
	title = {Bayesian causal mediation analysis with multiple ordered mediators},
	volume = {19},
	issn = {1471-082X},
	url = {https://doi.org/10.1177/1471082X18798067},
	doi = {10.1177/1471082X18798067},
	abstract = {Causal mediation analysis provides investigators insight into how a treatment or exposure can affect an outcome of interest through one or more mediators on causal pathway. When multiple mediators on the pathway are causally ordered, identification of mediation effects on certain causal pathways requires a sensitivity parameter to be specified. A mixed model-based approach was proposed in the Bayesian framework to connect potential outcomes at different treatment levels, and identify mediation effects independent of a sensitivity parameter, for the natural direct and indirect effects on all causal pathways. The proposed method is illustrated in a linear setting for mediators and outcome, with mediator-treatment interactions. Sensitivity analysis was performed for the prior choices in the Bayesian models. The proposed Bayesian method was applied to an adolescent dental health study, to see how social economic status can affect dental caries through a sequence of causally ordered mediators in dental visit and oral hygiene index.},
	language = {en},
	number = {6},
	urldate = {2025-01-23},
	journal = {Statistical Modelling},
	author = {Gao, Tianming and Albert, Jeffrey M},
	month = dec,
	year = {2019},
	note = {Publisher: SAGE Publications India},
	pages = {634--652},
	file = {Accepted Version:C\:\\Users\\anbe6\\Zotero\\storage\\ENV7RUTZ\\Gao and Albert - 2019 - Bayesian causal mediation analysis with multiple o.pdf:application/pdf},
}

@misc{brathovde_formalizing_2024,
	title = {Formalizing the causal interpretation in accelerated failure time models with unmeasured heterogeneity},
	url = {http://arxiv.org/abs/2409.01983},
	doi = {10.48550/arXiv.2409.01983},
	abstract = {In the presence of unmeasured heterogeneity, the hazard ratio for exposure has a complex causal interpretation. To address this, accelerated failure time (AFT) models, which assess the effect on the survival time ratio scale, are often suggested as a better alternative. AFT models also allow for straightforward confounder adjustment. In this work, we formalize the causal interpretation of the acceleration factor in AFT models using structural causal models and data under independent censoring. We prove that the acceleration factor is a valid causal effect measure, even in the presence of frailty and treatment effect heterogeneity. Through simulations, we show that the acceleration factor better captures the causal effect than the hazard ratio when both AFT and proportional hazards models apply. Additionally, we extend the interpretation to systems with time-dependent acceleration factors, revealing the challenge of distinguishing between a time-varying homogeneous effect and unmeasured heterogeneity. While the causal interpretation of acceleration factors is promising, we caution practitioners about potential challenges in estimating these factors in the presence of effect heterogeneity.},
	urldate = {2025-01-15},
	publisher = {arXiv},
	author = {Brathovde, Mari and Putter, Hein and Valberg, Morten and Post, Richard A. J.},
	month = sep,
	year = {2024},
	note = {arXiv:2409.01983 [stat]},
	keywords = {causal inference, survival analysis},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\PMIRWDCD\\Brathovde et al. - 2024 - Formalizing the causal interpretation in accelerat.pdf:application/pdf;Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\PWGD8J4W\\Brathovde et al. - 2024 - Formalizing the causal interpretation in accelerat.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\28ZQAGDQ\\2409.html:text/html},
}

@article{yimer_bayesgmed_2023,
	title = {{BayesGmed}: {An} {R}-package for {Bayesian} causal mediation analysis},
	volume = {18},
	issn = {1932-6203},
	shorttitle = {{BayesGmed}},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0287037},
	doi = {10.1371/journal.pone.0287037},
	abstract = {Background The past decade has seen an explosion of research in causal mediation analysis. However, most analytic tools developed so far rely on frequentist methods which may not be robust in the case of small sample sizes. In this paper, we propose a Bayesian approach for causal mediation analysis based on Bayesian g-formula, which will overcome the limitations of the frequentist methods. Methods We created BayesGmed, an R-package for fitting Bayesian mediation models in R. The application of the methodology (and software tool) is demonstrated by a secondary analysis of data collected as part of the MUSICIAN study, a randomised controlled trial of remotely delivered cognitive behavioural therapy (tCBT) for people with chronic pain. We tested the hypothesis that the effect of tCBT would be mediated by improvements in active coping, passive coping, fear of movement and sleep problems. We then demonstrate the use of informative priors to conduct probabilistic sensitivity analysis around violations of causal identification assumptions. Result The analysis of MUSICIAN data shows that tCBT has better-improved patients’ self-perceived change in health status compared to treatment as usual (TAU). The adjusted log-odds of tCBT compared to TAU range from 1.491 (95\% CI: 0.452–2.612) when adjusted for sleep problems to 2.264 (95\% CI: 1.063–3.610) when adjusted for fear of movement. Higher scores of fear of movement (log-odds, -0.141 [95\% CI: -0.245, -0.048]), passive coping (log-odds, -0.217 [95\% CI: -0.351, -0.104]), and sleep problem (log-odds, -0.179 [95\% CI: -0.291, -0.078]) leads to lower odds of a positive self-perceived change in health status. The result of BayesGmed, however, shows that none of the mediated effects are statistically significant. We compared BayesGmed with the mediation R- package, and the results were comparable. Finally, our sensitivity analysis using the BayesGmed tool shows that the direct and total effect of tCBT persists even for a large departure in the assumption of no unmeasured confounding. Conclusion This paper comprehensively overviews causal mediation analysis and provides an open-source software package to fit Bayesian causal mediation models.},
	language = {en},
	number = {6},
	urldate = {2025-01-15},
	journal = {PLOS ONE},
	author = {Yimer, Belay B. and Lunt, Mark and Beasley, Marcus and Macfarlane, Gary J. and McBeth, John},
	month = jun,
	year = {2023},
	note = {Publisher: Public Library of Science},
	keywords = {R, causal inference, Bayesian},
	pages = {e0287037},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\WI5XUF8B\\Yimer et al. - 2023 - BayesGmed An R-package for Bayesian causal mediat.pdf:application/pdf},
}

@article{chatton_causal_2024,
	title = {The {Causal} {Cookbook}: {Recipes} for {Propensity} {Scores}, {G}-{Computation}, and {Doubly} {Robust} {Standardization}},
	volume = {7},
	issn = {2515-2459},
	shorttitle = {The {Causal} {Cookbook}},
	url = {https://doi.org/10.1177/25152459241236149},
	doi = {10.1177/25152459241236149},
	abstract = {Recent developments in the causal-inference literature have renewed psychologists? interest in how to improve causal conclusions based on observational data. A lot of the recent writing has focused on concerns of causal identification (under which conditions is it, in principle, possible to recover causal effects?); in this primer, we turn to causal estimation (how do researchers actually turn the data into an effect estimate?) and modern approaches to it that are commonly used in epidemiology. First, we explain how causal estimands can be defined rigorously with the help of the potential-outcomes framework, and we highlight four crucial assumptions necessary for causal inference to succeed (exchangeability, positivity, consistency, and noninterference). Next, we present three types of approaches to causal estimation and compare their strengths and weaknesses: propensity-score methods (in which the independent variable is modeled as a function of controls), g-computation methods (in which the dependent variable is modeled as a function of both controls and the independent variable), and doubly robust estimators (which combine models for both independent and dependent variables). A companion R Notebook is available at github.com/ArthurChatton/CausalCookbook. We hope that this nontechnical introduction not only helps psychologists and other social scientists expand their causal toolbox but also facilitates communication across disciplinary boundaries when it comes to causal inference, a research goal common to all fields of research.},
	number = {1},
	urldate = {2025-01-15},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Chatton, Arthur and Rohrer, Julia M.},
	month = jan,
	year = {2024},
	note = {Publisher: SAGE Publications Inc},
	keywords = {causal inference},
	pages = {25152459241236149},
	file = {Chatton and Rohrer - 2024 - The Causal Cookbook Recipes for Propensity Scores.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\DPECCLKU\\Chatton and Rohrer - 2024 - The Causal Cookbook Recipes for Propensity Scores.pdf:application/pdf},
}

@article{zhang_bayesian_2023,
	title = {Bayesian nonparametric analysis of restricted mean survival time},
	volume = {79},
	copyright = {© 2022 The International Biometric Society.},
	issn = {1541-0420},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.13622},
	doi = {10.1111/biom.13622},
	abstract = {The restricted mean survival time (RMST) evaluates the expectation of survival time truncated by a prespecified time point, because the mean survival time in the presence of censoring is typically not estimable. The frequentist inference procedure for RMST has been widely advocated for comparison of two survival curves, while research from the Bayesian perspective is rather limited. For the RMST of both right- and interval-censored data, we propose Bayesian nonparametric estimation and inference procedures. By assigning a mixture of Dirichlet processes (MDP) prior to the distribution function, we can estimate the posterior distribution of RMST. We also explore another Bayesian nonparametric approach using the Dirichlet process mixture model and make comparisons with the frequentist nonparametric method. Simulation studies demonstrate that the Bayesian nonparametric RMST under diffuse MDP priors leads to robust estimation and under informative priors it can incorporate prior knowledge into the nonparametric estimator. Analysis of real trial examples demonstrates the flexibility and interpretability of the Bayesian nonparametric RMST for both right- and interval-censored data.},
	language = {en},
	number = {2},
	urldate = {2025-01-15},
	journal = {Biometrics},
	author = {Zhang, Chenyang and Yin, Guosheng},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/biom.13622},
	keywords = {rmst},
	pages = {1383--1396},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\JZ8MJFDN\\Zhang and Yin - 2023 - Bayesian nonparametric analysis of restricted mean.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\Z6AMFVSY\\biom.html:text/html},
}

@article{han_restricted_2022,
	title = {Restricted {Mean} {Survival} {Time} for {Survival} {Analysis}: {A} {Quick} {Guide} for {Clinical} {Researchers}},
	volume = {23},
	issn = {1229-6929},
	shorttitle = {Restricted {Mean} {Survival} {Time} for {Survival} {Analysis}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9081686/},
	doi = {10.3348/kjr.2022.0061},
	number = {5},
	urldate = {2024-02-16},
	journal = {Korean J Radiol},
	author = {Han, Kyunghwa and Jung, Inkyung},
	month = may,
	year = {2022},
	pmid = {35506526},
	pmcid = {PMC9081686},
	keywords = {rmst},
	pages = {495--499},
	file = {PubMed Central Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\AF7HZ9FU\\Han and Jung - 2022 - Restricted Mean Survival Time for Survival Analysi.pdf:application/pdf},
}

@misc{hanada_bayesian_2024,
	title = {Bayesian {Parametric} {Methods} for {Deriving} {Distribution} of {Restricted} {Mean} {Survival} {Time}},
	url = {http://arxiv.org/abs/2406.06071},
	doi = {10.48550/arXiv.2406.06071},
	abstract = {We propose a Bayesian method for deriving the distribution of restricted mean survival time (RMST) using posterior samples, which accounts for covariates and heterogeneity among clusters based on a parametric model for survival time. We derive an explicit RMST equation by devising an integral of the survival function, allowing for the calculation of not only the mean and credible interval but also the mode, median, and probability of exceeding a certain value. Additionally, We propose two methods: one using random effects to account for heterogeneity among clusters and another utilizing frailty. We developed custom Stan code for the exponential, Weibull, log-normal frailty, and log-logistic models, as they cannot be processed using the brm functions in R. We evaluate our proposed methods through computer simulations and analyze real data from the eight Empowered Action Group states in India to confirm consistent results across states after adjusting for cluster differences. In conclusion, we derived explicit RMST formulas for parametric models and their distributions, enabling the calculation of the mean, median, mode, and credible interval. Our simulations confirmed the robustness of the proposed methods, and using the shrinkage effect allowed for more accurate results for each cluster.},
	urldate = {2025-01-15},
	publisher = {arXiv},
	author = {Hanada, Keisuke and Kojima, Masahiro},
	month = jun,
	year = {2024},
	note = {arXiv:2406.06071 [stat]},
	keywords = {Bayesian, rmst},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\C8S28EHK\\Hanada and Kojima - 2024 - Bayesian Parametric Methods for Deriving Distribut.pdf:application/pdf;Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\FDJBF5CV\\Hanada and Kojima - 2024 - Bayesian Parametric Methods for Deriving Distribut.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\MVNJQSRR\\2406.html:text/html},
}

@article{keil_bayesian_2018,
	title = {A {Bayesian} approach to the g-formula},
	volume = {27},
	issn = {0962-2802},
	url = {https://doi.org/10.1177/0962280217694665},
	doi = {10.1177/0962280217694665},
	abstract = {Epidemiologists often wish to estimate quantities that are easy to communicate and correspond to the results of realistic public health interventions. Methods from causal inference can answer these questions. We adopt the language of potential outcomes under Rubin’s original Bayesian framework and show that the parametric g-formula is easily amenable to a Bayesian approach. We show that the frequentist properties of the Bayesian g-formula suggest it improves the accuracy of estimates of causal effects in small samples or when data are sparse. We demonstrate an approach to estimate the effect of environmental tobacco smoke on body mass index among children aged 4–9 years who were enrolled in a longitudinal birth cohort in New York, USA. We provide an algorithm and supply SAS and Stan code that can be adopted to implement this computational approach more generally.},
	language = {en},
	number = {10},
	urldate = {2025-01-14},
	journal = {Stat Methods Med Res},
	author = {Keil, Alexander P and Daza, Eric J and Engel, Stephanie M and Buckley, Jessie P and Edwards, Jessie K},
	month = oct,
	year = {2018},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {Bayesian, g-computation},
	pages = {3183--3204},
	file = {Accepted Version:C\:\\Users\\anbe6\\Zotero\\storage\\KTRP2Q58\\Keil et al. - 2018 - A Bayesian approach to the g-formula.pdf:application/pdf},
}

@article{rothbard_tutorial_2024,
	title = {A {Tutorial} on {Applying} the {Difference}-in-{Differences} {Method} to {Health} {Data}},
	volume = {11},
	issn = {2196-2995},
	url = {https://doi.org/10.1007/s40471-023-00327-x},
	doi = {10.1007/s40471-023-00327-x},
	abstract = {Difference-in-differences analyses are a useful tool for estimating group-level decisions, such as policy changes, training programs, or other non-randomized interventions, on outcomes which occur within the intervention group. However, there is little practical advice on how to apply difference-in-differences to epidemiologic and health data. Here, we provide a tutorial on applying the difference-in-differences method to health services data, targeted at epidemiologists and other biomedical researchers.},
	language = {en},
	number = {2},
	urldate = {2025-01-09},
	journal = {Curr Epidemiol Rep},
	author = {Rothbard, Sarah and Etheridge, James C. and Murray, Eleanor J.},
	month = jun,
	year = {2024},
	keywords = {DiD},
	pages = {85--95},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\APRQJTEN\\Rothbard et al. - 2024 - A Tutorial on Applying the Difference-in-Differenc.pdf:application/pdf},
}

@article{kennedy-shaffer_quasi-experimental_2024,
	title = {Quasi-experimental methods for pharmacoepidemiology: difference-in-differences and synthetic control methods with case studies for vaccine evaluation},
	volume = {193},
	issn = {0002-9262},
	shorttitle = {Quasi-experimental methods for pharmacoepidemiology},
	url = {https://doi.org/10.1093/aje/kwae019},
	doi = {10.1093/aje/kwae019},
	abstract = {Difference-in-differences and synthetic control methods have become common study designs for evaluating the effects of changes in policies, including health policies. They also have potential for providing real-world effectiveness and safety evidence in pharmacoepidemiology. To effectively add to the toolkit of the field, however, designs—including both their benefits and drawbacks—must be well understood. Quasi-experimental designs provide an opportunity to estimate the average treatment effect on the treated without requiring the measurement of all possible confounding factors, and to assess population-level effects. This requires, however, other key assumptions, including the parallel trends or stable weighting assumptions, a lack of other concurrent events that could alter time trends, and an absence of contamination between exposed and unexposed units. The targeted estimands are also highly specific to the settings of the study, and combining across units or time periods can be challenging. Case studies are presented for 3 vaccine evaluation studies, showcasing some of these challenges and opportunities in a specific field of pharmacoepidemiology. These methods provide feasible and valuable sources of evidence in various pharmacoepidemiologic settings and can be improved through research to identify and weigh the advantages and disadvantages in those settings.This article is part of a Special Collection on Pharmacoepidemiology.},
	number = {7},
	urldate = {2025-01-08},
	journal = {American Journal of Epidemiology},
	author = {Kennedy-Shaffer, Lee},
	month = jul,
	year = {2024},
	keywords = {DiD, synthetic controls},
	pages = {1050--1058},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\GGU6VXMA\\Kennedy-Shaffer - 2024 - Quasi-experimental methods for pharmacoepidemiolog.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\NWL656II\\7624199.html:text/html},
}

@article{shih_health_2022,
	title = {Health {Economics} {Research} in {Cancer} {Screening}: {Research} {Opportunities}, {Challenges}, and {Future} {Directions}},
	volume = {2022},
	copyright = {https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model},
	issn = {1052-6773, 1745-6614},
	shorttitle = {Health {Economics} {Research} in {Cancer} {Screening}},
	url = {https://academic.oup.com/jncimono/article/2022/59/42/6631504},
	doi = {10.1093/jncimonographs/lgac008},
	abstract = {Cancer screening has long been considered a worthy public health investment. Health economics offers the theoretical foundation and research methodology to understand the demand- and supply-side factors associated with screening and evaluate screening-related policies and interventions. This article provides an overview of health economic theories and methods related to cancer screening and discusses opportunities for future research. We review 2 academic disciplines most relevant to health economics research in cancer screening: applied microeconomics and decision science. We consider 3 emerging topics: cancer screening policies in national as well as local contexts, “choosing wisely” screening practices, and targeted screening efforts for vulnerable subpopulations. We also discuss the strengths and weaknesses of available data sources and opportunities for methodological research and training. Recommendations to strengthen research infrastructure include developing novel data linkage strategies, increasing access to electronic health records, establishing curriculum and training programs, promoting multidisciplinary collaborations, and enhancing research funding opportunities.},
	language = {en},
	number = {59},
	urldate = {2025-01-08},
	journal = {JNCI Monographs},
	author = {Shih, Ya-Chen Tina and Sabik, Lindsay M and Stout, Natasha K and Halpern, Michael T and Lipscomb, Joseph and Ramsey, Scott and Ritzwoller, Debra P},
	month = jul,
	year = {2022},
	keywords = {cancer screening},
	pages = {42--50},
	file = {Shih et al. - 2022 - Health Economics Research in Cancer Screening Res.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\UFNULHM9\\Shih et al. - 2022 - Health Economics Research in Cancer Screening Res.pdf:application/pdf},
}

@article{dimick_methods_2014,
	title = {Methods for {Evaluating} {Changes} in {Health} {Care} {Policy}: {The} {Difference}-in-{Differences} {Approach}},
	volume = {312},
	issn = {0098-7484},
	shorttitle = {Methods for {Evaluating} {Changes} in {Health} {Care} {Policy}},
	url = {https://doi.org/10.1001/jama.2014.16153},
	doi = {10.1001/jama.2014.16153},
	abstract = {Observational studies are commonly used to evaluate the changes in outcomes associated with health care policy implementation. An important limitation in using observational studies in this context is the need to control for background changes in outcomes that occur with time (eg, secular trends affecting outcomes). The difference-in-differences approach is increasingly applied to address this problem.In this issue of JAMA, studies by Rajaram and colleagues and Patel and colleagues used the difference-in-differences approach to evaluate the changes that occurred following the 2011 Accreditation Council for Graduate Medical Education (ACGME) duty hour reforms. The 2 studies were conducted with different data sources and study populations but used similar methods.},
	number = {22},
	urldate = {2025-01-08},
	journal = {JAMA},
	author = {Dimick, Justin B. and Ryan, Andrew M.},
	month = dec,
	year = {2014},
	keywords = {DiD},
	pages = {2401--2402},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\4Z8EXI3V\\Dimick and Ryan - 2014 - Methods for Evaluating Changes in Health Care Poli.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\RAIPBMX3\\2020357.html:text/html},
}

@article{howard_long-term_2022,
	title = {Long-term comparative effectiveness of gastric bypass and sleeve gastrectomy on use of antireflux medication: a difference-in-differences analysis},
	volume = {18},
	issn = {15507289},
	shorttitle = {Long-term comparative effectiveness of gastric bypass and sleeve gastrectomy on use of antireflux medication},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1550728922001629},
	doi = {10.1016/j.soard.2022.04.016},
	abstract = {Background: Gastroesophageal reﬂux (GERD) is common among patients with obesity who undergo bariatric surgery. Although gastric bypass and sleeve gastrectomy are the most common bariatric operations performed in the United States, their long-term comparative effectiveness on GERD medication use is unknown.
Objective: To compare the long-term effectiveness of gastric bypass and sleeve gastrectomy on use of antireﬂux medication. Setting: National cohort undergoing inpatient bariatric surgery.
Methods: This is a retrospective study of Medicare beneﬁciaries undergoing gastric bypass and sleeve gastrectomy between January 1, 2012, and December 31, 2017. A difference-in-differences analysis was conducted to evaluate the differential change in antireﬂux medication use between groups before and after surgery.
Results: A total of 16,640 patients underwent gastric bypass, and 26,724 patients underwent sleeve gastrectomy. Before surgery, GERD medication use was higher among patients who underwent gastric bypass (62.4\%; 95\% conﬁdence interval [CI]: 62.0\%–63.7\%) compared with patients who underwent sleeve gastrectomy (60.1\%; 95\% CI: 59.3\%–60.9\%). Five years after surgery, GERD medication use was lower in patients who underwent gastric bypass (47.8\%; 95\% CI: 46.3\%–49.3\%) compared with patients who underwent sleeve gastrectomy (53.7\%; 95\% CI: 50.5\%–56.9\%). The differential decrease from baseline GERD medication use was greater for patients who underwent gastric bypass at 2 years (–4.1 percentage points [pp]; 95\% CI: –1.7 to –6.5 pp), 3 years (–4.3 pp; 95\% CI: –1.6 to –7.0 pp), 4 years (–6.9 pp; 95\% CI: –4.1 to –9.6 pp), and 5 years (–8.3 pp; 95\% CI: –3.7 to 12.8 pp) after surgery.
Conclusion: Though use of antireﬂux medication decreased following both procedures, gastric bypass was associated with a greater reduction in antireﬂux medication use 5 years after surgery compared with sleeve gastrectomy. Understanding the long-term comparative effectiveness of these},
	language = {en},
	number = {8},
	urldate = {2025-01-08},
	journal = {Surgery for Obesity and Related Diseases},
	author = {Howard, Ryan and Yang, Jie and Thumma, Jyothi and Arterburn, David E. and Ryan, Andrew and Chao, Grace and Telem, Dana and Dimick, Justin B.},
	month = aug,
	year = {2022},
	keywords = {causal inference, DiD},
	pages = {1033--1041},
	file = {Howard et al. - 2022 - Long-term comparative effectiveness of gastric byp.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\WDSVWFK6\\Howard et al. - 2022 - Long-term comparative effectiveness of gastric byp.pdf:application/pdf},
}

@article{shen_bayesppd_2023,
	title = {{BayesPPD}: {An} {R} {Package} for {Bayesian} {Sample} {Size} {Determination} {Using} the {Power} and {Normalized} {Power} {Prior} for {Generalized} {Linear} {Models}},
	volume = {14},
	issn = {2073-4859},
	shorttitle = {{BayesPPD}},
	url = {https://journal.r-project.org/articles/RJ-2023-016},
	doi = {10.32614/RJ-2023-016},
	abstract = {The R package BayesPPD (Bayesian Power Prior Design) supports Bayesian power and type I error calculation and model fitting after incorporating historical data with the power prior and the normalized power prior for generalized linear models (GLM). The package accommodates summary level data or subject level data with covariate information. It supports use of multiple historical datasets as well as design without historical data. Supported distributions for responses include normal, binary (Bernoulli/binomial), Poisson and exponential. The power parameter can be fixed or modeled as random using a normalized power prior for each of these distributions. In addition, the package supports the use of arbitrary sampling priors for computing Bayesian power and type I error rates. In addition to describing the statistical methodology and functions implemented in the package to enable sample size determination (SSD), we also demonstrate the use of BayesPPD in two comprehensive case studies.},
	language = {en},
	number = {4},
	urldate = {2024-12-30},
	journal = {The R Journal},
	author = {Shen, Yueqi and A. Psioda, Matthew and G. Ibrahim, Joseph},
	month = feb,
	year = {2023},
	keywords = {sample size, Bayesian, trial design},
	pages = {335--351},
	file = {Shen et al. - 2023 - BayesPPD An R Package for Bayesian Sample Size De.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\UE2GQ2R2\\Shen et al. - 2023 - BayesPPD An R Package for Bayesian Sample Size De.pdf:application/pdf},
}

@article{westreich_table_2013,
	title = {The {Table} 2 {Fallacy}: {Presenting} and {Interpreting} {Confounder} and {Modifier} {Coefficients}},
	volume = {177},
	issn = {0002-9262, 1476-6256},
	shorttitle = {The {Table} 2 {Fallacy}},
	url = {https://academic.oup.com/aje/article-lookup/doi/10.1093/aje/kws412},
	doi = {10.1093/aje/kws412},
	language = {en},
	number = {4},
	urldate = {2024-12-18},
	journal = {American Journal of Epidemiology},
	author = {Westreich, D. and Greenland, S.},
	month = feb,
	year = {2013},
	keywords = {causal inference},
	pages = {292--298},
	file = {Westreich and Greenland - 2013 - The Table 2 Fallacy Presenting and Interpreting C.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\GWT4DX8W\\Westreich and Greenland - 2013 - The Table 2 Fallacy Presenting and Interpreting C.pdf:application/pdf},
}

@article{gamble_guidelines_2017,
	title = {Guidelines for the {Content} of {Statistical} {Analysis} {Plans} in {Clinical} {Trials}},
	volume = {318},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.2017.18556},
	doi = {10.1001/jama.2017.18556},
	abstract = {While guidance on statistical principles for clinical trials exists, there is an absence of guidance covering the required content of statistical analysis plans (SAPs) to support transparency and reproducibility.To develop recommendations for a minimum set of items that should be addressed in SAPs for clinical trials, developed with input from statisticians, previous guideline authors, journal editors, regulators, and funders.Funders and regulators (n = 39) of randomized trials were contacted and the literature was searched to identify existing guidance; a survey of current practice was conducted across the network of UK Clinical Research Collaboration–registered trial units (n = 46, 1 unit had 2 responders) and a Delphi survey (n = 73 invited participants) was conducted to establish consensus on SAPs. The Delphi survey was sent to statisticians in trial units who completed the survey of current practice (n = 46), CONSORT (Consolidated Standards of Reporting Trials) and SPIRIT (Standard Protocol Items: Recommendations for Interventional Trials) guideline authors (n = 16), pharmaceutical industry statisticians (n = 3), journal editors (n = 9), and regulators (n = 2) (3 participants were included in 2 groups each), culminating in a consensus meeting attended by experts (N = 12) with representatives from each group. The guidance subsequently underwent critical review by statisticians from the surveyed trial units and members of the expert panel of the consensus meeting (N = 51), followed by piloting of the guidance document in the SAPs of 5 trials.No existing guidance was identified. The registered trials unit survey (46 responses) highlighted diversity in current practice and confirmed support for developing guidance. The Delphi survey (54 of 73, 74\% participants completing both rounds) reached consensus on 42\% (n = 46) of 110 items. The expert panel (N = 12) agreed that 63 items should be included in the guidance, with an additional 17 items identified as important but may be referenced elsewhere. Following critical review and piloting, some overlapping items were combined, leaving 55 items.Recommendations are provided for a minimum set of items that should be addressed and included in SAPs for clinical trials. Trial registration, protocols, and statistical analysis plans are critically important in ensuring appropriate reporting of clinical trials.},
	number = {23},
	urldate = {2024-12-09},
	journal = {JAMA},
	author = {Gamble, Carrol and Krishan, Ashma and Stocken, Deborah and Lewis, Steff and Juszczak, Edmund and Doré, Caroline and Williamson, Paula R. and Altman, Douglas G. and Montgomery, Alan and Lim, Pilar and Berlin, Jesse and Senn, Stephen and Day, Simon and Barbachano, Yolanda and Loder, Elizabeth},
	month = dec,
	year = {2017},
	pages = {2337--2343},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\55BBYTAV\\Gamble et al. - 2017 - Guidelines for the Content of Statistical Analysis.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\SSBMQCUS\\2666509.html:text/html},
}

@article{fillion_professional_2009,
	title = {Professional {Patient} {Navigation} in {Head} and {Neck} {Cancer}},
	volume = {25},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {07492081},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0749208109000631},
	doi = {10.1016/j.soncn.2009.05.004},
	abstract = {OBJECTIVES: To discuss professional cancer navigation roles, models, implementation process and outcomes of patients and families dealing with head and neck cancers. One speciﬁc research is presented as an illustration. DATA SOURCES: Published scientiﬁc papers, research review articles, implementations studies. CONCLUSION: Two independent cohorts of patients with head and neck cancers were compared according to the presence of the professional navigator (Exposed cohort n¼83) or not (Historical cohort n¼75). The Exposed cohort showed a better proﬁle on several indicators of outcomes. The results clearly indicate an association between the presence of the professional navigator with continuity of care (higher satisfaction and shorter duration of hospitalization), and empowerment (fewer cancer-related problems, including body images concerns, and better emotional quality of life). IMPLICATIONS FOR NURSING PRACTICE: Oncology nurses can not only play an important role in continuity of care but also in supportive care by helping patients to cope better with cancer treatments, recovery or cancer progression and death issues.},
	language = {en},
	number = {3},
	urldate = {2024-12-04},
	journal = {Seminars in Oncology Nursing},
	author = {Fillion, Lise and De Serres, Marie and Cook, Sandra and Goupil, Richard L. and Bairati, Isabelle and Doll, Richard},
	month = aug,
	year = {2009},
	pages = {212--221},
	file = {Fillion et al. - 2009 - Professional Patient Navigation in Head and Neck C.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\LMXG6SWP\\Fillion et al. - 2009 - Professional Patient Navigation in Head and Neck C.pdf:application/pdf},
}

@article{lavigne_history_2024,
	title = {The {History} and {Future} of {Multidisciplinary} {Cancer} {Care}},
	volume = {34},
	issn = {10534296},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1053429624000572},
	doi = {10.1016/j.semradonc.2024.07.006},
	language = {en},
	number = {4},
	urldate = {2024-12-04},
	journal = {Seminars in Radiation Oncology},
	author = {LaVigne, Anna W. and Doss, Victoria L. and Berizzi, Donna and Johnston, Fabian M. and Kiess, Ana P. and Kirtane, Kedar S. and Moghanaki, Drew and Roumeliotis, Michael and Yang, George Q. and Viswanathan, Akila N.},
	month = oct,
	year = {2024},
	pages = {441--451},
	file = {LaVigne et al. - 2024 - The History and Future of Multidisciplinary Cancer.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\DXCVFUPA\\LaVigne et al. - 2024 - The History and Future of Multidisciplinary Cancer.pdf:application/pdf},
}

@misc{r_core_team_r_2024,
	address = {Vienna, Austria},
	title = {R: {A} {Language} and {Environment} for {Statistical} {Computing}},
	url = {https://www.R-project.org/},
	publisher = {R Foundation for Statistical Computing},
	author = {R Core Team},
	year = {2024},
}

@article{buuren_mice_2011,
	title = {mice: {Multivariate} {Imputation} by {Chained} {Equations} in {R}},
	volume = {45},
	copyright = {Copyright (c) 2009 Stef van Buuren, Karin Groothuis-Oudshoorn},
	issn = {1548-7660},
	shorttitle = {mice},
	url = {https://doi.org/10.18637/jss.v045.i03},
	doi = {10.18637/jss.v045.i03},
	abstract = {The R package mice imputes incomplete multivariate data by chained equations. The software mice 1.0 appeared in the year 2000 as an S-PLUS library, and in 2001 as an R package. mice 1.0 introduced predictor selection, passive imputation and automatic pooling. This article documents mice, which extends the functionality of mice 1.0 in several ways. In mice, the analysis of imputed data is made completely general, whereas the range of models under which pooling works is substantially extended. mice adds new functionality for imputing multilevel data, automatic predictor selection, data handling, post-processing imputed values, specialized pooling routines, model selection tools, and diagnostic graphs. Imputation of categorical data is improved in order to bypass problems caused by perfect prediction. Special attention is paid to transformations, sum scores, indices and interactions using passive imputation, and to the proper setup of the predictor matrix. mice can be downloaded from the Comprehensive R Archive Network. This article provides a hands-on, stepwise approach to solve applied incomplete data problems.},
	language = {en},
	urldate = {2024-12-03},
	journal = {Journal of Statistical Software},
	author = {Buuren, Stef van and Groothuis-Oudshoorn, Karin},
	month = dec,
	year = {2011},
	pages = {1--67},
}

@book{rubin_multiple_1987,
	address = {New York},
	title = {Multiple inputation for non-response in surveys},
	publisher = {John Wily \& SOns},
	author = {Rubin, D.B.},
	year = {1987},
	keywords = {Rubin's Rules},
}

@article{royston_multiple_2004,
	title = {Multiple {Imputation} of {Missing} {Values}},
	volume = {4},
	issn = {1536-867X},
	url = {https://doi.org/10.1177/1536867X0400400301},
	doi = {10.1177/1536867X0400400301},
	abstract = {Following the seminal publications of Rubin about thirty years ago, statisticians have become increasingly aware of the inadequacy of “complete-case” analysis of datasets with missing observations. In medicine, for example, observations may be missing in a sporadic way for different covariates, and a complete-case analysis may omit as many as half of the available cases. Hotdeck imputation was implemented in Stata in 1999 by Mander and Clayton. However, this technique may perform poorly when many rows of data have at least one missing value. This article describes an implementation for Stata of the MICE method of multiple multivariate imputation described by van Buuren, Boshuizen, and Knook (1999). MICE stands for multivariate imputation by chained equations. The basic idea of data analysis with multiple imputation is to create a small number (e.g., 5–10) of copies of the data, each of which has the missing values suitably imputed, and analyze each complete dataset independently. Estimates of parameters of interest are averaged across the copies to give a single estimate. Standard errors are computed according to the “Rubin rules”, devised to allow for the between- and within-imputation components of variation in the parameter estimates. This article describes five ado-files. mvis creates multiple multivariate imputations. uvis imputes missing values for a single variable as a function of several covariates, each with complete data. micombine fits a wide variety of regression models to a multiply imputed dataset, combining the estimates using Rubin's rules, and supports survival analysis models (stcox and streg), categorical data models, generalized linear models, and more. Finally, misplit and mijoin are utilities to intercon-vert datasets created by mvis and by the miset program from John Carlin and colleagues. The use of the routines is illustrated with an example of prognostic modeling in breast cancer.},
	language = {en},
	number = {3},
	urldate = {2024-12-03},
	journal = {The Stata Journal},
	author = {Royston, Patrick},
	month = aug,
	year = {2004},
	note = {Publisher: SAGE Publications},
	keywords = {Rubin's Rules},
	pages = {227--241},
	file = {SAGE PDF Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\4NKBTMK5\\Royston - 2004 - Multiple Imputation of Missing Values.pdf:application/pdf},
}

@book{hernan_causal_2020,
	address = {Boca Raton},
	title = {Causal {Inference}: {What} {If}},
	url = {https://www.hsph.harvard.edu/miguel-hernan/wp-content/uploads/sites/1268/2024/04/hernanrobins_WhatIf_26apr24.pdf},
	language = {en},
	publisher = {CRC Press},
	author = {Hernan, Miguel A and Robins, James M},
	year = {2020},
	keywords = {causal inference},
	file = {Hernan and Robins - Causal Inference What If.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\BRUU27E2\\Hernan and Robins - Causal Inference What If.pdf:application/pdf},
}

@article{arel-bundock_how_2024,
	title = {How to {Interpret}   {Statistical} {Models} {Using} marginaleffects in {R} and {Python}.},
	journal = {Journal of   Statistical Software},
	author = {Arel-Bundock, V and Greifer, N and Heiss, A},
	year = {2024},
}

@article{bako_minimally_2023,
	title = {Minimally {Invasive} {Surgery} {With} {Thrombolysis} for {Intracerebral} {Hemorrhage} {Evacuation}: {Bayesian} {Reanalysis} of a {Randomized} {Controlled} {Trial}},
	volume = {101},
	issn = {0028-3878, 1526-632X},
	shorttitle = {Minimally {Invasive} {Surgery} {With} {Thrombolysis} for {Intracerebral} {Hemorrhage} {Evacuation}},
	url = {https://www.neurology.org/doi/10.1212/WNL.0000000000207735},
	doi = {10.1212/WNL.0000000000207735},
	language = {en},
	number = {16},
	urldate = {2024-12-02},
	journal = {Neurology},
	author = {Bako, Abdulaziz T. and Potter, Thomas and Pan, Alan P. and Tannous, Jonika and Britz, Gavin and Ziai, Wendy C. and Awad, Issam and Hanley, Daniel and Vahidy, Farhaan S.},
	month = oct,
	year = {2023},
	keywords = {Bayesian, trial design},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\F3JIA6X8\\Bako et al. - 2023 - Minimally Invasive Surgery With Thrombolysis for I.pdf:application/pdf},
}

@article{corazziari_standard_2004,
	title = {Standard cancer patient population for age standardising survival ratios},
	volume = {40},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {09598049},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0959804904005283},
	doi = {10.1016/j.ejca.2004.07.002},
	abstract = {Standard adult cancer patients populations are derived in this paper as a tool for the calculation of age-standardised cancer survival ﬁgures. Previously used standards in survival analysis have been site- and/or study-speciﬁc. Here, multivariate methods have been used to deﬁne the smallest possible number of general standard cancer patient populations which are simple to use and provide standardised survival values close to the raw ones for the largest possible number of cancer sites. The analysis was based on data for over 1.1 million cancer patients included in the EUROCARE-2 study. The proposed standard populations consist of three age distributions, appropriate for cancers with incidence patterns: (1) increasing with age – the vast majority of cancers; (2) broadly constant with age and (3) mainly aﬀecting young adults. The three standard distributions are presented by both broad and ﬁve-year age classes. The latter can be used to determine which of the three standards would be used for sites not included in the cluster analysis because their survival is generally calculated in unusual age groups. Overall, standard 1 is appropriate for over 91\% of cases, standard 2 for just over 7\%, and standard 3 for less than 2\%. The proposed standards were tested on European (EUROCARE-2 and EUROCARE-3) and US (Surveillance, Epidemiology and End Results Program, SEER) relative survival data. There was very good correspondence between the raw (population weighted) and age-standardised survival ﬁgures.},
	language = {en},
	number = {15},
	urldate = {2024-12-02},
	journal = {European Journal of Cancer},
	author = {Corazziari, Isabella and Quinn, Mike and Capocaccia, Riccardo},
	month = oct,
	year = {2004},
	keywords = {age adjusted survival},
	pages = {2307--2316},
	file = {Corazziari et al. - 2004 - Standard cancer patient population for age standar.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\JU4F55HW\\Corazziari et al. - 2004 - Standard cancer patient population for age standar.pdf:application/pdf},
}

@article{sun_age-adjusted_2011,
	title = {Age-{Adjusted} {Incidence}, {Mortality}, and {Survival} {Rates} of {Stage}-{Specific} {Renal} {Cell} {Carcinoma} in {North} {America}: {A} {Trend} {Analysis}},
	volume = {59},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {03022838},
	shorttitle = {Age-{Adjusted} {Incidence}, {Mortality}, and {Survival} {Rates} of {Stage}-{Specific} {Renal} {Cell} {Carcinoma} in {North} {America}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0302283810009942},
	doi = {10.1016/j.eururo.2010.10.029},
	abstract = {Background: The rising incidence of renal cell carcinoma (RCC) has been largely attributed to the increasing use of imaging procedures.
Objective: Our aim was to examine stage-speciﬁc incidence, mortality, and survival trends of RCC in North America. Design, setting, and participants: We computed age-adjusted incidence, survival, and mortality rates using the Surveillance Epidemiology and End Results database. Between 1988 and 2006, 43 807 patients with histologically conﬁrmed RCC were included.
Measurements: We calculated incidence, mortality, and 5-yr survival rates by year. Reported ﬁndings were stratiﬁed according to disease stage.
Results and limitations: Age-adjusted incidence rate of RCC rose from 7.6 per 100 000 person-years in 1988 to 11.7 in 2006 (estimated annual percentage change [EAPC]: +2.39\%; p {\textless} 0.001). Stage-speciﬁc age-adjusted incidence rates increased for localized stage: 3.8 in 1988 to 8.2 in 2006 (EAPC: +4.29\%; p {\textless} 0.001) and decreased during the same period for distant stage: 2.1 to 1.6 (EAPC: À0.57\%; p = 0.01). Stage-speciﬁc survival rates improved over time for localized stage but remained stable for regional and distant stages. Mortality rates varied signiﬁcantly over the study period among localized stage, 1.3 in 1988 to 2.4 in 2006 (EAPC: +3.16\%; p {\textless} 0.001), and distant stage, 1.8 in 1988 to 1.6 in 2006 (EAPC: À0.53\%; p = 0.045). Better detailed staging information represents a main limitation of the study.
Conclusions: The incidence rates of localized RCC increased rapidly, whereas those of distant RCC declined. Mortality rates signiﬁcantly increased for localized stage and decreased for distant stage. Innovation in diagnosis and management of RCC remains necessary.},
	language = {en},
	number = {1},
	urldate = {2024-12-02},
	journal = {European Urology},
	author = {Sun, Maxine and Thuret, Rodolphe and Abdollah, Firas and Lughezzani, Giovanni and Schmitges, Jan and Tian, Zhe and Shariat, Shahrokh F. and Montorsi, Francesco and Patard, Jean-Jacques and Perrotte, Paul and Karakiewicz, Pierre I.},
	month = jan,
	year = {2011},
	keywords = {age adjusted survival},
	pages = {135--141},
	file = {Sun et al. - 2011 - Age-Adjusted Incidence, Mortality, and Survival Ra.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\WPA2A2UA\\Sun et al. - 2011 - Age-Adjusted Incidence, Mortality, and Survival Ra.pdf:application/pdf},
}

@article{brenner_age_2005,
	title = {Age adjustment of cancer survival rates: methods, point estimates and standard errors},
	volume = {93},
	copyright = {2005 The Author(s)},
	issn = {1532-1827},
	shorttitle = {Age adjustment of cancer survival rates},
	url = {https://www.nature.com/articles/6602704},
	doi = {10.1038/sj.bjc.6602704},
	abstract = {We empirically evaluated the performance of a new method for age adjustment of cancer survival compared to traditional age adjustment using data from the Finnish Cancer Registry. We find that both methods provide almost identical results for absolute survival but the new method generally provides more meaningful estimates of relative survival with often a smaller standard error.},
	language = {en},
	number = {3},
	urldate = {2024-12-02},
	journal = {Br J Cancer},
	author = {Brenner, H. and Hakulinen, T.},
	month = aug,
	year = {2005},
	note = {Publisher: Nature Publishing Group},
	pages = {372--375},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\UPUS324I\\Brenner and Hakulinen - 2005 - Age adjustment of cancer survival rates methods, .pdf:application/pdf},
}

@misc{harrell_why_2024,
	title = {Why a {Bayesian} {Approach} to {Drug} {Development} and {Evaluation}?},
	url = {https://hbiostat.org/doc/bayes/whybayes.pdf},
	abstract = {P -values and the p {\textless} 0.05 rule of thumb came into use before the computing revolution. Assuming the null hypothesis is true greatly simplified the model, often requiring only manual calculations. But the traditional straw-man null hypothesis testing approach to establishing statistical evidence about efficacy or safety of a drug has a number of deficiencies, many of them caused by the indirectness of the approach, including the use of probabilities of events that already occurred conditional on facts that are unknown. P -values and type I assertion probability α are often assumed to provide the error probabilities regulators need, but the chance that an approved drug is ineffective given the data is actually the direct Bayesian posterior probability that efficacy falls below an acceptable level. Furthermore, the rules of logic supporting proof by contradiction, based on certainties and not probabilities, don’t apply to the uncertainties of traditional null hypothesis testing. Just as in medical diagnosis, forward probabilistic thinking leads to optimum decisions. The Bayesian approach involves direct estimation of time-forward probabilities of clinical interest and does not need to concern itself with long-run operating characteristics such as the number of false positives from a large set of imagined exact replications of exactly null clinical trials. Instead, Bayesian methods aim to maximize the probability of making the best decision about drug efficacy and safety for the single problem at hand. The Bayesian approach applies to complex study designs, incorporates applicable prior information, results in cleaner interpretations on a clinical as opposed to a randomness scale, and provides a fully self-contained model-based approach to inference needing no after-the-fact adjustments for context/multiplicities. In some ways frequentist hypothesis testing involves modeling noise while Bayesian inference involves modeling signal.},
	language = {en},
	urldate = {2024-11-22},
	author = {Harrell, Frank E and Lavange, Lisa},
	month = apr,
	year = {2024},
	keywords = {Bayesian, trial design},
	file = {Jr - Why a Bayesian Approach to Drug Development and Ev.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\FQIY24NU\\Jr - Why a Bayesian Approach to Drug Development and Ev.pdf:application/pdf},
}

@misc{veitch_sense_2020,
	title = {Sense and {Sensitivity} {Analysis}: {Simple} {Post}-{Hoc} {Analysis} of {Bias} {Due} to {Unobserved} {Confounding}},
	shorttitle = {Sense and {Sensitivity} {Analysis}},
	url = {http://arxiv.org/abs/2003.01747},
	doi = {10.48550/arXiv.2003.01747},
	abstract = {It is a truth universally acknowledged that an observed association without known mechanism must be in want of a causal estimate. However, causal estimation from observational data often relies on the (untestable) assumption of `no unobserved confounding'. Violations of this assumption can induce bias in effect estimates. In principle, such bias could invalidate or reverse the conclusions of a study. However, in some cases, we might hope that the influence of unobserved confounders is weak relative to a `large' estimated effect, so the qualitative conclusions are robust to bias from unobserved confounding. The purpose of this paper is to develop {\textbackslash}emph\{Austen plots\}, a sensitivity analysis tool to aid such judgments by making it easier to reason about potential bias induced by unobserved confounding. We formalize confounding strength in terms of how strongly the confounder influences treatment assignment and outcome. For a target level of bias, an Austen plot shows the minimum values of treatment and outcome influence required to induce that level of bias. Domain experts can then make subjective judgments about whether such strong confounders are plausible. To aid this judgment, the Austen plot additionally displays the estimated influence strength of (groups of) the observed covariates. Austen plots generalize the classic sensitivity analysis approach of Imbens [Imb03]. Critically, Austen plots allow any approach for modeling the observed data and producing the initial estimate. We illustrate the tool by assessing biases for several real causal inference problems, using a variety of machine learning approaches for the initial data analysis. Code is available at https://github.com/anishazaveri/austen\_plots},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Veitch, Victor and Zaveri, Anisha},
	month = dec,
	year = {2020},
	note = {arXiv:2003.01747 [cs, stat]},
	keywords = {causal inference, sensitivity analysis},
	file = {arXiv Fulltext PDF:C\:\\Users\\anbe6\\Zotero\\storage\\G7Q6AVQB\\Veitch and Zaveri - 2020 - Sense and Sensitivity Analysis Simple Post-Hoc An.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\SKN6NG3K\\2003.html:text/html},
}

@article{neal_introduction_nodate,
	title = {Introduction to {Causal} {Inference}},
	language = {en},
	author = {Neal, Brady},
	keywords = {causal inference},
	file = {Neal - Introduction to Causal Inference.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\WACJF2HI\\Neal - Introduction to Causal Inference.pdf:application/pdf},
}

@misc{dobler_wild_2018,
	title = {Wild {Bootstrap} based {Confidence} {Bands} for {Multiplicative} {Hazards} {Models}},
	url = {http://arxiv.org/abs/1808.00242},
	abstract = {We propose new resampling-based approaches to construct asymptotically valid time simultaneous confidence bands for cumulative hazard functions in multi-state Cox models. In particular, we exemplify the methodology in detail for the simple Cox model with time dependent covariates, where the data may be subject to independent right-censoring or left-truncation. In extensive simulations we investigate their finite sample behaviour. Finally, the methods are utilized to analyze an empirical example.},
	urldate = {2024-02-27},
	publisher = {arXiv},
	author = {Dobler, Dennis and Pauly, Markus and Scheike, Thomas H.},
	month = aug,
	year = {2018},
	note = {arXiv:1808.00242 [math, stat]},
	keywords = {survival analysis},
	file = {arXiv.org Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\J6QY83ST\\1808.html:text/html;Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\GMQAZ278\\Dobler et al. - 2018 - Wild Bootstrap based Confidence Bands for Multipli.pdf:application/pdf},
}

@article{chatton_g-computation_2020,
	title = {G-computation, propensity score-based methods, and targeted maximum likelihood estimator for causal inference with different covariates sets: a comparative simulation study},
	volume = {10},
	copyright = {2020 The Author(s)},
	issn = {2045-2322},
	shorttitle = {G-computation, propensity score-based methods, and targeted maximum likelihood estimator for causal inference with different covariates sets},
	url = {https://www.nature.com/articles/s41598-020-65917-x},
	doi = {10.1038/s41598-020-65917-x},
	abstract = {Controlling for confounding bias is crucial in causal inference. Distinct methods are currently employed to mitigate the effects of confounding bias. Each requires the introduction of a set of covariates, which remains difficult to choose, especially regarding the different methods. We conduct a simulation study to compare the relative performance results obtained by using four different sets of covariates (those causing the outcome, those causing the treatment allocation, those causing both the outcome and the treatment allocation, and all the covariates) and four methods: g-computation, inverse probability of treatment weighting, full matching and targeted maximum likelihood estimator. Our simulations are in the context of a binary treatment, a binary outcome and baseline confounders. The simulations suggest that considering all the covariates causing the outcome led to the lowest bias and variance, particularly for g-computation. The consideration of all the covariates did not decrease the bias but significantly reduced the power. We apply these methods to two real-world examples that have clinical relevance, thereby illustrating the real-world importance of using these methods. We propose an R package RISCA to encourage the use of g-computation in causal inference.},
	language = {en},
	number = {1},
	urldate = {2024-02-29},
	journal = {Sci Rep},
	author = {Chatton, Arthur and Le Borgne, Florent and Leyrat, Clémence and Gillaizeau, Florence and Rousseau, Chloé and Barbin, Laetitia and Laplaud, David and Léger, Maxime and Giraudeau, Bruno and Foucher, Yohann},
	month = jun,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {R, causal inference, propensity score, g-computation},
	pages = {9219},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\XP8CK8FQ\\Chatton et al. - 2020 - G-computation, propensity score-based methods, and.pdf:application/pdf},
}

@article{le_borgne_g-computation_2021,
	title = {G-computation and machine learning for estimating the causal effects of binary exposure statuses on binary outcomes},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-81110-0},
	doi = {10.1038/s41598-021-81110-0},
	abstract = {In clinical research, there is a growing interest in the use of propensity score-based methods to estimate causal effects. G-computation is an alternative because of its high statistical power. Machine learning is also increasingly used because of its possible robustness to model misspecification. In this paper, we aimed to propose an approach that combines machine learning and G-computation when both the outcome and the exposure status are binary and is able to deal with small samples. We evaluated the performances of several methods, including penalized logistic regressions, a neural network, a support vector machine, boosted classification and regression trees, and a super learner through simulations. We proposed six different scenarios characterised by various sample sizes, numbers of covariates and relationships between covariates, exposure statuses, and outcomes. We have also illustrated the application of these methods, in which they were used to estimate the efficacy of barbiturates prescribed during the first 24 h of an episode of intracranial hypertension. In the context of GC, for estimating the individual outcome probabilities in two counterfactual worlds, we reported that the super learner tended to outperform the other approaches in terms of both bias and variance, especially for small sample sizes. The support vector machine performed well, but its mean bias was slightly higher than that of the super learner. In the investigated scenarios, G-computation associated with the super learner was a performant method for drawing causal inferences, even from small sample sizes.},
	language = {en},
	number = {1},
	urldate = {2024-03-08},
	journal = {Sci Rep},
	author = {Le Borgne, Florent and Chatton, Arthur and Léger, Maxime and Lenain, Rémi and Foucher, Yohann},
	month = jan,
	year = {2021},
	note = {Publisher: Nature Publishing Group},
	keywords = {causal inference, g-computation},
	pages = {1435},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\HZYV3YNU\\Le Borgne et al. - 2021 - G-computation and machine learning for estimating .pdf:application/pdf},
}

@article{domingo-relloso_causal_nodate,
	title = {Causal mediation for uncausally related mediators in the context of survival analysis},
	abstract = {Objective: The study of the potential intermediate effect of several variables on the association between an exposure and a time-to-event outcome is a question of interest in epidemiologic research. However, to our knowledge, no tools have been developed for the evaluation of multiple correlated mediators in a survival setting.},
	language = {en},
	author = {Domingo-Relloso, Arce and Jerolon, Allan and Tellez-Plaza, Maria and Bermudez, Jose D},
	keywords = {causal inference, survival analysis},
	file = {Domingo-Relloso et al. - Causal mediation for uncausally related mediators .pdf:C\:\\Users\\anbe6\\Zotero\\storage\\YDSE7TSS\\Domingo-Relloso et al. - Causal mediation for uncausally related mediators .pdf:application/pdf},
}

@article{yang_propensity_2023,
	title = {Propensity score weighting methods for causal subgroup analysis with time-to-event outcomes},
	volume = {32},
	issn = {0962-2802},
	url = {https://doi.org/10.1177/09622802231188517},
	doi = {10.1177/09622802231188517},
	abstract = {Evaluating causal effects of an intervention in pre-specified subgroups is a standard goal in comparative effectiveness research. Despite recent advancements in causal subgroup analysis, research on time-to-event outcomes has been lacking. This article investigates the propensity score weighting method for causal subgroup survival analysis. We introduce two causal estimands, the subgroup marginal hazard ratio and subgroup restricted average causal effect, and provide corresponding propensity score weighting estimators. We analytically established that the bias of subgroup-restricted average causal effect is determined by subgroup covariate balance. Using extensive simulations, we compare the performance of various combinations of propensity score models (logistic regression, random forests, least absolute shrinkage and selection operator, and generalized boosted models) and weighting schemes (inverse probability weighting, and overlap weighting) for estimating the causal estimands. We find that the logistic model with subgroup-covariate interactions selected by least absolute shrinkage and selection operator consistently outperforms other propensity score models. Also, overlap weighting generally outperforms inverse probability weighting in terms of balance, bias and variance, and the advantage is particularly pronounced in small subgroups and/or in the presence of poor overlap. We applied the methods to the observational Comparing Options for Management: PAtient-centered REsults for Uterine Fibroids study to evaluate the causal effects of myomectomy versus hysterectomy on the time to disease recurrence in a number of pre-specified subgroups of patients with uterine fibroids.},
	language = {en},
	number = {10},
	urldate = {2024-03-07},
	journal = {Stat Methods Med Res},
	author = {Yang, Siyun and Zhou, Ruiwen and Li, Fan and Thomas, Laine E},
	month = oct,
	year = {2023},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {causal inference, survival analysis, propensity score},
	pages = {1919--1935},
}

@misc{mogensen_time-dependent_2023,
	title = {Time-dependent mediators in survival analysis: {Graphical} representation of causal assumptions},
	shorttitle = {Time-dependent mediators in survival analysis},
	url = {http://arxiv.org/abs/2310.04709},
	abstract = {We study time-dependent mediators in survival analysis using a treatment separation approach due to Didelez [2019] and based on earlier work by Robins and Richardson [2011]. This approach avoids nested counterfactuals and crossworld assumptions which are otherwise common in mediation analysis. The causal model of treatment, mediators, covariates, confounders and outcome is represented by causal directed acyclic graphs (DAGs). However, the DAGs tend to be very complex when we have measurements at a large number of time points. We therefore suggest using so-called rolled graphs in which a node represents an entire coordinate process instead of a single random variable, leading us to far simpler graphical representations. The rolled graphs are not necessarily acyclic; they can be analyzed by \${\textbackslash}delta\$-separation which is the appropriate graphical separation criterion in this class of graphs and analogous to \$d\$-separation. In particular, \${\textbackslash}delta\$-separation is a graphical tool for evaluating if the conditions of the mediation analysis are met or if unmeasured confounders influence the estimated effects. We also state a mediational g-formula. This is similar to the approach in Vansteelandt et al. [2019] although that paper has a different conceptual basis. Finally, we apply this framework to a statistical model based on a Cox model with an added treatment effect.survival analysis; mediation; causal inference; graphical models; local independence graphs},
	urldate = {2024-03-07},
	publisher = {arXiv},
	author = {Mogensen, Søren Wengel and Aalen, Odd O. and Strohmaier, Susanne},
	month = oct,
	year = {2023},
	note = {arXiv:2310.04709 [stat]},
	keywords = {causal inference, survival analysis},
	file = {arXiv.org Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\5BVULTFE\\2310.html:text/html;Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\JV9QR937\\Mogensen et al. - 2023 - Time-dependent mediators in survival analysis Gra.pdf:application/pdf},
}

@article{axelrod_sensitivity_2023,
	title = {A sensitivity analysis approach for the causal hazard ratio in randomized and observational studies},
	volume = {79},
	issn = {1541-0420},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.13797},
	doi = {10.1111/biom.13797},
	abstract = {The hazard ratio (HR) is often reported as the main causal effect when studying survival data. Despite its popularity, the HR suffers from an unclear causal interpretation. As already pointed out in the literature, there is a built-in selection bias in the HR, because similarly to the truncation by death problem, the HR conditions on post-treatment survival. A recently proposed alternative, inspired by the Survivor Average Causal Effect, is the causal HR, defined as the ratio between hazards across treatment groups among the study participants that would have survived regardless of their treatment assignment. We discuss the challenge in identifying the causal HR and present a sensitivity analysis identification approach in randomized controlled trials utilizing a working frailty model. We further extend our framework to adjust for potential confounders using inverse probability of treatment weighting. We present a Cox-based and a flexible non-parametric kernel-based estimation under right censoring. We study the finite-sample properties of the proposed estimation methods through simulations. We illustrate the utility of our framework using two real-data examples.},
	language = {en},
	number = {3},
	urldate = {2024-03-07},
	journal = {Biometrics},
	author = {Axelrod, Rachel and Nevo, Daniel},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/biom.13797},
	keywords = {causal inference, survival analysis},
	pages = {2743--2756},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\R5ELDRUA\\Axelrod and Nevo - 2023 - A sensitivity analysis approach for the causal haz.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\CMKT5ZBW\\biom.html:text/html},
}

@article{robins_new_1986,
	title = {A new approach to causal inference in mortality studies with a sustained exposure period—application to control of the healthy worker survivor effect},
	volume = {7},
	issn = {0270-0255},
	url = {https://www.sciencedirect.com/science/article/pii/0270025586900886},
	doi = {10.1016/0270-0255(86)90088-6},
	abstract = {In observational cohort mortality studies with prolonged periods of exposure to the agent under study, it is not uncommon for risk factors for death to be determinants of subsequent exposure. For instance, in occupational mortality studies date of termination of employment is both a determinant of future exposure (since terminated individuals receive no further exposure) and an independent risk factor for death (since disabled individuals tend to leave employment). When current risk factor status determines subsequent exposure and is determined by previous exposure, standard analyses that estimate age-specific mortality rates as a function of cumulative exposure may underestimate the true effect of exposure on mortality whether or not one adjusts for the risk factor in the analysis. This observation raises the question, which if any population parameters can be given a causal interpretation in observational mortality studies? In answer, we offer a graphical approach to the identification and computation of causal parameters in mortality studies with sustained exposure periods. This approach is shown to be equivalent to an approach in which the observational study is identified with a hypothetical double-blind randomized trial in which data on each subject's assigned treatment protocol has been erased from the data file. Causal inferences can then be made by comparing mortality as a function of treatment protocol, since, in a double-blind randomized trial missing data on treatment protocol, the association of mortality with treatment protocol can still be estimated. We reanalyze the mortality experience of a cohort of arsenic-exposed copper smelter workers with our method and compare our results with those obtained using standard methods. We find an adverse effect of arsenic exposure on all-cause and lung cancer mortality which standard methods fail to detect.},
	number = {9},
	urldate = {2024-03-07},
	journal = {Mathematical Modelling},
	author = {Robins, James},
	month = jan,
	year = {1986},
	keywords = {causal inference, g-computation},
	pages = {1393--1512},
	file = {Robins - 1986 - A new approach to causal inference in mortality st.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\NSVQLJ5H\\Robins - 1986 - A new approach to causal inference in mortality st.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\QCULWT5J\\0270025586900886.html:text/html},
}

@article{kaspersen_short-_2021,
	title = {Short- and long-term mortality after deep sternal wound infection following cardiac surgery: experiences from {SWEDEHEART}},
	volume = {60},
	issn = {1010-7940},
	shorttitle = {Short- and long-term mortality after deep sternal wound infection following cardiac surgery},
	url = {https://doi.org/10.1093/ejcts/ezab080},
	doi = {10.1093/ejcts/ezab080},
	abstract = {Deep sternal wound infection (DSWI) is a serious complication after open-heart surgery. We investigated the association between DSWI and short- and long-term all-cause mortality in a large well-defined nationwide population.A retrospective, nationwide cohort study, which included 114676 consecutive patients who underwent coronary artery bypass grafting (CABG) and/or valve surgery from 1997 to 2015 in Sweden. Short- and long-term mortality was compared between DSWI patients and non-DSWI patients using propensity score inverse probability weighting adjustment based on patient characteristics and comorbidities. Median follow-up was 8.0 years (range 0–18.9).Altogether, 1516 patients (1.3\%) developed DSWI, most commonly in patients undergoing combined CABG and valve surgery (2.1\%). DSWI patients were older and had more disease burden than non-DSWI patients. The unadjusted cumulative mortality was higher in the DSWI group compared with the non-DSWI group at 90 days (7.9\% vs 3.0\%, P \&lt; 0.001) and at 1 year (12.8\% vs 4.5\%, P \&lt; 0.001). The adjusted absolute difference in risk of death was 2.3\% [95\% confidence interval (CI): 0.8–3.9] at 90 days and 4.7\% (95\% CI: 2.6–6.7) at 1 year. DSWI was independently associated with 90-day [adjusted relative risk (aRR) 1.89 (95\% CI: 1.38–2.59)], 1-year [aRR 2.13 (95\% CI: 1.68–2.71)] and long-term all-cause mortality [adjusted hazard ratio 1.56 (95\% CI: 1.30–1.88)].Both short- and long-term mortality risks are higher in DSWI patients compared to non-DSWI patients. These results stress the importance of preventing these infections and careful postoperative monitoring of DSWI patients.},
	number = {2},
	urldate = {2024-03-06},
	journal = {European Journal of Cardio-Thoracic Surgery},
	author = {Kaspersen, Alexander Emil and Nielsen, Susanne J and Orrason, Andri Wilberg and Petursdottir, Astridur and Sigurdsson, Martin Ingi and Jeppsson, Anders and Gudbjartsson, Tomas},
	month = aug,
	year = {2021},
	keywords = {causal inference, propensity score, surgery},
	pages = {233--241},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\NWA3RMBW\\Kaspersen et al. - 2021 - Short- and long-term mortality after deep sternal .pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\N2KT7M5U\\6148898.html:text/html},
}

@article{elze_comparison_2017,
	title = {Comparison of {Propensity} {Score} {Methods} and {Covariate} {Adjustment}},
	volume = {69},
	issn = {07351097},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S073510971637036X},
	doi = {10.1016/j.jacc.2016.10.060},
	abstract = {Propensity scores (PS) are an increasingly popular method to adjust for confounding in observational studies. Propensity score methods have theoretical advantages over conventional covariate adjustment, but their relative performance in real-word scenarios is poorly characterized. We used datasets from 4 large-scale cardiovascular observational studies (PROMETHEUS, ADAPT-DES [the Assessment of Dual AntiPlatelet Therapy with Drug-Eluting Stents], THIN [The Health Improvement Network], and CHARM [Candesartan in Heart Failure-Assessment of Reduction in Mortality and Morbidity]) to compare the performance of conventional covariate adjustment with 4 common PS methods: matching, stratiﬁcation, inverse probability weighting, and use of PS as a covariate. We found that stratiﬁcation performed poorly with few outcome events, and inverse probability weighting gave imprecise estimates of treatment effect and undue inﬂuence to a small number of observations when substantial confounding was present. Covariate adjustment and matching performed well in all of our examples, although matching tended to give less precise estimates in some cases. PS methods are not necessarily superior to conventional covariate adjustment, and care should be taken to select the most suitable method.},
	language = {en},
	number = {3},
	urldate = {2024-03-06},
	journal = {Journal of the American College of Cardiology},
	author = {Elze, Markus C. and Gregson, John and Baber, Usman and Williamson, Elizabeth and Sartori, Samantha and Mehran, Roxana and Nichols, Melissa and Stone, Gregg W. and Pocock, Stuart J.},
	month = jan,
	year = {2017},
	keywords = {causal inference},
	pages = {345--357},
	file = {Elze et al. - 2017 - Comparison of Propensity Score Methods and Covaria.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\YQP6L75B\\Elze et al. - 2017 - Comparison of Propensity Score Methods and Covaria.pdf:application/pdf},
}

@article{hernan_causal_2022,
	title = {Causal analyses of existing databases: no power calculations required},
	volume = {144},
	issn = {0895-4356},
	shorttitle = {Causal analyses of existing databases},
	url = {https://www.sciencedirect.com/science/article/pii/S0895435621002730},
	doi = {10.1016/j.jclinepi.2021.08.028},
	abstract = {Observational databases are often used to study causal questions. Before being granted access to data or funding, researchers may need to prove that “the statistical power of their analysis will be high.” Analyses expected to have low power, and hence result in imprecise estimates, will not be approved. This restrictive attitude towards observational analyses is misguided. A key misunderstanding is the belief that the goal of a causal analysis is to “detect” an effect. Causal effects are not binary signals that are either detected or undetected; causal effects are numerical quantities that need to be estimated. Because the goal is to quantify the effect as unbiasedly and precisely as possible, the solution to observational analyses with imprecise effect estimates is not avoiding observational analyses with imprecise estimates, but rather encouraging the conduct of many observational analyses. It is preferable to have multiple studies with imprecise estimates than having no study at all. After several studies become available, we will meta-analyze them and provide a more precise pooled effect estimate. Therefore, the justification to withhold an observational analysis of preexisting data cannot be that our estimates will be imprecise. Ethical arguments for power calculations before conducting a randomized trial which place individuals at risk are not transferable to observational analyses of existing databases. If a causal question is important, analyze your data, publish your estimates, encourage others to do the same, and then meta-analyze. The alternative is an unanswered question.},
	urldate = {2024-03-06},
	journal = {Journal of Clinical Epidemiology},
	author = {Hernán, Miguel A.},
	month = apr,
	year = {2022},
	keywords = {power, causal inference},
	pages = {203--205},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\UXB24QK7\\Hernán - 2022 - Causal analyses of existing databases no power ca.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\SKHDRGLC\\S0895435621002730.html:text/html},
}

@article{machado_p3statemsm_2011,
	title = {p3state.msm: {Analyzing} {Survival} {Data} from an {Illness}-{Death} {Model}},
	volume = {38},
	copyright = {Copyright (c) 2009 Luís Filipe Meira Machado, Javier Roca-Pardiñas},
	issn = {1548-7660},
	shorttitle = {p3state.msm},
	url = {https://doi.org/10.18637/jss.v038.i03},
	doi = {10.18637/jss.v038.i03},
	abstract = {In longitudinal studies of disease, patients can experience several events across a followup period. Analysis of such studies can be successfully performed by multi-state models. In the multi-state framework, issues of interest include the study of the relationship between covariates and disease evolution, estimation of transition probabilities, and survival rates. This paper introduces p3state.msm, a software application for R which performs inference in an illness-death model. It describes the capabilities of the program for estimating semi-parametric regression models and for implementing nonparametric estimators for several quantities. The main feature of the package is its ability for obtaining nonMarkov estimates for the transition probabilities. Moreover, the methods can also be used in progressive three-state models. In such a model, estimators for other quantities, such as the bivariate distribution function (for sequentially ordered events), are also given. The software is illustrated using data from the Stanford Heart Transplant Study.},
	language = {en},
	urldate = {2024-03-05},
	journal = {Journal of Statistical Software},
	author = {Machado, Luís Filipe Meira and Roca-Pardiñas, Javier},
	month = jan,
	year = {2011},
	keywords = {R, survival analysis, competing risks},
	pages = {1--18},
	file = {Submitted Version:C\:\\Users\\anbe6\\Zotero\\storage\\MS3Y6DC5\\Machado and Roca-Pardiñas - 2011 - p3state.msm Analyzing Survival Data from an Illne.pdf:application/pdf;Submitted Version:C\:\\Users\\anbe6\\Zotero\\storage\\TPHSUZLZ\\Machado and Roca-Pardiñas - 2011 - p3state.msm Analyzing Survival Data from an Illne.pdf:application/pdf},
}

@article{soutinho_survidm_2021,
	title = {survidm: {An} {R} package for {Inference} and {Prediction} in an {Illness}-{Death} {Model}},
	volume = {13},
	issn = {2073-4859},
	shorttitle = {survidm},
	url = {https://journal.r-project.org/archive/2021/RJ-2021-070/index.html},
	doi = {10.32614/RJ-2021-070},
	abstract = {Multi-state models are a useful way of describing a process in which an individual moves through a number of finite states in continuous time. The illness-death model plays a central role in the theory and practice of these models, describing the dynamics of healthy subjects who may move to an intermediate "diseased" state before entering into a terminal absorbing state. In these models, one important goal is the modeling of transition rates which is usually done by studying the relationship between covariates and disease evolution. However, biomedical researchers are also interested in reporting other interpretable results in a simple and summarized manner. These include estimates of predictive probabilities, such as the transition probabilities, occupation probabilities, cumulative incidence functions, and the sojourn time distributions. The development of survidm package has been motivated by recent contribution that provides answers to all these topics. An illustration of the software usage is included using real data.},
	language = {en},
	number = {2},
	urldate = {2024-03-05},
	journal = {The R Journal},
	author = {Soutinho, Gustavo and Sestelo, Marta and Meira-Machado, Luís},
	year = {2021},
	keywords = {R, survival analysis, competing risks},
	pages = {22},
	file = {Soutinho et al. - 2021 - survidm An R package for Inference and Prediction.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\YZQXG3ZW\\Soutinho et al. - 2021 - survidm An R package for Inference and Prediction.pdf:application/pdf},
}

@article{laughlin_impact_2023,
	title = {Impact of {Medicaid} {Expansion} on {Breast} {Cancer} {Diagnosis} and {Treatment} in {Southern} {States}},
	volume = {236},
	issn = {1879-1190},
	url = {https://journals.lww.com/journalacs/fulltext/2023/04000/impact_of_medicaid_expansion_on_breast_cancer.52.aspx},
	doi = {10.1097/XCS.0000000000000550},
	abstract = {BACKGROUND: 
          Medicaid expansion impacted patients when assessed at a national level. However, of the 32 states that expanded Medicaid, only three were Southern states. Whether results apply to Southern states that share similar geopolitical perspectives remains elusive. We aimed to assess the impact of Medicaid expansion on breast cancer diagnosis and treatment in 8 Southern states in the US.
          STUDY DESIGN: 
          We identified uninsured or Medicaid patients (age 40 to 64 years) diagnosed with invasive breast cancer from 2011 to 2018 in Southern states from the North American Association of Central Cancer Registries–Cancer in North America Research Dataset. Medicaid-expanded states ([MES], Louisiana, Kentucky, Arkansas) were compared with non-MES ([NMES], Tennessee, Alabama, Mississippi, Texas, Oklahoma) using multivariate logistic regression and differences-in-differences analyses during pre- and postexpansion periods; p {\textless} 0.05 was considered statistically significant.
          RESULTS: 
          Among 21,974 patients, patients in MES had increased odds of Medicaid insurance by 43\% (odds ratio 1.43, p {\textless} 0.01) and decreased odds of distant-stage disease by 7\% (odds ratio 0.93, p = 0.03). After Medicaid expansion, Medicaid patients increased by 10.6\% in MES (Arkansas, Kentucky), in contrast to a 1.3\% decrease in NMES (differences-in-differences 11.9\%, p {\textless} 0. 0001, adjusting for age, race/ethnicity, rural-urban status, and poverty status). MES (Arkansas, Kentucky) had 2.3\% fewer patients diagnosed with distant-stage disease compared with a 0.5\% increase in NMES (differences-in-differences 2.8\%, p = 0.01, after adjustment). Patients diagnosed in MES had higher odds of receiving treatment (odds ratio 2.27, p = 0.03).
          CONCLUSIONS: 
          Unlike NMES, MES experienced increased Medicaid insured, increased treatment, and decreased distant-stage disease at diagnosis. Medicaid expansion in the South leads to earlier and more comprehensive treatment of breast cancer.




        Export},
	language = {en-US},
	number = {4},
	urldate = {2024-03-04},
	journal = {Journal of the American College of Surgeons},
	author = {Laughlin, Amy I. and Li, Tingting and Yu, Qingzhao and Wu, Xiao-Cheng and Yi, Yong and Hsieh, Mei-Chin and Havron, William and Shoup, Margo and Chu, Quyen D.},
	month = apr,
	year = {2023},
	pages = {838},
	file = {Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\TISIK82F\\impact_of_medicaid_expansion_on_breast_cancer.52.html:text/html},
}

@article{carmezim_redcapdm_2024,
	title = {{REDCapDM}: {An} {R} package with a set of data management tools for a {REDCap} project},
	volume = {24},
	issn = {1471-2288},
	shorttitle = {{REDCapDM}},
	url = {https://doi.org/10.1186/s12874-024-02178-6},
	doi = {10.1186/s12874-024-02178-6},
	abstract = {Research Electronic Data CAPture (REDCap) is a web application for creating and managing online surveys and databases. Clinical data management is an essential process before performing any statistical analysis to ensure the quality and reliability of study information. Processing REDCap data in R can be complex and often benefits from automation. While there are several R packages available for specific tasks, none offer an expansive approach to data management.},
	number = {1},
	urldate = {2024-03-04},
	journal = {BMC Medical Research Methodology},
	author = {Carmezim, João and Satorra, Pau and Peñafiel, Judith and García-Lerma, Esther and Pallarès, Natàlia and Santos, Naiara and Tebé, Cristian},
	month = mar,
	year = {2024},
	keywords = {R, REDCap},
	pages = {55},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\6UU7QEB9\\Carmezim et al. - 2024 - REDCapDM An R package with a set of data manageme.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\PMVBKNPF\\s12874-024-02178-6.html:text/html},
}

@article{knuppel_dag_2010,
	title = {{DAG} {Program}:: {Identifying} {Minimal} {Sufficient} {Adjustment} {Sets}},
	volume = {21},
	issn = {1044-3983},
	shorttitle = {{DAG} {Program}},
	url = {https://journals.lww.com/epidem/fulltext/2010/01000/dag_program___identifying_minimal_sufficient.29.aspx},
	doi = {10.1097/EDE.0b013e3181c307ce},
	abstract = {An abstract is unavailable.},
	language = {en-US},
	number = {1},
	urldate = {2024-03-01},
	journal = {Epidemiology},
	author = {Knüppel, Sven and Stang, Andreas},
	month = jan,
	year = {2010},
	keywords = {causal inference, DAG},
	pages = {159},
	file = {Knüppel and Stang - 2010 - DAG Program Identifying Minimal Sufficient Adjus.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\9S3XZBIV\\Knüppel and Stang - 2010 - DAG Program Identifying Minimal Sufficient Adjus.pdf:application/pdf},
}

@article{jackson_multi-state_2011,
	title = {Multi-{State} {Models} for {Panel} {Data}: {The} msm {Package} for {R}},
	volume = {38},
	copyright = {Copyright (c) 2009 Christopher Jackson},
	issn = {1548-7660},
	shorttitle = {Multi-{State} {Models} for {Panel} {Data}},
	url = {https://doi.org/10.18637/jss.v038.i08},
	doi = {10.18637/jss.v038.i08},
	abstract = {Panel data are observations of a continuous-time process at arbitrary times, for example, visits to a hospital to diagnose disease status. Multi-state models for such data are generally based on the Markov assumption. This article reviews the range of Markov models and their extensions which can be fitted to panel-observed data, and their implementation in the msm package for R. Transition intensities may vary between individuals, or with piecewise-constant time-dependent covariates, giving an inhomogeneous Markov model. Hidden Markov models can be used for multi-state processes which are misclassified or observed only through a noisy marker. The package is intended to be straightforward to use, flexible and comprehensively documented. Worked examples are given of the use of msm to model chronic disease progression and screening. Assessment of model fit, and potential future developments of the software, are also discussed.},
	language = {en},
	urldate = {2024-02-29},
	journal = {Journal of Statistical Software},
	author = {Jackson, Christopher},
	month = jan,
	year = {2011},
	keywords = {R, survival analysis, multi-state},
	pages = {1--28},
	file = {Submitted Version:C\:\\Users\\anbe6\\Zotero\\storage\\7BRAQ43C\\Jackson - 2011 - Multi-State Models for Panel Data The msm Package.pdf:application/pdf},
}

@misc{greifer_choosing_2021,
	title = {Choosing the {Causal} {Estimand} for {Propensity} {Score} {Analysis} of {Observational} {Studies}},
	url = {http://arxiv.org/abs/2106.10577},
	doi = {10.48550/arXiv.2106.10577},
	abstract = {Matching and weighting methods for observational studies involve the choice of an estimand, the causal effect with reference to a specific target population. Commonly used estimands include the average treatment effect in the treated (ATT), the average treatment effect in the untreated (ATU), the average treatment effect in the population (ATE), and the average treatment effect in the overlap (i.e., equipoise population; ATO). Each estimand has its own assumptions, interpretation, and statistical methods that can be used to estimate it. This article provides guidance on selecting and interpreting an estimand to help medical researchers correctly implement statistical methods used to estimate causal effects in observational studies and to help audiences correctly interpret the results and limitations of these studies. The interpretations of the estimands resulting from regression and instrumental variable analyses are also discussed. Choosing an estimand carefully is essential for making valid inferences from the analysis of observational data and ensuring results are replicable and useful for practitioners.},
	urldate = {2024-02-29},
	publisher = {arXiv},
	author = {Greifer, Noah and Stuart, Elizabeth A.},
	month = jun,
	year = {2021},
	note = {arXiv:2106.10577 [stat]
version: 1},
	keywords = {causal inference, propensity score},
	file = {arXiv Fulltext PDF:C\:\\Users\\anbe6\\Zotero\\storage\\N43HEIUW\\Greifer and Stuart - 2021 - Choosing the Causal Estimand for Propensity Score .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\EEIVEGAH\\2106.html:text/html},
}

@article{suzuki_causal_2020,
	title = {Causal {Diagrams}: {Pitfalls} and {Tips}},
	volume = {30},
	issn = {0917-5040, 1349-9092},
	shorttitle = {Causal {Diagrams}},
	url = {https://www.jstage.jst.go.jp/article/jea/30/4/30_JE20190192/_article},
	doi = {10.2188/jea.JE20190192},
	abstract = {Graphical models are useful tools in causal inference, and causal directed acyclic graphs (DAGs) are used extensively to determine the variables for which it is suﬃcient to control for confounding to estimate causal eﬀects. We discuss the following ten pitfalls and tips that are easily overlooked when using DAGs: 1) Each node on DAGs corresponds to a random variable and not its realized values; 2) The presence or absence of arrows in DAGs corresponds to the presence or absence of individual causal eﬀect in the population; 3) “Non-manipulable” variables and their arrows should be drawn with care; 4) It is preferable to draw DAGs for the total population, rather than for the exposed or unexposed groups; 5) DAGs are primarily useful to examine the presence of confounding in distribution in the notion of confounding in expectation; 6) Although DAGs provide qualitative diﬀerences of causal structures, they cannot describe details of how to adjust for confounding; 7) DAGs can be used to illustrate the consequences of matching and the appropriate handling of matched variables in cohort and case-control studies; 8) When explicitly accounting for temporal order in DAGs, it is necessary to use separate nodes for each timing; 9) In certain cases, DAGs with signed edges can be used in drawing conclusions about the direction of bias; and 10) DAGs can be (and should be) used to describe not only confounding bias but also other forms of bias. We also discuss recent developments of graphical models and their future directions.},
	language = {en},
	number = {4},
	urldate = {2024-02-29},
	journal = {Journal of Epidemiology},
	author = {Suzuki, Etsuji and Shinozaki, Tomohiro and Yamamoto, Eiji},
	month = apr,
	year = {2020},
	keywords = {causal inference},
	pages = {153--162},
	file = {Suzuki et al. - 2020 - Causal Diagrams Pitfalls and Tips.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\G39CQSI7\\Suzuki et al. - 2020 - Causal Diagrams Pitfalls and Tips.pdf:application/pdf},
}

@article{varadhan_evaluating_2010,
	title = {Evaluating {Health} {Outcomes} in the {Presence} of {Competing} {Risks} {A} {Review} of {Statistical} {Methods} and {Clinical} {Applications}},
	url = {www.lww-medicalcare.com},
	abstract = {Background: An evaluation of the effect of a healthcare intervention (or an exposure) must consider multiple possible outcomes, including the primary outcome of interest and other outcomes such as adverse events or mortality. The determination of the likelihood of benefit from an intervention, in the presence of other competing outcomes, is a competing risks problem. Although statistical methods exist for quantifying the probability of benefit from an intervention while accounting for competing events, these methods have not been widely adopted by clinical researchers. Objectives: (1) To demonstrate the importance of considering competing risks in the evaluation of treatment effectiveness, and (2) to review appropriate statistical methods, and recommend how they might be applied. Research Design and Methods: We reviewed 3 statistical approaches for analyzing the competing risks problem: (a) cause-specific hazard (CSH), (b) cumulative incidence function (CIF), and (c) event-free survival (EFS). We compare these methods using a simulation study and a reanalysis of a randomized clinical trial. Results: Simulation studies evaluating the statistical power to detect the effect of intervention under different scenarios showed that: (1) CSH approach is best for detecting the effect of an intervention if the intervention only affects either the primary outcome or the competing event; (2) EFS approach is best only when the intervention affects both primary and competing events in the same manner; and (3) CIF approach is best when the intervention affects both primary and competing events, but in opposite directions. Using data from a randomized controlled trial, we demonstrated that a comprehensive approach using all 3 approaches provided useful insights on the},
	journal = {Medical Care},
	author = {Varadhan, Ravi and Weiss, Carlos O and Segal, Jodi B and Wu, Albert W and Scharfstein, Daniel and Boyd, Cynthia},
	year = {2010},
	keywords = {survival analysis, competing risks},
	file = {Varadhan et al. - 2010 - Evaluating Health Outcomes in the Presence of Comp.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\84RNJK8T\\Varadhan et al. - 2010 - Evaluating Health Outcomes in the Presence of Comp.pdf:application/pdf},
}

@article{scheike_analyzing_2011,
	title = {Analyzing {Competing} {Risk} {Data} {Using} the {R} timereg {Package}},
	volume = {38},
	issn = {1548-7660},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3375021/},
	abstract = {In this paper we describe flexible competing risks regression models using the comp.risk() function available in the timereg package for R based on Scheike et al. (2008). Regression models are specified for the transition probabilities, that is the cumulative incidence in the competing risks setting. The model contains the  model as a special case. This can be used to do goodness-of-fit test for the subdistribution hazards’ proportionality assumption (). The program can also construct confidence bands for predicted cumulative incidence curves., We apply the methods to data on follicular cell lymphoma from , where the competing risks are disease relapse and death without relapse. There is important non-proportionality present in the data, and it is demonstrated how one can analyze these data using the flexible regression models.},
	number = {2},
	urldate = {2024-02-07},
	journal = {J Stat Softw},
	author = {Scheike, Thomas H. and Zhang, Mei-Jie},
	month = jan,
	year = {2011},
	pmid = {22707920},
	pmcid = {PMC3375021},
	keywords = {R, survival analysis, competing risks},
	pages = {i02},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\XPXCTJ9M\\Scheike and Zhang - 2011 - Analyzing Competing Risk Data Using the R timereg .pdf:application/pdf},
}

@article{bascoul-mollevi_eortc_2015,
	title = {{EORTC} {QLQ}-{C30} {Descriptive} {Analysis} with the qlqc30 {Command}},
	volume = {15},
	issn = {1536-867X},
	url = {https://doi.org/10.1177/1536867X1501500407},
	doi = {10.1177/1536867X1501500407},
	abstract = {Health-related quality of life is often an endpoint in oncology clinical trials. The European Organization for Research and Treatment of Cancer (EORTC) developed the cancer-specific quality of life questionnaire (QLQ-C30), which includes five functions, nine symptoms, and a global health status. These questionnaires are completed by the patients themselves throughout the process of care. The recommended approaches for processing EORTC QLQ-C30 data are usually descriptive and graphic.Our aim was to develop a user-written command that provided an automatic descriptive analysis of EORTC QLQ-C30 data, consisting of profile plots per visit and longitudinal plots per functional and symptom scale.},
	number = {4},
	urldate = {2024-02-14},
	journal = {The Stata Journal},
	author = {Bascoul-Mollevi, Caroline and Castan, Florence and Azria, David and Gourgou-Bourgade, Sophie},
	month = dec,
	year = {2015},
	note = {Publisher: SAGE Publications},
	keywords = {qol},
	pages = {1060--1074},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\54J9GRT7\\Bascoul-Mollevi et al. - 2015 - EORTC QLQ-C30 Descriptive Analysis with the qlqc30.pdf:application/pdf},
}

@article{amoah_comparing_2020,
	title = {Comparing {Propensity} {Score} {Methods} {Versus} {Traditional} {Regression} {Analysis} for the {Evaluation} of {Observational} {Data}: {A} {Case} {Study} {Evaluating} the {Treatment} of {Gram}-{Negative} {Bloodstream} {Infections}},
	volume = {71},
	issn = {1058-4838},
	shorttitle = {Comparing {Propensity} {Score} {Methods} {Versus} {Traditional} {Regression} {Analysis} for the {Evaluation} of {Observational} {Data}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7713675/},
	doi = {10.1093/cid/ciaa169},
	abstract = {This article assists clinicians with understanding the relative pros and cons, the general steps involved, and the appropriate interpretation of the results of traditional regression analysis, propensity score matching, propensity score weighting, and propensity score stratification.},
	number = {9},
	urldate = {2024-02-23},
	journal = {Clin Infect Dis},
	author = {Amoah, Joe and Stuart, Elizabeth A and Cosgrove, Sara E and Harris, Anthony D and Han, Jennifer H and Lautenbach, Ebbing and Tamma, Pranita D},
	month = mar,
	year = {2020},
	pmid = {32069360},
	pmcid = {PMC7713675},
	keywords = {causal inference, propensity score},
	pages = {e497--e505},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\QMHYHRF9\\Amoah et al. - 2020 - Comparing Propensity Score Methods Versus Traditio.pdf:application/pdf},
}

@article{zhang_time-varying_2018,
	title = {Time-varying covariates and coefficients in {Cox} regression models},
	volume = {6},
	issn = {2305-5839},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6015946/},
	doi = {10.21037/atm.2018.02.12},
	abstract = {Time-varying covariance occurs when a covariate changes over time during the follow-up period. Such variable can be analyzed with the Cox regression model to estimate its effect on survival time. For this it is essential to organize the data in a counting process style. In situations when the proportional hazards assumption of the Cox regression model does not hold, we say that the effect of the covariate is time-varying. The proportional hazards assumption can be tested by examining the residuals of the model. The rejection of the null hypothesis induces the use of time varying coefficient to describe the data. The time varying coefficient can be described with a step function or a parametric time function. This article aims to illustrate how to carry out statistical analyses in the presence of time-varying covariates or coefficients with R.},
	number = {7},
	urldate = {2024-02-07},
	journal = {Ann Transl Med},
	author = {Zhang, Zhongheng and Reinikainen, Jaakko and Adeleke, Kazeem Adedayo and Pieterse, Marcel E. and Groothuis-Oudshoorn, Catharina G. M.},
	month = apr,
	year = {2018},
	pmid = {29955581},
	pmcid = {PMC6015946},
	keywords = {survival analysis},
	pages = {121},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\9ISK4PQE\\Zhang et al. - 2018 - Time-varying covariates and coefficients in Cox re.pdf:application/pdf},
}

@article{ioroi_data_2015,
	title = {Data analysis methods for assessing palliative care interventions in one-group pre–post studies},
	volume = {3},
	issn = {2050-3121},
	url = {https://doi.org/10.1177/2050312115621313},
	doi = {10.1177/2050312115621313},
	abstract = {Objectives: Studies of palliative care are often performed using single-arm pre–post study designs that lack causal inference. Thus, in this study, we propose a novel data analysis approach that incorporates risk factors from single-arm studies instead of using paired t-tests to assess intervention effects.
Methods: Physical, psychological and social evaluations of eligible cancer inpatients were conducted by a hospital-based palliative care team. Quality of life was assessed at baseline and after 7 days of symptomatic treatment using the European Organization for Research and Treatment of Cancer QLQ-C15-PAL. Among 35 patients, 9 were discharged within 1 week and 26 were included in analyses. Structural equation models with observed measurements were applied to estimate direct and indirect intervention effects and simultaneously consider risk factors.
Results: Parameters were estimated using full models that included associations among covariates and reduced models that excluded covariates with small effects. The total effect was calculated as the sum of intervention and covariate effects and was equal to the mean of the difference (0.513) between pre- and post-intervention quality of life (reduced model intervention effect, 14.749; 95\% confidence intervals, −4.407 and 33.905; p = 0.131; covariate effect, −14.236; 95\% confidence interval, −33.708 and 5.236; p = 0.152).
Conclusion: Using the present analytical method for single-arm pre–post study designs, factors that modulate effects of interventions were modelled, and intervention and covariate effects were distinguished based on structural equation model.},
	language = {en},
	urldate = {2024-02-23},
	journal = {SAGE Open Medicine},
	author = {Ioroi, Takeshi and Kakuma, Tatsuyuki and Sakashita, Akihiro and Miki, Yuki and Ohtagaki, Kanako and Fujiwara, Yuka and Utsubo, Yuko and Nishimura, Yoshihiro and Hirai, Midori},
	month = jan,
	year = {2015},
	note = {Publisher: SAGE Publications Ltd},
	keywords = {palliative care},
	pages = {2050312115621313},
	file = {SAGE PDF Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\BBZHV6T2\\Ioroi et al. - 2015 - Data analysis methods for assessing palliative car.pdf:application/pdf},
}

@article{kaufman_selection_2021,
	title = {Selection {Bias} in {Observational} {Studies} of {Palliative} {Care}: {Lessons} {Learned}},
	volume = {61},
	issn = {08853924},
	shorttitle = {Selection {Bias} in {Observational} {Studies} of {Palliative} {Care}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885392420307399},
	doi = {10.1016/j.jpainsymman.2020.09.011},
	abstract = {Objectives. To quantify selection bias because of observed and unobserved characteristics in a PC demonstration program.
Methods. Program administrative data and 100\% Medicare claims data in two states and a 20\% sample in eight states (2013e2017). The sample included 2983 Medicare fee-for-service beneﬁciaries aged 65þ participating in the PC program and three matched cohorts: regional; two states; and eight states. Confounding because of observed factors was measured by comparing patient baseline characteristics. Confounding because of unobserved factors was measured by comparing days of follow-up and six-month and one-year mortality rates.
Results. After matching, evidence for observed confounding included differences in observable baseline characteristics, including race, morbidity, and utilization. Evidence for unobserved confounding included signiﬁcantly longer mean follow-up in the regional, two-state, and eight-state comparison cohorts, with 207 (P {\textless} 0.001), 192 (P {\textless} 0.001), and 187 (P {\textless} 0.001) days, respectively, compared with the 162 days for the PC cohort. The PC cohort had higher six-month and one-year mortality rates of 53.5\% and 64.5\% compared with 43.5\% and 48.0\% in the regional comparison, 53.4\% and 57.4\% in the two-state comparison, and 55.0\% and 59.0\% in the eight-state comparison.
Conclusion. This case study demonstrates that selection of comparison groups impacts the magnitude of measured and unmeasured confounding, which may change effect estimates. The substantial impact of confounding on effect estimates in this study raises concerns about the evaluation of novel serious illness care models in the absence of randomization. We present key lessons learned for improving future evaluations of PC using observational study designs. J Pain Symptom Manage 2021;61:1002e1011. Published by Elsevier Inc. on behalf of American Academy of Hospice and Palliative Medicine.},
	language = {en},
	number = {5},
	urldate = {2024-02-23},
	journal = {Journal of Pain and Symptom Management},
	author = {Kaufman, Brystana G. and Van Houtven, Courtney H. and Greiner, Melissa A. and Hammill, Bradley G. and Harker, Matthew and Anderson, David and Petry, Sarah and Bull, Janet and Taylor, Donald H.},
	month = may,
	year = {2021},
	keywords = {causal inference, propensity score, palliative care},
	pages = {1002--1011.e2},
	file = {Kaufman et al. - 2021 - Selection Bias in Observational Studies of Palliat.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\KIF3ZTAC\\Kaufman et al. - 2021 - Selection Bias in Observational Studies of Palliat.pdf:application/pdf},
}

@article{kern_impact_2020,
	title = {Impact on place of death in cancer patients: a causal exploration in southern {Switzerland}},
	volume = {19},
	issn = {1472-684X},
	shorttitle = {Impact on place of death in cancer patients},
	url = {https://doi.org/10.1186/s12904-020-00664-4},
	doi = {10.1186/s12904-020-00664-4},
	abstract = {Most terminally ill cancer patients prefer to die at home, but a majority die in institutional settings. Research questions about this discrepancy have not been fully answered. This study applies artificial intelligence and machine learning techniques to explore the complex network of factors and the cause-effect relationships affecting the place of death, with the ultimate aim of developing policies favouring home-based end-of-life care.},
	number = {1},
	urldate = {2024-02-23},
	journal = {BMC Palliative Care},
	author = {Kern, Heidi and Corani, Giorgio and Huber, David and Vermes, Nicola and Zaffalon, Marco and Varini, Marco and Wenzel, Claudia and Fringer, André},
	month = oct,
	year = {2020},
	keywords = {causal inference},
	pages = {160},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\YGZ4GQGF\\Kern et al. - 2020 - Impact on place of death in cancer patients a cau.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\BFL2IZYA\\s12904-020-00664-4.html:text/html},
}

@article{baiocchi_tutorial_2014,
	title = {Tutorial in {Biostatistics}: {Instrumental} {Variable} {Methods} for {Causal} {Inference}*},
	volume = {33},
	issn = {0277-6715},
	shorttitle = {Tutorial in {Biostatistics}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4201653/},
	doi = {10.1002/sim.6128},
	abstract = {A goal of many health studies is to determine the causal effect of a treatment or intervention on health outcomes. Often, it is not ethically or practically possible to conduct a perfectly randomized experiment and instead an observational study must be used. A major challenge to the validity of observational studies is the possibility of unmeasured confounding (i.e., unmeasured ways in which the treatment and control groups differ before treatment administration which also affect the outcome). Instrumental variables analysis is a method for controlling for unmeasured confounding. This type of analysis requires the measurement of a valid instrumental variable, which is a variable that (i) is independent of the unmeasured confounding; (ii) affects the treatment; and (iii) affects the outcome only indirectly through its effect on the treatment. This tutorial discusses the types of causal effects that can be estimated by instrumental variables analysis; the assumptions needed for instrumental variables analysis to provide valid estimates of causal effects and sensitivity analysis for those assumptions; methods of estimation of causal effects using instrumental variables; and sources of instrumental variables in health studies.},
	number = {13},
	urldate = {2024-02-23},
	journal = {Stat Med},
	author = {Baiocchi, Michael and Cheng, Jing and Small, Dylan S.},
	month = jun,
	year = {2014},
	pmid = {24599889},
	pmcid = {PMC4201653},
	keywords = {causal inference},
	pages = {2297--2340},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\QGTFGPKW\\Baiocchi et al. - 2014 - Tutorial in Biostatistics Instrumental Variable M.pdf:application/pdf},
}

@article{buranupakorn_emulation_2021,
	title = {Emulation of a {Target} {Trial} to {Evaluate} the {Causal} {Effect} of {Palliative} {Care} {Consultation} on the {Survival} {Time} of {Patients} with {Hepatocellular} {Carcinoma}},
	volume = {13},
	issn = {2072-6694},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7956840/},
	doi = {10.3390/cancers13050992},
	abstract = {Simple Summary
Hepatocellular carcinoma (HCC) is a common and aggressive liver cancer. As most patients are diagnosed during an incurable stage of the disease, they usually face great suffering during the end-of-life period. Palliative care can improve the patient’s quality of life and alleviate both physical and psychological symptoms. However, the discipline is underutilized due to a common misconception that it will accelerate the patient’s death. We emulated a hypothetical target trial to evaluate the causal effect of palliative care consultation on the survival time of patients diagnosed with HCC from retrospective observational data of a Thai tertiary care center. Although no clear survival benefit or harm was identified, palliative care consultation significantly reduced the use of unnecessary life-sustaining intervention, healthcare costs, and the risk of dying in the hospital among patients with HCC during their end-of-life period.

Abstract
Palliative care has the potential to improve the quality of life of patients with incurable diseases or cancer, such as hepatocellular carcinoma (HCC). A common misconception of palliative care with respect to the patient’s survival remains a significant barrier to the discipline. This study aimed to provide causal evidence for the effect of palliative care consultation on the survival time after diagnosis among HCC patients. An emulation of a target trial was conducted on a retrospective cohort of HCC patients from January 2017 to August 2019. The primary endpoint was the restricted mean survival time (RMST) at 12 months after HCC diagnosis. We used the clone–censor–weight approach to account for potential immortal time bias. In this study, 86 patients with palliative care consultation and 71 patients without palliative care consultation were included. The adjusted RMST difference was −29.7 (95\% confidence interval (CI): −81.7, 22.3; p-value = 0.263) days in favor of no palliative care consultation. However, palliative care consultation was associated with an increase in the prescription of symptom control medications, as well as a reduction in life-sustaining interventions and healthcare costs. Our findings suggest that palliative care consultation was associated with neither additional survival benefit nor harm in HCC patients. The misconception that it significantly accelerates the dying process should be disregarded.},
	number = {5},
	urldate = {2024-02-23},
	journal = {Cancers (Basel)},
	author = {Buranupakorn, Tassaya and Thangsuk, Phaviga and Patumanond, Jayanton and Phinyo, Phichayut},
	month = feb,
	year = {2021},
	pmid = {33673534},
	pmcid = {PMC7956840},
	keywords = {causal inference},
	pages = {992},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\WPWETEJZ\\Buranupakorn et al. - 2021 - Emulation of a Target Trial to Evaluate the Causal.pdf:application/pdf},
}

@article{hernan_using_2016,
	title = {Using {Big} {Data} to {Emulate} a {Target} {Trial} {When} a {Randomized} {Trial} {Is} {Not} {Available}},
	volume = {183},
	url = {https://dx.doi.org/10.1093/aje/kwv254},
	doi = {10.1093/AJE/KWV254},
	abstract = {Ideally, questions about comparative effectiveness or safety would be answered using an appropriately designed and conducted randomized experiment. When we cannot conduct a randomized experiment, we analyze observational data. Causal inference from large observational databases (big data) can be viewed as an attempt to emulate a randomized experiment - the target experiment or target trial - that would answer the question of interest. When the goal is to guide decisions among several strategies, causal analyses of observational data need to be evaluated with respect to how well they emulate a particular target trial. We outline a framework for comparative effectiveness research using big data that makes the target trial explicit. This framework channels counterfactual theory for comparing the effects of sustained treatment strategies, organizes analytic approaches, provides a structured process for the criticism of observational studies, and helps avoid common methodologic pitfalls.},
	number = {8},
	journal = {American Journal of Epidemiology},
	author = {Hernán, Miguel A and Robins, James M},
	month = apr,
	year = {2016},
	note = {Publisher: Oxford Academic},
	keywords = {causal inference, trial design, observational},
	pages = {758--764},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\RJ96PSV4\\Hernán and Robins - 2016 - Using Big Data to Emulate a Target Trial When a Ra.pdf:application/pdf},
}

@article{chesnaye_introduction_2022,
	title = {An introduction to inverse probability of treatment weighting in observational research},
	volume = {15},
	issn = {2048-8505, 2048-8513},
	url = {https://academic.oup.com/ckj/article/15/1/14/6358134},
	doi = {10.1093/ckj/sfab158},
	abstract = {In this article we introduce the concept of inverse probability of treatment weighting (IPTW) and describe how this method can be applied to adjust for measured confounding in observational research, illustrated by a clinical example from nephrology. IPTW involves two main steps. First, the probability—or propensity—of being exposed to the risk factor or intervention of interest is calculated, given an individual’s characteristics (i.e. propensity score). Second, weights are calculated as the inverse of the propensity score. The application of these weights to the study population creates a pseudopopulation in which confounders are equally distributed across exposed and unexposed groups. We also elaborate on how weighting can be applied in longitudinal studies to deal with informative censoring and time-dependent confounding in the setting of treatment-confounder feedback.},
	language = {en},
	number = {1},
	urldate = {2024-02-26},
	journal = {Clinical Kidney Journal},
	author = {Chesnaye, Nicholas C and Stel, Vianda S and Tripepi, Giovanni and Dekker, Friedo W and Fu, Edouard L and Zoccali, Carmine and Jager, Kitty J},
	month = jan,
	year = {2022},
	keywords = {causal inference, propensity score},
	pages = {14--20},
	file = {Chesnaye et al. - 2022 - An introduction to inverse probability of treatmen.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\PLJ9FRHJ\\Chesnaye et al. - 2022 - An introduction to inverse probability of treatmen.pdf:application/pdf},
}

@article{austin_moving_2015,
	title = {Moving towards best practice when using inverse probability of treatment weighting ({IPTW}) using the propensity score to estimate causal treatment effects in observational studies},
	volume = {34},
	copyright = {© 2015 The Authors. Statistics in Medicine Published by John Wiley \& Sons Ltd.},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.6607},
	doi = {10.1002/sim.6607},
	abstract = {The propensity score is defined as a subject's probability of treatment selection, conditional on observed baseline covariates. Weighting subjects by the inverse probability of treatment received creates a synthetic sample in which treatment assignment is independent of measured baseline covariates. Inverse probability of treatment weighting (IPTW) using the propensity score allows one to obtain unbiased estimates of average treatment effects. However, these estimates are only valid if there are no residual systematic differences in observed baseline characteristics between treated and control subjects in the sample weighted by the estimated inverse probability of treatment. We report on a systematic literature review, in which we found that the use of IPTW has increased rapidly in recent years, but that in the most recent year, a majority of studies did not formally examine whether weighting balanced measured covariates between treatment groups. We then proceed to describe a suite of quantitative and qualitative methods that allow one to assess whether measured baseline covariates are balanced between treatment groups in the weighted sample. The quantitative methods use the weighted standardized difference to compare means, prevalences, higher-order moments, and interactions. The qualitative methods employ graphical methods to compare the distribution of continuous baseline covariates between treated and control subjects in the weighted sample. Finally, we illustrate the application of these methods in an empirical case study. We propose a formal set of balance diagnostics that contribute towards an evolving concept of ‘best practice’ when using IPTW to estimate causal treatment effects using observational data. © 2015 The Authors. Statistics in Medicine Published by John Wiley \& Sons Ltd.},
	language = {en},
	number = {28},
	urldate = {2024-02-23},
	journal = {Statistics in Medicine},
	author = {Austin, Peter C. and Stuart, Elizabeth A.},
	year = {2015},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.6607},
	keywords = {causal inference, propensity score},
	pages = {3661--3679},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\8F2DZE8U\\Austin and Stuart - 2015 - Moving towards best practice when using inverse pr.pdf:application/pdf},
}

@article{austin_variance_2016,
	title = {Variance estimation when using inverse probability of treatment weighting ({IPTW}) with survival analysis},
	volume = {35},
	copyright = {© 2016 The Authors. Statistics in Medicine published by John Wiley \& Sons Ltd.},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7084},
	doi = {10.1002/sim.7084},
	abstract = {Propensity score methods are used to reduce the effects of observed confounding when using observational data to estimate the effects of treatments or exposures. A popular method of using the propensity score is inverse probability of treatment weighting (IPTW). When using this method, a weight is calculated for each subject that is equal to the inverse of the probability of receiving the treatment that was actually received. These weights are then incorporated into the analyses to minimize the effects of observed confounding. Previous research has found that these methods result in unbiased estimation when estimating the effect of treatment on survival outcomes. However, conventional methods of variance estimation were shown to result in biased estimates of standard error. In this study, we conducted an extensive set of Monte Carlo simulations to examine different methods of variance estimation when using a weighted Cox proportional hazards model to estimate the effect of treatment. We considered three variance estimation methods: (i) a naïve model-based variance estimator; (ii) a robust sandwich-type variance estimator; and (iii) a bootstrap variance estimator. We considered estimation of both the average treatment effect and the average treatment effect in the treated. We found that the use of a bootstrap estimator resulted in approximately correct estimates of standard errors and confidence intervals with the correct coverage rates. The other estimators resulted in biased estimates of standard errors and confidence intervals with incorrect coverage rates. Our simulations were informed by a case study examining the effect of statin prescribing on mortality. © 2016 The Authors. Statistics in Medicine published by John Wiley \& Sons Ltd.},
	language = {en},
	number = {30},
	urldate = {2024-02-27},
	journal = {Statistics in Medicine},
	author = {Austin, Peter C.},
	year = {2016},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.7084},
	keywords = {causal inference, propensity score},
	pages = {5642--5655},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\6XRD4DU7\\Austin - 2016 - Variance estimation when using inverse probability.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\CDIYBJ5P\\sim.html:text/html},
}

@article{austin_introduction_2011,
	title = {An {Introduction} to {Propensity} {Score} {Methods} for {Reducing} the {Effects} of {Confounding} in {Observational} {Studies}},
	volume = {46},
	issn = {0027-3171},
	url = {https://doi.org/10.1080/00273171.2011.568786},
	doi = {10.1080/00273171.2011.568786},
	abstract = {The propensity score is the probability of treatment assignment conditional on observed baseline characteristics. The propensity score allows one to design and analyze an observational (nonrandomized) study so that it mimics some of the particular characteristics of a randomized controlled trial. In particular, the propensity score is a balancing score: conditional on the propensity score, the distribution of observed baseline covariates will be similar between treated and untreated subjects. I describe 4 different propensity score methods: matching on the propensity score, stratification on the propensity score, inverse probability of treatment weighting using the propensity score, and covariate adjustment using the propensity score. I describe balance diagnostics for examining whether the propensity score model has been adequately specified. Furthermore, I discuss differences between regression-based methods and propensity score-based methods for the analysis of observational data. I describe different causal average treatment effects and their relationship with propensity score analyses.},
	number = {3},
	urldate = {2024-02-23},
	journal = {Multivariate Behavioral Research},
	author = {Austin, Peter C.},
	month = may,
	year = {2011},
	pmid = {21818162},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/00273171.2011.568786},
	keywords = {causal inference, propensity score},
	pages = {399--424},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\A55SV839\\Austin - 2011 - An Introduction to Propensity Score Methods for Re.pdf:application/pdf},
}

@article{denz_comparison_2023,
	title = {A comparison of different methods to adjust survival curves for confounders},
	volume = {42},
	copyright = {© 2023 The Authors. Statistics in Medicine published by John Wiley \& Sons Ltd.},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.9681},
	doi = {10.1002/sim.9681},
	abstract = {Treatment specific survival curves are an important tool to illustrate the treatment effect in studies with time-to-event outcomes. In non-randomized studies, unadjusted estimates can lead to biased depictions due to confounding. Multiple methods to adjust survival curves for confounders exist. However, it is currently unclear which method is the most appropriate in which situation. Our goal is to compare forms of inverse probability of treatment weighting, the G-Formula, propensity score matching, empirical likelihood estimation and augmented estimators as well as their pseudo-values based counterparts in different scenarios with a focus on their bias and goodness-of-fit. We provide a short review of all methods and illustrate their usage by contrasting the survival of smokers and non-smokers, using data from the German Epidemiological Trial on Ankle-Brachial-Index. Subsequently, we compare the methods using a Monte-Carlo simulation. We consider scenarios in which correctly or incorrectly specified models for describing the treatment assignment and the time-to-event outcome are used with varying sample sizes. The bias and goodness-of-fit is determined by taking the entire survival curve into account. When used properly, all methods showed no systematic bias in medium to large samples. Cox regression based methods, however, showed systematic bias in small samples. The goodness-of-fit varied greatly between different methods and scenarios. Methods utilizing an outcome model were more efficient than other techniques, while augmented estimators using an additional treatment assignment model were unbiased when either model was correct with a goodness-of-fit comparable to other methods. These “doubly-robust” methods have important advantages in every considered scenario.},
	language = {en},
	number = {10},
	urldate = {2024-02-29},
	journal = {Statistics in Medicine},
	author = {Denz, Robin and Klaaßen-Mielke, Renate and Timmesfeld, Nina},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.9681},
	keywords = {R, survival analysis},
	pages = {1461--1479},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\LIMXQDRD\\Denz et al. - 2023 - A comparison of different methods to adjust surviv.pdf:application/pdf},
}

@article{tian_predicting_2014,
	title = {Predicting the restricted mean event time with the subject's baseline covariates in survival analysis},
	volume = {15},
	issn = {1465-4644},
	url = {https://doi.org/10.1093/biostatistics/kxt050},
	doi = {10.1093/biostatistics/kxt050},
	abstract = {For designing, monitoring, and analyzing a longitudinal study with an event time as the outcome variable, the restricted mean event time (RMET) is an easily interpretable, clinically meaningful summary of the survival function in the presence of censoring. The RMET is the average of all potential event times measured up to a time point τ and can be estimated consistently by the area under the Kaplan–Meier curve over \$[0, {\textbackslash}tau ]\$. In this paper, we study a class of regression models, which directly relates the RMET to its “baseline” covariates for predicting the future subjects’ RMETs. Since the standard Cox and the accelerated failure time models can also be used for estimating such RMETs, we utilize a cross-validation procedure to select the “best” among all the working models considered in the model building and evaluation process. Lastly, we draw inferences for the predicted RMETs to assess the performance of the final selected model using an independent data set or a “hold-out” sample from the original data set. All the proposals are illustrated with the data from the an HIV clinical trial conducted by the AIDS Clinical Trials Group and the primary biliary cirrhosis study conducted by the Mayo Clinic.},
	number = {2},
	urldate = {2024-02-29},
	journal = {Biostatistics},
	author = {Tian, Lu and Zhao, Lihui and Wei, L. J.},
	month = apr,
	year = {2014},
	keywords = {survival analysis, restricted mean survival time},
	pages = {222--233},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\9J7D2B3V\\Tian et al. - 2014 - Predicting the restricted mean event time with the.pdf:application/pdf},
}

@article{xu_statistical_2010,
	title = {Statistical {Analysis} of {Illness}–{Death} {Processes} and {Semicompeting} {Risks} {Data}},
	volume = {66},
	copyright = {© 2009, The International Biometric Society},
	issn = {1541-0420},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2009.01340.x},
	doi = {10.1111/j.1541-0420.2009.01340.x},
	abstract = {In many instances, a subject can experience both a nonterminal and terminal event where the terminal event (e.g., death) censors the nonterminal event (e.g., relapse) but not vice versa. Typically, the two events are correlated. This situation has been termed semicompeting risks (e.g., Fine, Jiang, and Chappell, 2001, Biometrika 88, 907–939; Wang, 2003, Journal of the Royal Statistical Society, Series B 65, 257–273), and analysis has been based on a joint survival function of two event times over the positive quadrant but with observation restricted to the upper wedge. Implicitly, this approach entertains the idea of latent failure times and leads to discussion of a marginal distribution of the nonterminal event that is not grounded in reality. We argue that, similar to models for competing risks, latent failure times should generally be avoided in modeling such data. We note that semicompeting risks have more classically been described as an illness–death model and this formulation avoids any reference to latent times. We consider an illness–death model with shared frailty, which in its most restrictive form is identical to the semicompeting risks model that has been proposed and analyzed, but that allows for many generalizations and the simple incorporation of covariates. Nonparametric maximum likelihood estimation is used for inference and resulting estimates for the correlation parameter are compared with other proposed approaches. Asymptotic properties, simulations studies, and application to a randomized clinical trial in nasopharyngeal cancer evaluate and illustrate the methods. A simple and fast algorithm is developed for its numerical implementation.},
	language = {en},
	number = {3},
	urldate = {2024-02-28},
	journal = {Biometrics},
	author = {Xu, Jinfeng and Kalbfleisch, John D. and Tai, Beechoo},
	year = {2010},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1541-0420.2009.01340.x},
	keywords = {survival analysis, competing risks},
	pages = {716--725},
	file = {Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\RJDLDPVD\\j.1541-0420.2009.01340.html:text/html;Submitted Version:C\:\\Users\\anbe6\\Zotero\\storage\\Y29V9VW7\\Xu et al. - 2010 - Statistical Analysis of Illness–Death Processes an.pdf:application/pdf},
}

@article{ye_semiparametric_2007,
	title = {Semiparametric {Analysis} of {Correlated} {Recurrent} and {Terminal} {Events}},
	volume = {63},
	issn = {1541-0420},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2006.00677.x},
	doi = {10.1111/j.1541-0420.2006.00677.x},
	abstract = {In clinical and observational studies, recurrent event data (e.g., hospitalization) with a terminal event (e.g., death) are often encountered. In many instances, the terminal event is strongly correlated with the recurrent event process. In this article, we propose a semiparametric method to jointly model the recurrent and terminal event processes. The dependence is modeled by a shared gamma frailty that is included in both the recurrent event rate and terminal event hazard function. Marginal models are used to estimate the regression effects on the terminal and recurrent event processes, and a Poisson model is used to estimate the dispersion of the frailty variable. A sandwich estimator is used to achieve additional robustness. An analysis of hospitalization data for patients in the peritoneal dialysis study is presented to illustrate the proposed method.},
	language = {en},
	number = {1},
	urldate = {2024-02-28},
	journal = {Biometrics},
	author = {Ye, Yining and Kalbfleisch, John D. and Schaubel, Douglas E.},
	year = {2007},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1541-0420.2006.00677.x},
	keywords = {survival analysis, competing risks, recurrent event},
	pages = {78--87},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\89RCTV3K\\Ye et al. - 2007 - Semiparametric Analysis of Correlated Recurrent an.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\LQ63IS77\\j.1541-0420.2006.00677.html:text/html},
}

@article{haneuse_semi-competing_2016,
	title = {Semi-{Competing} {Risks} {Data} {Analysis}},
	volume = {9},
	url = {https://www.ahajournals.org/doi/10.1161/CIRCOUTCOMES.115.001841},
	doi = {10.1161/CIRCOUTCOMES.115.001841},
	abstract = {Hospital readmission is a key marker of quality of health care. Notwithstanding its widespread use, however, it remains controversial in part because statistical methods used to analyze readmission, primarily logistic regression and related models, may not appropriately account for patients who die before experiencing a readmission event within the time frame of interest. Toward resolving this, we describe and illustrate the semi-competing risks framework, which refers to the general setting where scientific interest lies with some nonterminal event (eg, readmission), the occurrence of which is subject to a terminal event (eg, death). Although several statistical analysis methods have been proposed for semi-competing risks data, we describe in detail the use of illness–death models primarily because of their relation to well-known methods for survival analysis and the availability of software. We also describe and consider in detail several existing approaches that could, in principle, be used to analyze semi-competing risks data, including composite end point and competing risks analyses. Throughout we illustrate the ideas and methods using data on N=49 763 Medicare beneficiaries hospitalized between 2011 and 2013 with a principle discharge diagnosis of heart failure.},
	number = {3},
	urldate = {2024-02-26},
	journal = {Circulation: Cardiovascular Quality and Outcomes},
	author = {Haneuse, Sebastien and Lee, Kyu Ha},
	month = may,
	year = {2016},
	note = {Publisher: American Heart Association},
	keywords = {survival analysis, competing risks},
	pages = {322--331},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\WF3MBMLY\\Haneuse and Lee - 2016 - Semi-Competing Risks Data Analysis.pdf:application/pdf},
}

@article{kennedy_repeated_2001,
	title = {Repeated {Hospitalizations} and {Self}-rated {Health} among the {Elderly}: {A} {Multivariate} {Failure} {Time} {Analysis}},
	volume = {153},
	issn = {0002-9262},
	shorttitle = {Repeated {Hospitalizations} and {Self}-rated {Health} among the {Elderly}},
	url = {https://doi.org/10.1093/aje/153.3.232},
	doi = {10.1093/aje/153.3.232},
	abstract = {The purpose of this study was to determine to what extent a single measure, self-rated health (SRH), independently predicts long-term hospitalizations due to all causes and to cardiovascular diseases by using both the standard Cox proportional hazards model and a more robust events model. The study cohort consisted of 2,812 elderly subjects residing in New Haven, Connecticut, who were followed from 1982 to 1996 as part of the Established Populations for Epidemiologic Study of the Elderly. After adjustment for baseline risk factors, using the Cox model, a favorable SRH was associated with a significantly lowered risk for a first hospitalization for all causes (risk ratio (RR) = 0.850, 95\% confidence interval (CI): 0.774, 0.934) and congestive heart failure (RR = 0.599, 95\% CI: 0.426, 0.841) but not for myocardial infarction (RR = 0.882, 95\% CI: 0.565, 1.379). With the adjusted robust events model, a positive SRH was associated with a decreased risk in both a first (RR = 0.813, 95\% CI: 0.744, 0.889) and a second (RR = 0.870, 95\% CI: 0.782, 0.968) hospitalization for any cause. These results indicate that a single measurement of SRH predicts long-term patterns of hospitalization, especially for heart failure, among older adults.},
	number = {3},
	urldate = {2024-02-29},
	journal = {American Journal of Epidemiology},
	author = {Kennedy, Byron S. and Kasl, Stanislav V. and Vaccarino, Viola},
	month = feb,
	year = {2001},
	keywords = {survival analysis, recurrent event, hospitalization},
	pages = {232--241},
	file = {Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\YUB82YCI\\80363.html:text/html},
}

@misc{gorfine_shared_2022,
	title = {Shared {Frailty} {Methods} for {Complex} {Survival} {Data}: {A} {Review} of {Recent} {Advances}},
	shorttitle = {Shared {Frailty} {Methods} for {Complex} {Survival} {Data}},
	url = {http://arxiv.org/abs/2205.05322},
	abstract = {Dependent survival data arise in many contexts. One context is clustered survival data, where survival data are collected on clusters such as families or medical centers. Dependent survival data also arise when multiple survival times are recorded for each individual. Frailty models is one common approach to handle such data. In frailty models, the dependence is expressed in terms of a random effect, called the frailty. Frailty models have been used with both Cox proportional hazards model and the accelerated failure time model. This paper reviews recent developments in the area of frailty models in a variety of settings. In each setting we provide a detailed model description, assumptions, available estimation methods, and R packages.},
	urldate = {2024-02-28},
	publisher = {arXiv},
	author = {Gorfine, Malka and Zucker, David M.},
	month = may,
	year = {2022},
	note = {arXiv:2205.05322 [stat]},
	keywords = {Statistics - Methodology},
	file = {arXiv.org Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\EWZ7L4Z5\\2205.html:text/html;Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\LEAQG3R9\\Gorfine and Zucker - 2022 - Shared Frailty Methods for Complex Survival Data .pdf:application/pdf},
}

@article{jung_joint_2019,
	title = {A joint model for recurrent events and a semi-competing risk in the presence of multi-level clustering},
	volume = {28},
	issn = {0962-2802},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7366508/},
	doi = {10.1177/0962280218790107},
	abstract = {Clinical trial designs often include multiple levels of clustering in which patients are nested within clinical sites and recurrent outcomes are nested within patients who may also experience a semi-competing risk. Traditional survival methods that analyze these processes separately may lead to erroneous inferences as they ignore possible dependencies. To account for the association between recurrent events and a semi-competing risk in the presence of two levels of clustering, we developed a semiparametric joint model. The Gaussian quadrature with a piecewise constant baseline hazard was used to estimate the unspecified baseline hazards and the likelihood. Simulations showed that the proposed joint model has good statistical properties (i.e., {\textless}5\% bias and 95\% coverage) compared to the shared frailty and joint frailty models when informative censoring and multiple levels of clustering were present. The proposed method was applied to data from an AIDS clinical trial to investigate the impact of antiretroviral treatment on recurrent AIDS-defining events (ADE) in the presence of a semi-competing risk of death and multi-level clustering and showed a significant dependency between ADE and death at the patient-level but not at the clinic-level.},
	number = {10-11},
	urldate = {2024-02-28},
	journal = {Stat Methods Med Res},
	author = {Jung, Tae Hyun and Peduzzi, Peter and Allore, Heather and Kyriakides, Tassos and Esserman, Denise},
	year = {2019},
	pmid = {30062911},
	pmcid = {PMC7366508},
	pages = {2897--2911},
	file = {PubMed Central Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\JETCTWX8\\Jung et al. - 2019 - A joint model for recurrent events and a semi-comp.pdf:application/pdf},
}

@article{lee_hierarchical_2016,
	title = {Hierarchical {Models} for {Semicompeting} {Risks} {Data} {With} {Application} to {Quality} of {End}-of-{Life} {Care} for {Pancreatic} {Cancer}},
	volume = {111},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2016.1164052},
	doi = {10.1080/01621459.2016.1164052},
	abstract = {Readmission following discharge from an initial hospitalization is a key marker of quality of healthcare in the United States. For the most part, readmission has been studied among patients with “acute” health conditions, such as pneumonia and heart failure, with analyses based on a logistic-Normal generalized linear mixed model. Naïve application of this model to the study of readmission among patients with “advanced” health conditions such as pancreatic cancer, however, is problematic because it ignores death as a competing risk. A more appropriate analysis is to imbed such a study within the semicompeting risks framework. To our knowledge, however, no comprehensive statistical methods have been developed for cluster-correlated semicompeting risks data. To resolve this gap in the literature we propose a novel hierarchical modeling framework for the analysis of cluster-correlated semicompeting risks data that permits parametric or nonparametric specifications for a range of components giving analysts substantial flexibility as they consider their own analyses. Estimation and inference is performed within the Bayesian paradigm since it facilitates the straightforward characterization of (posterior) uncertainty for all model parameters, including hospital-specific random effects. Model comparison and choice is performed via the deviance information criterion and the log-pseudo marginal likelihood statistic, both of which are based on a partially marginalized likelihood. An efficient computational scheme, based on the Metropolis-Hastings-Green algorithm, is developed and had been implemented in the R package SemiCompRisks. A comprehensive simulation study shows that the proposed framework performs very well in a range of data scenarios, and outperforms competitor analysis strategies. The proposed framework is motivated by and illustrated with an ongoing study of the risk of readmission among Medicare beneficiaries diagnosed with pancreatic cancer. Using data on n = 5298 patients at J=112 hospitals in the six New England states between 2000–2009, key scientific questions we consider include the role of patient-level risk factors on the risk of readmission and the extent of variation in risk across hospitals not explained by differences in patient case-mix. Supplementary materials for this article are available online.},
	number = {515},
	urldate = {2024-02-28},
	journal = {Journal of the American Statistical Association},
	author = {Lee, Kyu Ha and Dominici, Francesca and Schrag, Deborah and Haneuse, Sebastien},
	month = jul,
	year = {2016},
	pmid = {28303074},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.2016.1164052},
	keywords = {competing risks},
	pages = {1075--1095},
	file = {Accepted Version:C\:\\Users\\anbe6\\Zotero\\storage\\FZYSK6KX\\Lee et al. - 2016 - Hierarchical Models for Semicompeting Risks Data W.pdf:application/pdf},
}

@article{wu_analysis_2022,
	title = {Analysis of hospital readmissions with competing risks},
	volume = {31},
	issn = {0962-2802},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9931495/},
	doi = {10.1177/09622802221115879},
	abstract = {The 30-day hospital readmission rate has been used in provider profiling for evaluating inter-provider care coordination, medical cost effectiveness, and patient quality of life. Current profiling analyzes use logistic regression to model 30-day readmission as a binary outcome, but one disadvantage of this approach is that this outcome is strongly affected by competing risks (e.g., death). Thus, one, perhaps unintended, consequence is that if two facilities have the same rates of readmission, the one with the higher rate of competing risks will have the lower 30-day readmission rate. We propose a discrete time competing risk model wherein the cause-specific readmission hazard is used to assess provider-level effects. This approach takes account of the timing of events and focuses on the readmission rates which are of primary interest. The quality measure, then is a standardized readmission ratio, akin to a standardized mortality ratio. This measure is not systematically affected by the rate of competing risks. To facilitate the estimation and inference of a large number of provider effects, we develop an efficient Blockwise Inversion Newton algorithm, and a stabilized robust score test that overcomes the conservative nature of the classical robust score test. An application to dialysis patients demonstrates improved profiling, model fitting, and outlier detection over existing methods.},
	number = {11},
	urldate = {2024-02-28},
	journal = {Stat Methods Med Res},
	author = {Wu, Wenbo and He, Kevin and Shi, Xu and Schaubel, Douglas E and Kalbfleisch, John D},
	month = nov,
	year = {2022},
	pmid = {35899312},
	pmcid = {PMC9931495},
	keywords = {competing risks},
	pages = {2189--2200},
	file = {PubMed Central Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\7LG4FJP2\\Wu et al. - 2022 - Analysis of hospital readmissions with competing r.pdf:application/pdf},
}

@misc{noauthor_selection_nodate,
	title = {Selection {Bias} in {Observational} {Studies} of {Palliative} {Care}: {Lessons} {Learned} - {ClinicalKey}},
	url = {https://www.clinicalkey.com/#!/content/playContent/1-s2.0-S0885392420307399?returnurl=null&referrer=null},
	urldate = {2024-02-23},
	file = {Selection Bias in Observational Studies of Palliative Care\: Lessons Learned - ClinicalKey:C\:\\Users\\anbe6\\Zotero\\storage\\6L74D9CF\\www.clinicalkey.com.html:text/html},
}

@article{bujang_sample_2018,
	title = {Sample {Size} {Guidelines} for {Logistic} {Regression} from {Observational} {Studies} with {Large} {Population}: {Emphasis} on the {Accuracy} {Between} {Statistics} and {Parameters} {Based} on {Real} {Life} {Clinical} {Data}},
	volume = {25},
	issn = {1394-195X},
	shorttitle = {Sample {Size} {Guidelines} for {Logistic} {Regression} from {Observational} {Studies} with {Large} {Population}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6422534/},
	doi = {10.21315/mjms2018.25.4.12},
	abstract = {Background
Different study designs and population size may require different sample size for logistic regression. This study aims to propose sample size guidelines for logistic regression based on observational studies with large population.

Methods
We estimated the minimum sample size required based on evaluation from real clinical data to evaluate the accuracy between statistics derived and the actual parameters. Nagelkerke r-squared and coefficients derived were compared with their respective parameters.

Results
With a minimum sample size of 500, results showed that the differences between the sample estimates and the population was sufficiently small. Based on an audit from a medium size of population, the differences were within ± 0.5 for coefficients and ± 0.02 for Nagelkerke r-squared. Meanwhile for large population, the differences are within ± 1.0 for coefficients and ± 0.02 for Nagelkerke r-squared.

Conclusions
For observational studies with large population size that involve logistic regression in the analysis, taking a minimum sample size of 500 is necessary to derive the statistics that represent the parameters. The other recommended rules of thumb are EPV of 50 and formula; n = 100 + 50i where i refers to number of independent variables in the final model.},
	number = {4},
	urldate = {2024-02-22},
	journal = {Malays J Med Sci},
	author = {Bujang, Mohamad Adam and Sa’at, Nadiah and Sidik, Tg Mohd Ikhwan Tg Abu Bakar and Joo, Lim Chien},
	month = jul,
	year = {2018},
	pmid = {30914854},
	pmcid = {PMC6422534},
	keywords = {sample size, logistic regresssion},
	pages = {122--130},
	file = {PubMed Central Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\YR6RAC4W\\Bujang et al. - 2018 - Sample Size Guidelines for Logistic Regression fro.pdf:application/pdf},
}

@misc{noauthor_minimally_nodate,
	title = {Minimally important differences for interpreting {EORTC} {QLQ}-{C30} change scores over time: {A} synthesis across 21 clinical trials involving nine different cancer types - {ClinicalKey}},
	url = {https://www.clinicalkey.com/#!/content/playContent/1-s2.0-S0959804923002289?returnurl=https:%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0959804923002289%3Fshowall%3Dtrue&referrer=https:%2F%2Fpubmed.ncbi.nlm.nih.gov%2F},
	urldate = {2024-02-14},
	file = {Minimally important differences for interpreting E.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\52XVRZPQ\\Minimally important differences for interpreting E.pdf:application/pdf;Minimally important differences for interpreting EORTC QLQ-C30 change scores over time\: A synthesis across 21 clinical trials involving nine different cancer types - ClinicalKey:C\:\\Users\\anbe6\\Zotero\\storage\\LRBFQPTR\\www.clinicalkey.com.html:text/html},
}

@article{williams_costutility_2004,
	title = {Cost–utility analysis of osteopathy in primary care: results from a pragmatic randomized controlled trial},
	volume = {21},
	issn = {0263-2136},
	shorttitle = {Cost–utility analysis of osteopathy in primary care},
	url = {https://doi.org/10.1093/fampra/cmh612},
	doi = {10.1093/fampra/cmh612},
	abstract = {Background. Spinal pain is common and costly to health services and society. Management guidelines have encouraged primary care referral for spinal manipulation, but the evidence base is weak. More economic evaluations alongside pragmatic trials have been recommended.Objective. Our aim was to assess the cost–utility of a practice-based osteopathy clinic for subacute spinal pain.Methods. A cost–utility analysis was performed alongside a pragmatic single-centre randomized controlled trial in a primary care osteopathy clinic accepting referrals from 14 neighbouring practices in North West Wales. Patients with back pain of 2–12 weeks duration were randomly allocated to treatment with osteopathy plus usual GP care or usual GP care alone. Costs were measured from a National Health Service (NHS) perspective. All primary and secondary health care interventions recorded in GP notes were collected for the study period. We calculated quality adjusted life year (QALY) gains based on EQ-5D responses from patients in the trial, and then cost per QALY ratios. Confidence intervals (CIs) were estimated using non-parametric bootstrapping.Results. Osteopathy plus usual GP care was more effective but resulted in more health care costs than usual GP care alone. The point estimate of the incremental cost per QALY ratio was £3560 (80\% CI £542–£77 100). Sensitivity analysis examining spine-related costs alone and total costs excluding outliers resulted in lower cost per QALY ratios.Conclusion. A primary care osteopathy clinic may be a cost-effective addition to usual GP care, but this conclusion was subject to considerable random error. Rigorous multi-centre studies are needed to assess the generalizability of this approach.},
	number = {6},
	urldate = {2024-02-12},
	journal = {Family Practice},
	author = {Williams, Nefyn H and Edwards, Rhiannon T and Linck, Pat and Muntz, Rachel and Hibbs, Richard and Wilkinson, Clare and Russell, Ian and Russell, Daphne and Hounsome, Barry},
	month = dec,
	year = {2004},
	pages = {643--650},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\3GQFWUJD\\Williams et al. - 2004 - Cost–utility analysis of osteopathy in primary car.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\ZC7U9TB2\\508717.html:text/html},
}

@article{snyder_using_2013,
	title = {Using the {EORTC}-{QLQ}-{C30} in clinical practice for patient management: identifying scores requiring a clinician’s attention {\textbar} {Quality} of {Life} {Research}},
	url = {https://link.springer.com/article/10.1007/s11136-013-0387-8},
	doi = {10.1007/s11136-013-0387-8},
	abstract = {Purpose Patient-reported outcomes (PROs) are used increasingly for individual patient management. Identifying which PRO scores require a clinician’s attention is an ongoing challenge. Previous research used a needs assessment to identify EORTC-QLQ-C30 cutoff scores representing unmet needs. This analysis attempted to replicate the previous findings in a new and larger sample. Methods This analysis used data from 408 Japanese ambulatory breast cancer patients who completed the QLQ-C30 and Supportive Care Needs Survey-Short Form34 (SCNS-SF34). Applying the methods used previously, SCNS-SF34 item/domain scores were dichotomized as no versus some unmet need. We calculated area under the receiver operating characteristic curve (AUC) to evaluate QLQ-C30 scores’ ability to discriminate between patients with no versus some unmet need based on SCNS-SF34 items/domains. For QLQ-C30 domains with AUC C 0.70, we calculated the sensitivity, specificity, and predictive value of various cutoffs for identifying unmet needs. We hypothesized that compared to our original analysis, (1) the same six QLQ-C30 domains would have AUC C 0.70, (2) the same SCNS-SF34 items would be best discriminated by QLQ-C30 scores, and (3) the sensitivity and specificity of our original cutoff scores would be supported. Results The findings from our original analysis were supported. The same six domains with AUC C 0.70 in the original analysis had AUC C 0.70 in this new sample, and the same SCNS-SF34 item was best discriminated by QLQ-C30 scores. Cutoff scores were identified with sensitivity C0.84 and specificity C0.54. Conclusion Given these findings’ concordance with our previous analysis, these QLQ-C30 cutoffs could be implemented in clinical practice and their usefulness evaluated},
	urldate = {2024-02-12},
	journal = {Quality of Life Research},
	author = {Snyder, Claire and Blackford, Amanda and Okuyama, Toru and Akech, Tatsuo},
	year = {2013},
	keywords = {qol, EORTC-QLQ-C30},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\JEUYUWL6\\Using the EORTC-QLQ-C30 in clinical practice for p.pdf:application/pdf;Using the EORTC-QLQ-C30 in clinical practice for patient management\: identifying scores requiring a clinician’s attention | Quality of Life Research:C\:\\Users\\anbe6\\Zotero\\storage\\SK3PDA9V\\s11136-013-0387-8.html:text/html},
}

@article{yan_boin_2020,
	title = {{BOIN}: {An} {R} {Package} for {Designing} {Single}-{Agent} and {Drug}-{Combination} {Dose}-{Finding} {Trials} {Using} {Bayesian} {Optimal} {Interval} {Designs}},
	volume = {94},
	url = {https://www.jstatsoft.org/index.php/jss/article/view/v094i13},
	doi = {10.18637/JSS.V094.I13},
	abstract = {This article describes the R package BOIN, which implements a recently developed methodology for designing single-agent and drug-combination dose-finding clinical trials using Bayesian optimal interval designs (Liu and Yuan 2015; Yuan, Hess, Hilsenbeck, and Gilbert 2016). The BOIN designs are novel "model-assisted" phase I trial designs that can be implemented simply and transparently, similar to the 3 + 3 design, but yield excellent performance comparable to those of more complicated, model-based designs. The BOIN package provides tools for designing, conducting, and analyzing single-agent and drug-combination dose-finding trials.},
	journal = {Journal of Statistical Software},
	author = {Yan, Fangrong and Zhang, Liangcai and Zhou, Yanhong and Pan, Haitao and Liu, Suyu and Yuan, Ying},
	month = sep,
	year = {2020},
	note = {Publisher: American Statistical Association},
	keywords = {R, Bayesian, trial design},
	pages = {1--32},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\P2XSHFEM\\Yan et al. - 2020 - BOIN An R Package for Designing Single-Agent and .pdf:application/pdf},
}

@article{thabane_tutorial_2013,
	title = {A tutorial on sensitivity analyses in clinical trials: the what, why, when and how},
	volume = {13},
	issn = {1471-2288},
	shorttitle = {A tutorial on sensitivity analyses in clinical trials},
	url = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-13-92},
	doi = {10.1186/1471-2288-13-92},
	abstract = {Background: Sensitivity analyses play a crucial role in assessing the robustness of the findings or conclusions based on primary analyses of data in clinical trials. They are a critical way to assess the impact, effect or influence of key assumptions or variations—such as different methods of analysis, definitions of outcomes, protocol deviations, missing data, and outliers—on the overall conclusions of a study. The current paper is the second in a series of tutorial-type manuscripts intended to discuss and clarify aspects related to key methodological issues in the design and analysis of clinical trials. Discussion: In this paper we will provide a detailed exploration of the key aspects of sensitivity analyses including: 1) what sensitivity analyses are, why they are needed, and how often they are used in practice; 2) the different types of sensitivity analyses that one can do, with examples from the literature; 3) some frequently asked questions about sensitivity analyses; and 4) some suggestions on how to report the results of sensitivity analyses in clinical trials. Summary: When reporting on a clinical trial, we recommend including planned or posthoc sensitivity analyses, the corresponding rationale and results along with the discussion of the consequences of these analyses on the overall findings of the study.},
	language = {en},
	number = {1},
	urldate = {2024-02-12},
	journal = {BMC Med Res Methodol},
	author = {Thabane, Lehana and Mbuagbaw, Lawrence and Zhang, Shiyuan and Samaan, Zainab and Marcucci, Maura and Ye, Chenglin and Thabane, Marroon and Giangregorio, Lora and Dennis, Brittany and Kosa, Daisy and Debono, Victoria Borg and Dillenburg, Rejane and Fruci, Vincent and Bawor, Monica and Lee, Juneyoung and Wells, George and Goldsmith, Charles H},
	month = dec,
	year = {2013},
	keywords = {sensitivity analysis, missing data},
	pages = {92},
	file = {Thabane et al. - 2013 - A tutorial on sensitivity analyses in clinical tri.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\WF6T9E8Y\\Thabane et al. - 2013 - A tutorial on sensitivity analyses in clinical tri.pdf:application/pdf},
}

@article{troxel_statistical_1998,
	title = {Statistical analysis of quality of life with missing data in cancer clinical trials},
	volume = {17},
	copyright = {Copyright © 1998 John Wiley \& Sons, Ltd.},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-0258%2819980315/15%2917%3A5/7%3C653%3A%3AAID-SIM812%3E3.0.CO%3B2-M},
	doi = {10.1002/(SICI)1097-0258(19980315/15)17:5/7<653::AID-SIM812>3.0.CO;2-M},
	abstract = {We summarize issues that arise when considering quality of life (QOL) data in cancer clinical trials, especially those related to missing data. We describe different types of missing data mechanisms, and discuss ways of assessing and testing missing data mechanisms. A section on presentation of study design and results describes how graphical displays can effectively document the extent of the missing data problem, as well as describe its impact on interpretation of results. Finally, we describe several different statistical methods used to analyse repeated measures, with an emphasis on their properties and their ability to adequately handle different types of missing data mechanisms. We make recommendations as to the most appropriate methods, and suggest important directions for future research. © 1998 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {5-7},
	urldate = {2024-02-12},
	journal = {Statistics in Medicine},
	author = {Troxel, Andrea B. and Fairclough, Diane L. and Curran, Desmond and Hahn, Elizabeth A.},
	year = {1998},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291097-0258\%2819980315/15\%2917\%3A5/7\%3C653\%3A\%3AAID-SIM812\%3E3.0.CO\%3B2-M},
	keywords = {qol, missing data},
	pages = {653--666},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\P9CA53RN\\Troxel et al. - 1998 - Statistical analysis of quality of life with missi.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\4KAZKK8P\\7653AID-SIM8123.0.html:text/html},
}

@article{eisenhauer_new_2009,
	title = {New response evaluation criteria in solid tumours: {Revised} {RECIST} guideline (version 1.1)},
	volume = {45},
	issn = {09598049},
	shorttitle = {New response evaluation criteria in solid tumours},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0959804908008733},
	doi = {10.1016/j.ejca.2008.10.026},
	abstract = {Background: Assessment of the change in tumour burden is an important feature of the clinical evaluation of cancer therapeutics: both tumour shrinkage (objective response) and disease progression are useful endpoints in clinical trials. Since RECIST was published in 2000, many investigators, cooperative groups, industry and government authorities have adopted these criteria in the assessment of treatment outcomes. However, a number of questions and issues have arisen which have led to the development of a revised RECIST guideline (version 1.1). Evidence for changes, summarised in separate papers in this special issue, has come from assessment of a large data warehouse ({\textgreater}6500 patients), simulation studies and literature reviews.},
	language = {en},
	number = {2},
	urldate = {2024-02-09},
	journal = {European Journal of Cancer},
	author = {Eisenhauer, E.A. and Therasse, P. and Bogaerts, J. and Schwartz, L.H. and Sargent, D. and Ford, R. and Dancey, J. and Arbuck, S. and Gwyther, S. and Mooney, M. and Rubinstein, L. and Shankar, L. and Dodd, L. and Kaplan, R. and Lacombe, D. and Verweij, J.},
	month = jan,
	year = {2009},
	pages = {228--247},
	file = {Eisenhauer et al. - 2009 - New response evaluation criteria in solid tumours.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\G24468TW\\Eisenhauer et al. - 2009 - New response evaluation criteria in solid tumours.pdf:application/pdf},
}

@article{austin_missing_2021,
	title = {Missing {Data} in {Clinical} {Research}: {A} {Tutorial} on {Multiple} {Imputation}},
	volume = {37},
	issn = {0828-282X},
	shorttitle = {Missing {Data} in {Clinical} {Research}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8499698/},
	doi = {10.1016/j.cjca.2020.11.010},
	abstract = {Missing data is a common occurrence in clinical research. Missing data occurs when the value of the variables of interest are not measured or recorded for all subjects in the sample. Common approaches to addressing the presence of missing data include complete-case analyses, where subjects with missing data are excluded, and mean-value imputation, where missing values are replaced with the mean value of that variable in those subjects for whom it is not missing. However, in many settings, these approaches can lead to biased estimates of statistics (eg, of regression coefficients) and/or confidence intervals that are artificially narrow. Multiple imputation (MI) is a popular approach for addressing the presence of missing data. With MI, multiple plausible values of a given variable are imputed or filled in for each subject who has missing data for that variable. This results in the creation of multiple completed data sets. Identical statistical analyses are conducted in each of these complete data sets and the results are pooled across complete data sets. We provide an introduction to MI and discuss issues in its implementation, including developing the imputation model, how many imputed data sets to create, and addressing derived variables. We illustrate the application of MI through an analysis of data on patients hospitalised with heart failure. We focus on developing a model to estimate the probability of 1-year mortality in the presence of missing data. Statistical software code for conducting MI in R, SAS, and Stata are provided.},
	number = {9},
	urldate = {2024-02-08},
	journal = {Can J Cardiol},
	author = {Austin, Peter C. and White, Ian R. and Lee, Douglas S. and van Buuren, Stef},
	month = sep,
	year = {2021},
	pmid = {33276049},
	pmcid = {PMC8499698},
	keywords = {missing data},
	pages = {1322--1331},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\TPDPCFYY\\Austin et al. - 2021 - Missing Data in Clinical Research A Tutorial on M.pdf:application/pdf},
}

@article{jin_left_2023,
	title = {Left truncation in linked data: {A} practical guide to understanding left truncation and applying it using {SAS} and {R}},
	volume = {22},
	copyright = {© 2022 John Wiley \& Sons Ltd.},
	issn = {1539-1612},
	shorttitle = {Left truncation in linked data},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pst.2257},
	doi = {10.1002/pst.2257},
	abstract = {Time-to-event data such as time to death are broadly used in medical research and drug development to understand the efficacy of a therapeutic. For time-to-event data, right censoring (data only observed up to a certain point of time) is common and easy to recognize. Methods that use right censored data, such as the Kaplan–Meier estimator and the Cox proportional hazard model, are well established. Time-to-event data can also be left truncated, which arises when patients are excluded from the sample because their events occur before a specific milestone, potentially resulting in an immortal time bias. For example, in a study evaluating the association between biomarker status and overall survival, patients who did not live long enough to receive a genomic test were not observed in the study. Left truncation causes selection bias and often leads to an overestimate of survival time. In this tutorial, we used a nationwide electronic health record-derived de-identified database to demonstrate how to analyze left truncated and right censored data without bias using example code from SAS and R.},
	language = {en},
	number = {1},
	urldate = {2024-02-05},
	journal = {Pharmaceutical Statistics},
	author = {Jin, Yanling and Ton, Thanh G. N. and Incerti, Devin and Hu, Sylvia},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/pst.2257},
	keywords = {R, survival analysis, left truncation},
	pages = {194--204},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\AJ7LYSMZ\\Jin et al. - 2023 - Left truncation in linked data A practical guide .pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\99K9QESB\\pst.html:text/html},
}

@article{keene_why_2023,
	title = {Why estimands are needed to define treatment effects in clinical trials {\textbar} {BMC} {Medicine}},
	volume = {21},
	url = {https://link.springer.com/article/10.1186/s12916-023-02969-6},
	doi = {https://doi.org/10.1186/s12916-023-02969-6},
	abstract = {Background The estimand for a clinical trial is a precise definition of the treatment effect to be estimated. Traditionally, estimates of treatment effects are based on either an ITT analysis or a per-protocol analysis. However, there are important clinical questions which are not addressed by either of these analyses. For example, consider a trial where patients take a rescue medication. The ITT analysis includes data after use of rescue, while the per-protocol analysis excludes these patients altogether. Neither of these analyses addresses the important question of what the treatment effect would have been if patients did not take rescue medication. Main text Trial estimands provide a broader perspective compared to the limitations of ITT and per-protocol analysis. Trial treatment effects depend on how events occurring after treatment initiation such as use of alternative medication or discontinuation of the intervention are included in the definition. These events can be accounted for in different ways, depending on the clinical question of interest. Conclusion The estimand framework is an important step forward in improving the clarity and transparency of clinical trials. The centrality of estimands to clinical trials is currently not reflected in methods recommended by the Cochrane group or the CONSORT statement, the current standard for reporting clinical trials in medical journals. We encourage revisions to these guidelines.},
	number = {276},
	urldate = {2024-02-02},
	journal = {BMC Medicine},
	author = {Keene, Oliver N. and Lynggaard, Helle and Englert, Stefan and Lanius, Vivian and Wright, David},
	year = {2023},
	keywords = {estimands},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\8MQ9FI34\\Why estimands are needed to define treatment effec.pdf:application/pdf;Why estimands are needed to define treatment effects in clinical trials | BMC Medicine:C\:\\Users\\anbe6\\Zotero\\storage\\ZGKSVNBK\\s12916-023-02969-6.html:text/html},
}

@misc{allignol_cran_2023,
	title = {{CRAN} {Task} {View}: {Survival} {Analysis}},
	shorttitle = {{CRAN} {Task} {View}},
	url = {https://CRAN.R-project.org/view=Survival},
	abstract = {Survival analysis, also called event history analysis in social science, or reliability analysis in engineering, deals with time until occurrence of an event of interest. However, this failure time may not be observed within the relevant time period, producing so-called censored observations.},
	urldate = {2024-02-05},
	author = {Allignol, Arthur and Latouche, Aurelien},
	month = sep,
	year = {2023},
	note = {Publisher: Comprehensive R Archive Network (CRAN)},
	keywords = {R, survival analysis},
	file = {Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\AVT8H57E\\Survival.html:text/html},
}

@article{brilleman_simsurv_nodate,
	title = {simsurv: {A} {Package} for {Simulating} {Simple} or {Complex} {Survival} {Data}},
	language = {en},
	journal = {Event history analysis},
	author = {Brilleman, Sam and Wolfe, Rory and Moreno-Betancur, Margarita and Crowther, Michael J},
	keywords = {R, simulation},
	file = {Brilleman et al. - simsurv A Package for Simulating Simple or Comple.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\KLQSJTJS\\Brilleman et al. - simsurv A Package for Simulating Simple or Comple.pdf:application/pdf},
}

@article{heinze_variable_2018,
	title = {Variable selection – {A} review and recommendations for the practicing statistician},
	volume = {60},
	issn = {0323-3847},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5969114/},
	doi = {10.1002/bimj.201700067},
	abstract = {Statistical models support medical research by facilitating individualized outcome prognostication conditional on independent variables or by estimating effects of risk factors adjusted for covariates. Theory of statistical models is well‐established if the set of independent variables to consider is fixed and small. Hence, we can assume that effect estimates are unbiased and the usual methods for confidence interval estimation are valid. In routine work, however, it is not known a priori which covariates should be included in a model, and often we are confronted with the number of candidate variables in the range 10–30. This number is often too large to be considered in a statistical model. We provide an overview of various available variable selection methods that are based on significance or information criteria, penalized likelihood, the change‐in‐estimate criterion, background knowledge, or combinations thereof. These methods were usually developed in the context of a linear regression model and then transferred to more generalized linear models or models for censored survival data. Variable selection, in particular if used in explanatory modeling where effect estimates are of central interest, can compromise stability of a final model, unbiasedness of regression coefficients, and validity of p‐values or confidence intervals. Therefore, we give pragmatic recommendations for the practicing statistician on application of variable selection methods in general (low‐dimensional) modeling problems and on performing stability investigations and inference. We also propose some quantities based on resampling the entire variable selection process to be routinely reported by software packages offering automated variable selection algorithms.},
	number = {3},
	urldate = {2024-02-02},
	journal = {Biom J},
	author = {Heinze, Georg and Wallisch, Christine and Dunkler, Daniela},
	month = may,
	year = {2018},
	pmid = {29292533},
	pmcid = {PMC5969114},
	keywords = {variable selection},
	pages = {431--449},
	file = {PubMed Central Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\VYGBQCAX\\Heinze et al. - 2018 - Variable selection – A review and recommendations .pdf:application/pdf},
}

@article{murray_causal_2021,
	title = {Causal survival analysis: {A} guide to estimating intention-to-treat and per-protocol effects from randomized clinical trials with non-adherence},
	volume = {2},
	issn = {2632-0843},
	shorttitle = {Causal survival analysis},
	url = {https://doi.org/10.1177/2632084320961043},
	doi = {10.1177/2632084320961043},
	abstract = {When reporting results from randomized experiments, researchers often choose to present a per-protocol effect in addition to an intention-to-treat effect. However, these per-protocol effects are often described retrospectively, for example, comparing outcomes among individuals who adhered to their assigned treatment strategy throughout the study. This retrospective definition of a per-protocol effect is often confounded and cannot be interpreted causally because it encounters treatment-confounder feedback loops, where past confounders affect future treatment, and current treatment affects future confounders. Per-protocol effects estimated using this method are highly susceptible to the placebo paradox, also called the “healthy adherers” bias, where individuals who adhere to placebo appear to have better survival than those who don’t. This result is generally not due to a benefit of placebo, but rather is most often the result of uncontrolled confounding. Here, we aim to provide an overview to causal inference for survival outcomes with time-varying exposures for static interventions using inverse probability weighting. The basic concepts described here can also apply to other types of exposure strategies, although these may require additional design or analytic considerations. We provide a workshop guide with solutions manual, fully reproducible R, SAS, and Stata code, and a simulated dataset on a GitHub repository for the reader to explore.},
	language = {en},
	number = {1},
	urldate = {2024-02-02},
	journal = {Research Methods in Medicine \& Health Sciences},
	author = {Murray, Eleanor J and Caniglia, Ellen C and Petito, Lucia C},
	month = jan,
	year = {2021},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {causal inference, survival analysis, intention to treat, per protocol},
	pages = {39--49},
	file = {SAGE PDF Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\UZB2R6CY\\Murray et al. - 2021 - Causal survival analysis A guide to estimating in.pdf:application/pdf},
}

@article{demets_interim_1994,
	title = {Interim analysis: the alpha spending function approach},
	volume = {13},
	issn = {0277-6715},
	shorttitle = {Interim analysis},
	doi = {10.1002/sim.4780131308},
	abstract = {Interim analysis of accumulating data in a clinical trial is now an established practice for ethical and scientific reasons. Repeatedly testing interim data can inflate false positive error rates if not handled appropriately. Group sequential methods are a commonly used frequentist approach to control this error rate. Motivated by experience of clinical trials, the alpha spending function is one way to implement group sequential boundaries that control the type I error rate while allowing flexibility in how many interim analyses are to be conducted and at what times. In this paper, we review the alpha spending function approach, and detail its applicability to a variety of commonly used statistical procedures, including survival and longitudinal methods.},
	language = {eng},
	number = {13-14},
	journal = {Stat Med},
	author = {DeMets, D. L. and Lan, K. K.},
	month = jul,
	year = {1994},
	pmid = {7973215},
	keywords = {trial design, group sequential},
	pages = {1341--1352; discussion 1353--1356},
}

@article{demets_alpha_1995,
	title = {The alpha spending function approach to interim data analyses},
	volume = {75},
	issn = {0927-3042},
	doi = {10.1007/978-1-4615-2009-2_1},
	language = {eng},
	journal = {Cancer Treat Res},
	author = {DeMets, D. L. and Lan, G.},
	year = {1995},
	pmid = {7640159},
	keywords = {trial design, group sequential},
	pages = {1--27},
}

@article{kahan_estimands_2024,
	title = {The estimands framework: a primer on the {ICH} {E9}({R1}) addendum},
	volume = {384},
	copyright = {© Author(s) (or their employer(s)) 2019. Re-use permitted under CC BY. No commercial re-use. See rights and permissions. Published by BMJ.. http://creativecommons.org/licenses/by/4.0/This is an Open Access article distributed in accordance with the terms of the Creative Commons Attribution (CC BY 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/.},
	issn = {1756-1833},
	shorttitle = {The estimands framework},
	url = {https://www.bmj.com/content/384/bmj-2023-076316},
	doi = {10.1136/bmj-2023-076316},
	abstract = {{\textless}p{\textgreater}Estimands can be used in studies of healthcare interventions to clarify the interpretation of treatment effects. The addendum to the ICH E9 harmonised guideline on statistical principles for clinical trials (ICH E9(R1)) describes a framework for using estimands as part of a study. This paper provides an overview of the estimands framework, as outlined in the addendum, with the aim of explaining why estimands are beneficial; clarifying the terminology being used; and providing practical guidance on using estimands to decide the appropriate study design, data collection, and estimation methods. This article illustrates how to use the estimands framework by applying it to an ongoing trial in emergency bowel surgery. Estimands can be a useful way of clarifying the exact research question being evaluated in a study, both to avoid misinterpretation and to ensure that study methods are aligned to the overall study objectives.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2024-02-02},
	journal = {BMJ},
	author = {Kahan, Brennan C. and Hindley, Joanna and Edwards, Mark and Cro, Suzie and Morris, Tim P.},
	month = jan,
	year = {2024},
	pmid = {38262663},
	note = {Publisher: British Medical Journal Publishing Group
Section: Research Methods \&amp; Reporting},
	keywords = {estimands},
	pages = {e076316},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\XYYQ8IZC\\Kahan et al. - 2024 - The estimands framework a primer on the ICH E9(R1.pdf:application/pdf},
}

@article{smith_interpreting_2021,
	title = {Interpreting the {Results} of {Intention}-to-{Treat}, {Per}-{Protocol}, and {As}-{Treated} {Analyses} of {Clinical} {Trials}},
	volume = {326},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.2021.2825},
	doi = {10.1001/jama.2021.2825},
	abstract = {Nonadherence in a randomized clinical trial (RCT) occurs when study participants do not follow the randomly assigned treatment protocol. Reasons for nonadherence may include the study participant not taking trial medications, crossing over to the other intervention being studied, or accessing treatment outside of the trial. Nonadherence also may occur when the clinician is unable to complete the assigned therapy (eg, a surgical procedure) as intended.The CABANA clinical trial published in JAMA by Packer et al was difficult to interpret because of nonadherence with the treatment protocol that resulted from substantial crossover between groups. In this trial, patients with atrial fibrillation were randomized to either undergo catheter ablation or receive conventional medical therapy. Of the 1108 participants randomized to ablation, 102 (9\%) did not receive the procedure. Of the 1096 patients randomized to drug therapy, 301 (27\%) underwent ablation during the follow-up period, resulting in nonadherence to assigned treatment in both groups of the study. Interpretation of the effect of catheter ablation on atrial fibrillation differed based on alternate ways of analyzing the trials results. Intention-to-treat (ITT), per-protocol (PP), and as-treated (AT) approaches to analysis differ in how the included patient population and treatment assignments are defined, with important implications for interpretation of treatment effects in clinical trials.},
	number = {5},
	urldate = {2024-02-02},
	journal = {JAMA},
	author = {Smith, Valerie A. and Coffman, Cynthia J. and Hudgens, Michael G.},
	month = aug,
	year = {2021},
	keywords = {intention to treat, per protocol},
	pages = {433--434},
	file = {Accepted Version:C\:\\Users\\anbe6\\Zotero\\storage\\TTSA5S5N\\Smith et al. - 2021 - Interpreting the Results of Intention-to-Treat, Pe.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\EA9QMJTT\\2782658.html:text/html},
}

@article{tripepi_intention_2020,
	title = {Intention to treat and per protocol analysis in clinical trials},
	volume = {25},
	issn = {1440-1797},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/nep.13709},
	doi = {10.1111/nep.13709},
	abstract = {In clinical epidemiology, experimental studies usually take the form of randomized controlled clinical trials (RCTs). The data analysis of an RCT can be performed by using two complementary strategies, that is according to the intention to treat (ITT) principle and the per protocol (PP) analysis. By using the ITT approach, investigators aim to assess the effect of assigning a drug whereas by adopting the PP analysis, researchers investigate the effect of receiving the assigned treatment, as specified in the protocol. Both ITT and PP analyses are essentially valid but they have different scopes and interpretations dependent on the context.},
	language = {en},
	number = {7},
	urldate = {2024-02-02},
	journal = {Nephrology},
	author = {Tripepi, Giovanni and Chesnaye, Nicholas C. and Dekker, Friedo W. and Zoccali, Carmine and Jager, Kitty J.},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/nep.13709},
	keywords = {intention to treat},
	pages = {513--517},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\WN5UHAKK\\Tripepi et al. - 2020 - Intention to treat and per protocol analysis in cl.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\LEXH7A7K\\nep.html:text/html},
}

@incollection{noauthor_international_2008,
	title = {International {Conference} on {Harmonisation} of {Technical} {Requirements} for {Registration} of {Pharmaceuticals} for {Human} {Use} ( {ICH} )},
	booktitle = {Wiley {Encyclopedia} of {Clinical} {Trials}},
	year = {2008},
	doi = {10.1002/9780471462422.eoct457},
	keywords = {ICH},
	pages = {1--1},
}

@article{zhu_group_2011,
	title = {Group sequential methods and software applications},
	volume = {65},
	doi = {10.1198/tast.2011.10213},
	abstract = {Group sequential methods are commonly used in clinical trials as they allow valid inference when statistical tests are performed on accumulating data from ongoing trials. Several software applications have become available to meet the computational need for group sequential trial design and monitoring. This article summarizes four software applications: EAST® v5.2, ADDPLAN®v5.0, the gsDesign package v2.3 in R©, and the SEQDESIGN and SEQTEST procedures in SAS® v9.2. The capabilities and special features of the software applications are illustrated and compared through practical examples. Supplementary materials for this article are available online. © 2011 American Statistical Association.},
	number = {2},
	journal = {American Statistician},
	author = {Zhu, Li and Ni, Liyun and Yao, Bin},
	month = may,
	year = {2011},
	keywords = {trial design, group sequential},
	pages = {127--135},
}

@article{yuan_bayesian_2016,
	title = {Bayesian optimal interval design: {A} simple and well-performing design for phase i oncology trials},
	volume = {22},
	doi = {10.1158/1078-0432.CCR-16-0592},
	abstract = {Despite more than two decades of publications that offer more innovative model-based designs, the classical 3 +3 design remains the most dominant phase I trial design in practice. In this article, we introduce a new trial design, the Bayesian optimal interval (BOIN) design. The BOIN design is easy to implement in a way similar to the 3 + 3 design, but is more flexible for choosing the target toxicity rate and cohort size and yields a substantially better performance that is comparable with that of more complex model-based designs. The BOIN design contains the 3 + 3 design and the accelerated titration design as special cases, thus linking itto established phase I approaches. A numerical study shows that the BOIN design generally outperforms the 3 + 3 design and the modified toxicity probability interval (mTPI) design. The BOIN design is more likely than the 3 + 3 design to correctly select the MTD and allocate more patients to the MTD. Compared with the mTPI design, the BOIN design has a substantially lower risk of overdosing patients and generally a higher probability of correctly selecting the MTD. User-friendly software is freely available to facilitate the application of the BOIN design.},
	number = {17},
	journal = {Clinical Cancer Research},
	author = {Yuan, Ying and Hess, Kenneth R. and Hilsenbeck, Susan G. and Gilbert, Mark R.},
	month = sep,
	year = {2016},
	note = {Publisher: American Association for Cancer Research Inc.},
	keywords = {Bayesian, phase I},
	pages = {4291--4301},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\84EWYRMM\\Yuan et al. - 2016 - Bayesian optimal interval design A simple and wel.pdf:application/pdf},
}

@article{yang_fear_2023,
	title = {Fear of recurrence in postoperative lung cancer patients: {Trajectories}, influencing factors and impacts on quality of life},
	doi = {10.1111/jocn.16922},
	abstract = {Aims: To investigate the trajectory, influencing factors and dynamic relationships between fear of cancer recurrence (FCR) and quality of life (QOL) in lung cancer patients. Design: Prospective longitudinal study. Methods: Longitudinal data from 310 lung cancer patients across three hospitals in China were assessed at 1, 3, 6 and 12 months postoperatively (T1–T4). Descriptive statistics characterised patient demographics, clinical characteristics, levels of FCR and QOL. A linear mixed-effects model was employed to analyse FCR trajectories, identify influencing factors on these trajectories, and predict the impact of FCR on QOL. Results: FCR changed significantly over time, with a slight decrease during T1–T2, an increase at T3 and gradual decline at T4. Higher fear levels were associated with female sex, suburban or rural residency, being a family breadwinner, presence of comorbidities and negative coping behaviours, and low family resilience. QOL negatively correlated with FCR, and FCR predicted lower QOL. Conclusions: At 3 and 6 months postoperatively, lung cancer patients, especially women, suburban or rural residents, family breadwinners, those with comorbidities, negative coping behaviours and low family resilience, reported high levels of FCR. Healthcare providers should pay special attention to lung cancer patients especially during the period of 3–6 months post-surgery and offer tailored interventions to improve their QOL. Implications for the Profession and Patient Care: Understanding the FCR trajectories, its influencing factors and its negative impacts on QOL can guide the development of targeted interventions to reduce fear and enhance well-being in patients with cancer. Impact: Identifying the trajectories and influencing factors of fear of lung cancer recurrence in patients at different time points informs future research on targeted interventions to improve QOL. Reporting Method: The study adhered to the guidelines outlined in the Statement on Reporting Observational Longitudinal Research.},
	journal = {Journal of Clinical Nursing},
	author = {Yang, Xiaoyan and Li, Yonglin and Lin, Jialing and Zheng, Jianqing and Xiao, Huimin and Chen, Weiti and Huang, Feifei},
	year = {2023},
	note = {Publisher: John Wiley and Sons Inc},
	keywords = {R, lung cancer},
}

@article{yamamoto_pragmatic_2017,
	title = {A pragmatic method for transforming clinical research data from the research electronic data capture “{REDCap}” to {Clinical} {Data} {Interchange} {Standards} {Consortium} ({CDISC}) {Study} {Data} {Tabulation} {Model} ({SDTM}): {Development} and evaluation of {REDCap2SDTM}},
	volume = {70},
	doi = {10.1016/j.jbi.2017.05.003},
	abstract = {The Clinical Data Interchange Standards Consortium (CDISC) Study Data Tabulation Model (SDTM) can be used for new drug application studies as well as secondarily for creating a clinical research data warehouse to leverage clinical research study data across studies conducted within the same disease area. However, currently not all clinical research uses Clinical Data Acquisition Standards Harmonization (CDASH) beginning in the set-up phase of the study. Once already initiated, clinical studies that have not utilized CDASH are difficult to map in the SDTM format. In addition, most electronic data capture (EDC) systems are not equipped to export data in SDTM format; therefore, in many cases, statistical software is used to generate SDTM datasets from accumulated clinical data. In order to facilitate efficient secondary use of accumulated clinical research data using SDTM, it is necessary to develop a new tool to enable mapping of information for SDTM, even during or after the clinical research. REDCap is an EDC system developed by Vanderbilt University and is used globally by over 2100 institutions across 108 countries. In this study, we developed a simulated clinical trial to evaluate a tool called REDCap2SDTM that maps information in the Field Annotation of REDCap to SDTM and executes data conversion, including when data must be pivoted to accommodate the SDTM format, dynamically, by parsing the mapping information using R. We confirmed that generating SDTM data and the define.xml file from REDCap using REDCap2SDTM was possible. Conventionally, generation of SDTM data and the define.xml file from EDC systems requires the creation of individual programs for each clinical study. However, our proposed method can be used to generate this data and file dynamically without programming because it only involves entering the mapping information into the Field Annotation, and additional data into specific files. Our proposed method is adaptable not only to new drug application studies but also to all types of research, including observational and public health studies. Our method is also adaptable to clinical data collected with CDASH at the beginning of a study in non-standard format. We believe that this tool will reduce the workload of new drug application studies and will support data sharing and reuse of clinical research data in academia.},
	journal = {Journal of Biomedical Informatics},
	author = {Yamamoto, Keiichi and Ota, Keiko and Akiya, Ippei and Shintani, Ayumi},
	month = jun,
	year = {2017},
	note = {Publisher: Academic Press Inc.},
	keywords = {REDCap},
	pages = {65--76},
}

@article{willan_value_2005,
	title = {The value of information and optimal clinical trial design},
	volume = {24},
	url = {https://onlinelibrary.wiley.com/doi/full/10.1002/sim.2069},
	doi = {10.1002/SIM.2069},
	abstract = {Traditional sample size calculations for randomized clinical trials depend on somewhat arbitrarily chosen factors, such as type I and II errors. Type I error, the probability of rejecting the null hypothesis of no difference when it is true, is most often set to 0.05, regardless of the cost of such an error. In addition, the traditional use of 0.2 for the type II error means that the money and effort spent on the trial will be wasted 20 per cent of the time, even when the true treatment difference is equal to the smallest clinically important one and, again, will not reflect the cost of making such an error. An effectiveness trial (otherwise known as a pragmatic trial or management trial) is essentially an effort to inform decision-making, i.e. should treatment be adopted over standard? As such, a decision theoretic approach will lead to an optimal sample size determination. Using incremental net benefit and the theory of the expected value of information, and taking a societal perspective, it is shown how to determine the sample size that maximizes the difference between the cost of doing the trial and the value of the information gained from the results. The methods are illustrated using examples from oncology and obstetrics. Copyright © 2005 John Wiley \& Sons, Ltd.},
	number = {12},
	journal = {Statistics in Medicine},
	author = {Willan, Andrew R and Pinto, Eleanor M},
	month = jun,
	year = {2005},
	note = {Publisher: John Wiley \& Sons, Ltd},
	keywords = {trial design},
	pages = {1791--1806},
}

@article{wharton_estimating_2021,
	title = {Estimating and reporting treatment effects in clinical trials for weight management: using estimands to interpret effects of intercurrent events and missing data},
	volume = {45},
	url = {https://doi.org/10.1038/s41366-020-00733-x},
	doi = {10.1038/s41366-020-00733-x},
	abstract = {In the approval process for new weight management therapies, regulators typically require estimates of effect size. Usually, as with other drug evaluations, the placebo-adjusted treatment effect (i.e., the difference between weight losses with pharmacotherapy and placebo, when given as an adjunct to lifestyle intervention) is provided from data in randomized clinical trials (RCTs). At first glance, this may seem appropriate and straightforward. However, weight loss is not a simple direct drug effect, but is also mediated by other factors such as changes in diet and physical activity. Interpreting observed differences between treatment arms in weight management RCTs can be challenging; intercurrent events that occur after treatment initiation may affect the interpretation of results at the end of treatment. Utilizing estimands helps to address these uncertainties and improve transparency in clinical trial reporting by better matching the treatment-effect estimates to the scientific and/or clinical questions of interest. Estimands aim to provide an indication of trial outcomes that might be expected in the same patients under different conditions. This article reviews how intercurrent events during weight management trials can influence placebo-adjusted treatment effects, depending on how they are accounted for and how missing data are handled. The most appropriate method for statistical analysis is also discussed, including assessment of the last observation carried forward approach, and more recent methods, such as multiple imputation and mixed models for repeated measures. The use of each of these approaches, and that of estimands, is discussed in the context of the SCALE phase 3a and 3b RCTs evaluating the effect of liraglutide 3.0 mg for the treatment of obesity.},
	journal = {International Journal of Obesity},
	author = {Wharton, Sean and Astrup, Arne and Endahl, Lars and Michael, • and Lean, E J and Satylganova, Altynai and Skovgaard, Dorthe and Wadden, Thomas A and John, • and Wilding, P H},
	year = {2021},
	keywords = {estimands, trial design},
	pages = {923--933},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\XVSI4XIP\\Wharton et al. - 2021 - Estimating and reporting treatment effects in clin.pdf:application/pdf},
}

@article{us_fda_e6r2_2018,
	title = {E6({R2}) {Good} {Clinical} {Practice}: {Integrated} {Addendum} to {ICH} {E6}({R1}) {Guidance} for {Industry} {Procedural} {Contains} {Nonbinding} {Recommendations}},
	issn = {8855433784},
	url = {https://www.fda.gov/drugs/guidance-compliance-regulatory-information/guidances-drugsand/or},
	author = {{US FDA}},
	year = {2018},
	keywords = {ICH, GCP},
}

@article{scrucca_regression_2010,
	title = {Regression modeling of competing risk using {R}: an in depth guide for clinicians},
	volume = {45},
	url = {https://www.nature.com/articles/bmt2009359},
	doi = {10.1038/bmt.2009.359},
	abstract = {We describe how to conduct a regression analysis for competing risks data. The use of an add-on package for the R statistical software is described, which allows for the estimation of the semiparametric proportional hazards model for the subdistribution of a competing risk analysis as proposed by Fine and Gray. J Am Stat Assoc 1999; 94: 496–509.},
	number = {9},
	journal = {Bone Marrow Transplantation 2010 45:9},
	author = {Scrucca, L. and Santucci, A. and Aversa, F.},
	month = jan,
	year = {2010},
	note = {Publisher: Nature Publishing Group},
	keywords = {R, competing risks},
	pages = {1388--1395},
}

@article{reinke_randomized_2022,
	title = {A {Randomized} {Trial} of a {Nurse}-{Led} {Palliative} {Care} {Intervention} for {Patients} with {Newly} {Diagnosed} {Lung} {Cancer}},
	volume = {25},
	doi = {10.1089/jpm.2022.0008},
	abstract = {Background: Specialist palliative care improves quality of life (QOL), symptom burden, and may prolong survival among patients with advanced lung cancer. Previous trials focused on advanced disease, and less is known about patients across a broad range of stages. Objective: We sought to assess the effect of a nurse-led telephone-based primary palliative care intervention that focused on patients across a broad range of stages. Design, Setting, and Participants: We conducted a multisite randomized controlled trial in the United States involving patients diagnosed within two months with any stage or histology of lung cancer to compare the effects of a telephone-based palliative care intervention delivered by registered nurses trained in primary palliative care versus usual care. Main Outcomes and Measures: The primary outcome was the Functional Assessment of Cancer Therapy-Lung Scale Total Outcome Index (FACT-L TOI), which measures QOL and symptoms. We estimated having 80\% power to detect a 5-point change from baseline to three months. Secondary outcome was a change in satisfaction of care, measured by the FAMCARE-P13. Results: A total of 151 patients were enrolled over 30 months. Patients were, on average, male (98\%), age 70 years, White (85\%), and 36\% diagnosed with stage I-II, and 64\% had stage III-IV. In comparison to usual care, patients in the nurse-led intervention did not report improvement in QOL from baseline to three months follow-up or demonstrate differences in treatment effect by site or cancer stage: FACT-L TOI 1.03 (95\% confidence interval [CI]: -3.98 to 6.04). Satisfaction with care did not significantly improve: 0.66 (95\% CI: -2.01 to 3.33). Conclusions: Among patients with newly diagnosed lung cancer, a nurse-led, primary palliative care intervention did not significantly improve QOL, symptom burden, or satisfaction of care. In contrast to several clinical trials demonstrating the effectiveness of delivering specialty palliative care with disease-modifying treatments on QOL among patients with advanced lung cancer, this intervention did not significantly improve QOL among patients with any stage lung cancer. Future research should identify which specific components of primary palliative care improve outcomes for patients newly diagnosed with lung cancer.},
	number = {11},
	journal = {Journal of Palliative Medicine},
	author = {Reinke, Lynn F. and Sullivan, Donald R. and Slatore, Christopher and Dransfield, Mark T. and Ruedebusch, Susan and Smith, Patti and Rise, Peter J. and Tartaglione, Erica V. and Vig, Elizabeth K. and Au, David H.},
	month = nov,
	year = {2022},
	note = {Publisher: Mary Ann Liebert Inc.},
	keywords = {palliative care, lung cancer},
	pages = {1668--1676},
}

@article{prentice_analysis_1978,
	title = {The analysis of failure times in the presence of competing risks},
	volume = {34},
	url = {https://www.jstor.org/stable/2530374},
	abstract = {Distinct problems in the analysis of failure times with competing causes of failure include the estimation of treatment or exposure effects on specific failure types, the study of interrelations among failure types, and the estimation of failure rates for some causes given the removal of certain other failure types. The usual formation of these problems is in terms of conceptual or latent failure times for each failure type. This approach is criticized on the basis of unwarranted assumptions, lack of physical interpretation and identifiability problems. An alternative approach utilizing cause-specific hazard functions for observable quantities, including time-dependent covariates, is proposed. Cause-specific hazard functions are shown to be the basic estimable quantities in the competing risks framework. A method, involving the estimation of parameters that relate time-dependent risk indicators for some causes to cause-specific hazard functions for other causes, is proposed for the study of interrelations among failure types. Further, it is argued that the problem of estimation of failure rates under the removal of certain causes is not well posed until a mechanism for cause removal is specified. Following such a specification, one will sometimes be in a position to make sensible extrapolations from available data to situations involving cause removal. A clinical program in bone marrow transplantation for leukemia provides a setting for discussion and illustration of each of these ideas. Failure due to censoring in a survivorship study leads to further discussion.},
	number = {4},
	journal = {Biometrics},
	author = {Prentice, R.L and Kalbfleisch, JD},
	year = {1978},
	keywords = {competing risks},
	pages = {541--554},
}

@article{park_sample_2023,
	title = {Sample size calculation in clinical trial using {R}},
	volume = {26},
	url = {https://www.e-jmis.org/journal/view.html?doi=10.7602/jmis.2023.26.1.9},
	doi = {10.7602/jmis.2023.26.1.9},
	abstract = {Since the era of evidence-based medicine, it has become a matter of course to use statistics to create objective evidence in clinical research. As an extension of this, it has become essential in clinical research to calculate the correct sample size to demonstrate a clinically significant difference before starting the study. Also, because sample size calculation methods vary from study design to study design, there is no formula for sample size calculation that applies to all designs. It is very important for us to understand this. In this review, each sample size calculation method suitable for various study designs was introduced using the R program (R Foundation for Statistical Computing). In order for clinical researchers to directly utilize it according to future research, we presented practice codes, output results, and interpretation of results for each situation.},
	number = {1},
	journal = {Journal of Minimally Invasive Surgery},
	author = {Park, Suyeon and Kim, Yeong-Haw and Bang, Hae In and Park, Youngho},
	month = mar,
	year = {2023},
	note = {Publisher: The Korean Society of Endo-Laparoscopic \& Robotic Surgery},
	keywords = {R, sample size, trial design},
	pages = {9--18},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\XKSUITKS\\Park et al. - 2023 - Sample size calculation in clinical trial using R.pdf:application/pdf},
}

@article{lundberg_what_2021,
	title = {What {Is} {Your} {Estimand}? {Defining} the {Target} {Quantity} {Connects} {Statistical} {Evidence} to {Theory}},
	volume = {86},
	url = {https://journals.sagepub.com/doi/full/10.1177/00031224211004187?casa_token=F9FRMC3qANIAAAAA%3A5dNDSw6Dt6Txt44PjLQbQKU2jC_XHuhEW6fGW0ybsxmqjIlU0jOR0bIcHctdG_Ma-WfppNciD8Y},
	doi = {10.1177/00031224211004187/ASSET/IMAGES/LARGE/10.1177_00031224211004187-FIG4.JPEG},
	abstract = {We make only one point in this article. Every quantitative study must be able to answer the question: what is your estimand? The estimand is the target quantity—the purpose of the statistical analysis. Much attention is already placed on how to do estimation; a similar degree of care should be given to defining the thing we are estimating. We advocate that authors state the central quantity of each analysis—the theoretical estimand—in precise terms that exist outside of any statistical model. In our framework, researchers do three things: (1) set a theoretical estimand, clearly connecting this quantity to theory; (2) link to an empirical estimand, which is informative about the theoretical estimand under some identification assumptions; and (3) learn from data. Adding precise estimands to research practice expands the space of theoretical questions, clarifies how evidence can speak to those questions, and unlocks new tools for estimation. By grounding all three steps in a precise statement of the target quantity, our framework connects statistical evidence to theory.},
	number = {3},
	journal = {American Sociological Review},
	author = {Lundberg, Ian and Johnson, Rebecca and Stewart, Brandon M},
	month = jun,
	year = {2021},
	note = {Publisher: SAGE Publications Ltd},
	keywords = {estimands},
	pages = {532--565},
}

@article{kahan_estimands_2021,
	title = {Estimands in published protocols of randomised trials: urgent improvement needed},
	volume = {22},
	url = {https://link.springer.com/articles/10.1186/s13063-021-05644-4},
	doi = {10.1186/S13063-021-05644-4/TABLES/7},
	abstract = {Background: An estimand is a precise description of the treatment effect to be estimated from a trial (the question) and is distinct from the methods of statistical analysis (how the question is to be answered). The potential use of estimands to improve trial research and reporting has been underpinned by the recent publication of the ICH E9(R1) Addendum on the use of estimands in clinical trials in 2019. We set out to assess how well estimands are described in published trial protocols. Methods: We reviewed 50 trial protocols published in October 2020 in Trials and BMJ Open. For each protocol, we determined whether the estimand for the primary outcome was explicitly stated, not stated but inferable (i.e. could be constructed from the information given), or not inferable. Results: None of the 50 trials explicitly described the estimand for the primary outcome, and in 74\% of trials, it was impossible to infer the estimand from the information included in the protocol. The population attribute of the estimand could not be inferred in 36\% of trials, the treatment condition attribute in 20\%, the population-level summary measure in 34\%, and the handling of intercurrent events in 60\% (the strategy for handling non-adherence was not inferable in 32\% of protocols, and the strategy for handling mortality was not inferable in 80\% of the protocols for which it was applicable). Conversely, the outcome attribute was stated for all trials. In 28\% of trials, three or more of the five estimand attributes could not be inferred. Conclusions: The description of estimands in published trial protocols is poor, and in most trials, it is impossible to understand exactly what treatment effect is being estimated. Given the utility of estimands to improve clinical research and reporting, this urgently needs to change.},
	number = {1},
	journal = {Trials},
	author = {Kahan, Brennan C and Morris, Tim P and White, Ian R and Carpenter, James and Cro, Suzie},
	month = dec,
	year = {2021},
	note = {Publisher: BioMed Central Ltd},
	keywords = {estimands},
	pages = {1--10},
}

@inproceedings{international_conference_on_harmonisation_of_technical_requirements_for_registration_of_pharmaceuticals_for_human_use_international_2016,
	title = {{INTERNATIONAL} {CONFERENCE} {ON} {HARMONISATION} {OF} {TECHNICAL} {REQUIREMENTS} {FOR} {REGISTRATION} {OF} {PHARMACEUTICALS} {FOR} {HUMAN} {USE} {ICH} {HARMONISED} {GUIDELINE} {INTEGRATED} {ADDENDUM} {TO} {ICH} {E6}({R1}): {GUIDELINE} {FOR} {GOOD} {CLINICAL} {PRACTICE} {E6}({R2})},
	abstract = {Good Clinical Practice (GCP) is an international ethical and scientific quality standard for designing, conducting, recording and reporting trials that involve the participation of human subjects. Compliance with this standard provides public assurance that the rights, safety and well-being of trial subjects are protected, consistent with the principles that have their origin in the Declaration of Helsinki, and that the clinical trial data are credible.},
	author = {{International Conference on Harmonisation of Technical Requirements for Registration of Pharmaceuticals for Human Use}},
	year = {2016},
	note = {Issue: 4},
	keywords = {ICH},
}

@article{hernan_specifying_2016,
	title = {Specifying a target trial prevents immortal time bias and other self-inflicted injuries in observational analyses},
	volume = {79},
	url = {http://www.jclinepi.com/article/S0895435616301366/fulltext},
	doi = {10.1016/j.jclinepi.2016.04.014},
	abstract = {Many analyses of observational data are attempts to emulate a target trial. The emulation of the target trial may fail when researchers deviate from simple principles that guide the design and analysis of randomized experiments. We review a framework to describe and prevent biases, including immortal time bias, that result from a failure to align start of follow-up, specification of eligibility, and treatment assignment. We review some analytic approaches to avoid these problems in comparative effectiveness or safety research.},
	journal = {Journal of Clinical Epidemiology},
	author = {Hernán, Miguel A and Sauer, Brian C and Hernández-Díaz, Sonia and Platt, Robert and Shrier, Ian},
	month = nov,
	year = {2016},
	note = {Publisher: Elsevier USA},
	keywords = {trial design, observational},
	pages = {70--75},
	file = {Accepted Version:C\:\\Users\\anbe6\\Zotero\\storage\\W4E9T2ER\\Hernán et al. - 2016 - Specifying a target trial prevents immortal time b.pdf:application/pdf},
}

@article{fine_proportional_1999,
	title = {A {Proportional} {Hazards} {Model} for the {Subdistribution} of a {Competing} {Risk}},
	volume = {94},
	doi = {10.1080/01621459.1999.10474144},
	abstract = {With explanatory covariates, the standard analysis for competing risks data involves modeling the cause-specific hazard functions via a proportional hazards assumption. Unfortunately, the cause-specific hazard function does not have a direct interpretation in terms of survival probabilities for the particular failure type. In recent years many clinicians have begun using the cumulative incidence function, the marginal failure probabilities for a particular cause, which is intuitively appealing and more easily explained to the nonstatistician. The cumulative incidence is especially relevant in cost-effectiveness analyses in which the survival probabilities are needed to determine treatment utility. Previously, authors have considered methods for combining estimates of the cause-specific hazard functions under the proportional hazards formulation. However, these methods do not allow the analyst to directly assess the effect of a covariate on the marginal probability function. In this article we propose a novel semiparametric proportional hazards model for the subdistribution. Using the partial likelihood principle and weighting techniques, we derive estimation and inference procedures for the finite-dimensional regression parameter under a variety of censoring scenarios. We give a uniformly consistent estimator for the predicted cumulative incidence for an individual with certain covariates; confidence intervals and bands can be obtained analytically or with an easy-to-implement simulation technique. To contrast the two approaches, we analyze a dataset from a breast cancer clinical trial under both models. © 1999 Taylor \& Francis Group, LLC.},
	number = {446},
	journal = {Journal of the American Statistical Association},
	author = {Fine, Jason P. and Gray, Robert J.},
	month = jun,
	year = {1999},
	keywords = {survival analysis, competing risks},
	pages = {496--509},
}

@article{eisenhauer_phase_2000,
	title = {Phase {I} {Clinical} {Trial} {Design} in {Cancer} {Drug} {Development}},
	volume = {18},
	abstract = {The past decade has seen the publication of a number of new proposals for the design of phase I trials of anticancer agents. The purpose of these proposals has been to address ethical concerns about treating excessive numbers of patients at subtherapeu-tic doses of a new agent and to increase the overall efficiency of the process while enhancing the precision of the recommended phase II dose. In early 1998, a workshop of phase I investigators was held under the sponsorship of Bristol-Myers Squibb Pharmaceutical Research Institute (Wallingford, CT) to review the experience to date with novel phase I methodologies, with a particular focus on their efficiency and safety. This report summarizes the material presented. It was concluded that for phase I trials of antineoplastics (cytotox-ics), which begin at 0.1 mouse-equivalent LD10 doses, evidence to date suggests that the historic approach of using a modified Fibonacci escalation and three patients per dose level is not necessary and is seldom used. One patient per dose level and more rapid escalation schemes, both empirically based and statistically based, are commonly used with apparent safety. There remain questions, however: Which of the dose escalation schemes is optimal? Are there alternatives to toxicity as a phase I end point, and will these end points be reliable in defining active doses? Answering these questions in a reasonable time frame will be important if new anticancer agents are not to suffer undue delays in phase I evaluation.},
	journal = {J Clin Oncol},
	author = {Eisenhauer, E A and O'dwyer, P J and Christian, M and Humphrey, J S},
	year = {2000},
	keywords = {trial design, phase I},
	pages = {684--692},
}

@article{bower_multicentre_1997,
	title = {Multicentre {CRC} phase {II} trial of temozolomide in recurrent or progressive high-grade glioma},
	volume = {40},
	url = {https://link.springer.com/article/10.1007/s002800050691},
	doi = {10.1007/S002800050691/METRICS},
	abstract = {Purpose: Patients with progressive or recurrent supratentorial high-grade gliomas were entered into a multicentre phase II trial to evaluate the efficacy and toxicity of temozolomide. Methods: The treatment schedule was 150-200 mg/m2 per day orally for 5 days repeated every 28 days. Response evaluation was by a combination of neurological status evaluation (MRC scale) and imaging. Results: Of 103 eligible patients enrolled, 11 (11\%) achieved an objective response and a further 48 (47\%) had stable disease. The median response duration was 4.6 months. Response rates were similar for anaplastic astrocytomas (grade III) and glioblastoma multiforme (grade IV) tumours. Predictable myelosuppression was the major toxicity. Conclusion: The observation of objective response and tolerable side effects in this heterogeneous population of patients supports the further investigation of this agent in high-grade gliomas.},
	number = {6},
	journal = {Cancer Chemotherapy and Pharmacology},
	author = {Bower, M. and Newlands, E. S. and Bleehen, N. M. and Brada, M. and Begent, R. J.H. and Calvert, H. and Colquhoun, I. and Lewis, P. and Brampton, M. H.},
	year = {1997},
	note = {Publisher: Springer},
	keywords = {glioblastoma},
	pages = {484--488},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\RYSFRVFL\\Bower et al. - 1997 - Multicentre CRC phase II trial of temozolomide in .pdf:application/pdf},
}

@article{ruberg_application_2023,
	title = {Application of {Bayesian} approaches in drug development: starting a virtuous cycle},
	volume = {22},
	copyright = {2023 Springer Nature Limited},
	issn = {1474-1784},
	shorttitle = {Application of {Bayesian} approaches in drug development},
	url = {https://www.nature.com/articles/s41573-023-00638-0},
	doi = {10.1038/s41573-023-00638-0},
	abstract = {The pharmaceutical industry and its global regulators have routinely used frequentist statistical methods, such as null hypothesis significance testing and p values, for evaluation and approval of new treatments. The clinical drug development process, however, with its accumulation of data over time, can be well suited for the use of Bayesian statistical approaches that explicitly incorporate existing data into clinical trial design, analysis and decision-making. Such approaches, if used appropriately, have the potential to substantially reduce the time and cost of bringing innovative medicines to patients, as well as to reduce the exposure of patients in clinical trials to ineffective or unsafe treatment regimens. Nevertheless, despite advances in Bayesian methodology, the availability of the necessary computational power and growing amounts of relevant existing data that could be used, Bayesian methods remain underused in the clinical development and regulatory review of new therapies. Here, we highlight the value of Bayesian methods in drug development, discuss barriers to their application and recommend approaches to address them. Our aim is to engage stakeholders in the process of considering when the use of existing data is appropriate and how Bayesian methods can be implemented more routinely as an effective tool for doing so.},
	language = {en},
	number = {3},
	urldate = {2024-11-22},
	journal = {Nat Rev Drug Discov},
	author = {Ruberg, Stephen J. and Beckers, Francois and Hemmings, Rob and Honig, Peter and Irony, Telba and LaVange, Lisa and Lieberman, Grazyna and Mayne, James and Moscicki, Richard},
	month = mar,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Bayesian, trial design},
	pages = {235--250},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\BCHI5JV3\\Ruberg et al. - 2023 - Application of Bayesian approaches in drug develop.pdf:application/pdf},
}

@misc{volfovsky_causal_2015,
	title = {Causal inference for ordinal outcomes},
	url = {http://arxiv.org/abs/1501.01234},
	doi = {10.48550/arXiv.1501.01234},
	abstract = {Many outcomes of interest in the social and health sciences, as well as in modern applications in computational social science and experimentation on social media platforms, are ordinal and do not have a meaningful scale. Causal analyses that leverage this type of data, termed ordinal non-numeric, require careful treatment, as much of the classical potential outcomes literature is concerned with estimation and hypothesis testing for outcomes whose relative magnitudes are well defined. Here, we propose a class of finite population causal estimands that depend on conditional distributions of the potential outcomes, and provide an interpretable summary of causal effects when no scale is available. We formulate a relaxation of the Fisherian sharp null hypothesis of constant effect that accommodates the scale-free nature of ordinal non-numeric data. We develop a Bayesian procedure to estimate the proposed causal estimands that leverages the rank likelihood. We illustrate these methods with an application to educational outcomes in the General Social Survey.},
	urldate = {2024-10-23},
	publisher = {arXiv},
	author = {Volfovsky, Alexander and Airoldi, Edoardo M. and Rubin, Donald B.},
	month = jan,
	year = {2015},
	note = {arXiv:1501.01234},
	keywords = {causal inference, Bayesian, ordinal regression},
	file = {Preprint PDF:C\:\\Users\\anbe6\\Zotero\\storage\\IMG26SFK\\Volfovsky et al. - 2015 - Causal inference for ordinal outcomes.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\EJP8NT4X\\1501.html:text/html},
}

@article{chiba_bayesian_2018,
	title = {Bayesian {Inference} of {Causal} {Effects} for an {Ordinal} {Outcome} in {Randomized} {Trials}},
	volume = {6},
	copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
	issn = {2193-3685},
	url = {https://www.degruyter.com/document/doi/10.1515/jci-2017-0019/html},
	doi = {10.1515/jci-2017-0019},
	abstract = {In randomized trials in which two treatment arms are compared with a binary outcome, the causal effect can be identified by assuming that the two treatment arms are exchangeable. In trials with an ordinal outcome, which is categorized as more than two, the causal effect can be identified by assuming that the potential outcomes are independent and that the two treatment arms are exchangeable. In this article, we propose a Bayesian approach to causal inference that does not rely on these two assumptions. To achieve this purpose, we use a randomization-based approach and response type. Then, the likelihood function is derived by physical randomization in which subjects who belong to a response type are randomly assigned to the treatment or control, with no modeling assumption on the outcome. Our approach can derive not only the posterior distribution of the causal effect but also that of the number of subjects in each response type. The proposed approach is illustrated with two examples from randomized clinical trials.},
	language = {en},
	number = {2},
	urldate = {2024-10-23},
	journal = {Journal of Causal Inference},
	author = {Chiba, Yasutaka},
	month = sep,
	year = {2018},
	note = {Publisher: De Gruyter},
	keywords = {causal inference, Bayesian, ordinal regression, RCT},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\SS43Z7XN\\Chiba - 2018 - Bayesian Inference of Causal Effects for an Ordina.pdf:application/pdf},
}

@article{morris_planning_2022,
	title = {Planning a method for covariate adjustment in individually randomised trials: a practical guide},
	volume = {23},
	issn = {1745-6215},
	shorttitle = {Planning a method for covariate adjustment in individually randomised trials},
	url = {https://doi.org/10.1186/s13063-022-06097-z},
	doi = {10.1186/s13063-022-06097-z},
	abstract = {It has long been advised to account for baseline covariates in the analysis of confirmatory randomised trials, with the main statistical justifications being that this increases power and, when a randomisation scheme balanced covariates, permits a valid estimate of experimental error. There are various methods available to account for covariates but it is not clear how to choose among them.},
	number = {1},
	urldate = {2024-10-20},
	journal = {Trials},
	author = {Morris, Tim P. and Walker, A. Sarah and Williamson, Elizabeth J. and White, Ian R.},
	month = apr,
	year = {2022},
	keywords = {clinical trial design, risk difference},
	pages = {328},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\PLAGP96U\\Morris et al. - 2022 - Planning a method for covariate adjustment in indi.pdf:application/pdf},
}

@article{weissman_identifying_2011,
	title = {Identifying {Patients} in {Need} of a {Palliative} {Care} {Assessment} in the {Hospital} {Setting} \textit{{A} {Consensus} {Report} from the {Center} to {Advance} {Palliative} {Care}}},
	volume = {14},
	copyright = {http://www.liebertpub.com/nv/resources-tools/text-and-data-mining-policy/121/},
	issn = {1096-6218, 1557-7740},
	url = {http://www.liebertpub.com/doi/10.1089/jpm.2010.0347},
	doi = {10.1089/jpm.2010.0347},
	abstract = {Workforce shortages, late referrals, and palliative care program resource constraints present signiﬁcant barriers to meeting the needs of hospitalized patients facing serious illnesses. The Center to Advance Palliative Care convened a consensus panel to select criteria by which patients at high risk for unmet palliative care needs can be identiﬁed in advance for a palliative care screening assessment. The consensus panel developed primary and secondary criteria for two checklists—one to use for screening at the time of admission and one for daily patient rounds. The consensus panel believes that by implementing a checklist approach to screening patients for unmet palliative care needs, combined with educational initiatives and other system-change work, hospital staff engaged in day-to-day patient care can identify a majority of such needs, reserving specialty palliative care services for more complex problems.},
	language = {en},
	number = {1},
	urldate = {2024-10-16},
	journal = {Journal of Palliative Medicine},
	author = {Weissman, David E. and Meier, Diane E.},
	month = jan,
	year = {2011},
	keywords = {palliative care},
	pages = {17--23},
	file = {Weissman and Meier - 2011 - Identifying Patients in Need of a Palliative Care .pdf:C\:\\Users\\anbe6\\Zotero\\storage\\LQ74TZZY\\Weissman and Meier - 2011 - Identifying Patients in Need of a Palliative Care .pdf:application/pdf},
}

@article{temel_stepped_2024,
	title = {Stepped {Palliative} {Care} for {Patients} {With} {Advanced} {Lung} {Cancer}: {A} {Randomized} {Clinical} {Trial}},
	volume = {332},
	issn = {0098-7484},
	shorttitle = {Stepped {Palliative} {Care} for {Patients} {With} {Advanced} {Lung} {Cancer}},
	url = {https://doi.org/10.1001/jama.2024.10398},
	doi = {10.1001/jama.2024.10398},
	abstract = {Despite the evidence for early palliative care improving outcomes, it has not been widely implemented in part due to palliative care workforce limitations.To evaluate a stepped-care model to deliver less resource-intensive and more patient-centered palliative care for patients with advanced cancer.Randomized, nonblinded, noninferiority trial of stepped vs early palliative care conducted between February 12, 2018, and December 15, 2022, at 3 academic medical centers in Boston, Massachusetts, Philadelphia, Pennsylvania, and Durham, North Carolina, among 507 patients who had been diagnosed with advanced lung cancer within the past 12 weeks.Step 1 of the intervention was an initial palliative care visit within 4 weeks of enrollment and subsequent visits only at the time of a change in cancer treatment or after a hospitalization. During step 1, patients completed a measure of quality of life (QOL; Functional Assessment of Cancer Therapy–Lung [FACT-L]; range, 0-136, with higher scores indicating better QOL) every 6 weeks, and those with a 10-point or greater decrease from baseline were stepped up to meet with the palliative care clinician every 4 weeks (intervention step 2). Patients assigned to early palliative care had palliative care visits every 4 weeks after enrollment.Noninferiority (margin = −4.5) of the effect of stepped vs early palliative care on patient-reported QOL on the FACT-L at week 24.The sample (n = 507) mostly included patients with advanced non–small cell lung cancer (78.3\%; mean age, 66.5 years; 51.4\% female; 84.6\% White). The mean number of palliative care visits by week 24 was 2.4 for stepped palliative care and 4.7 for early palliative care (adjusted mean difference, −2.3; P \&lt; .001). FACT-L scores at week 24 for the stepped palliative care group were noninferior to scores among those receiving early palliative care (adjusted FACT-L mean score, 100.6 vs 97.8, respectively; difference, 2.9; lower 1-sided 95\% confidence limit, −0.1; P \&lt; .001 for noninferiority). Although the rate of end-of-life care communication was also noninferior between groups, noninferiority was not demonstrated for days in hospice (adjusted mean, 19.5 with stepped palliative care vs 34.6 with early palliative care; P = .91).A stepped-care model, with palliative care visits occurring only at key points in patients’ cancer trajectories and using a decrement in QOL to trigger more intensive palliative care exposure, resulted in fewer palliative care visits without diminishing the benefits for patients’ QOL. While stepped palliative care was associated with fewer days in hospice, it is a more scalable way to deliver early palliative care to enhance patient-reported outcomes.ClinicalTrials.gov Identifier: NCT03337399},
	number = {6},
	urldate = {2024-10-16},
	journal = {JAMA},
	author = {Temel, Jennifer S. and Jackson, Vicki A. and El-Jawahri, Areej and Rinaldi, Simone P. and Petrillo, Laura A. and Kumar, Pallavi and McGrath, Kathryn A. and LeBlanc, Thomas W. and Kamal, Arif H. and Jones, Christopher A. and Rabideau, Dustin J. and Horick, Nora and Pintro, Kedie and Gallagher Medeiros, Emily R. and Post, Kathryn E. and Greer, Joseph A.},
	month = aug,
	year = {2024},
	keywords = {palliative care},
	pages = {471--481},
	file = {Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\BRUKDPWC\\2819643.html:text/html},
}

@article{wang_severity_2019,
	title = {Severity and incidence of complications assessed by the {Clavien}–{Dindo} classification following robotic and laparoscopic gastrectomy for advanced gastric cancer: a retrospective and propensity score-matched study},
	volume = {33},
	issn = {1432-2218},
	shorttitle = {Severity and incidence of complications assessed by the {Clavien}–{Dindo} classification following robotic and laparoscopic gastrectomy for advanced gastric cancer},
	url = {https://doi.org/10.1007/s00464-018-06624-7},
	doi = {10.1007/s00464-018-06624-7},
	abstract = {Robot-assisted gastrectomy (RAG) has been increasingly used for the treatment of advanced gastric cancer (AGC), and many advantages over laparoscopy-assisted gastrectomy (LAG) have been reported. However, its postgastrectomy complications still under investigation and the results remain controversial. This study aimed to objectively assess the incidence and severity of complications following RAG vs. LAG using Clavien–Dindo (C–D) classification and to identify risk factors related to complications.},
	language = {en},
	number = {10},
	urldate = {2024-10-16},
	journal = {Surg Endosc},
	author = {Wang, Wen-Jie and Li, Hong-Tao and Yu, Jian-Ping and Su, Lin and Guo, Chang-An and Chen, Peng and Yan, Long and Li, Kun and Ma, You-Wei and Wang, Ling and Hu, Wei and Li, Yu-Min and Liu, Hong-Bin},
	month = oct,
	year = {2019},
	keywords = {Clavien–Dindo classification, Gastric cancer, Laparoscopy-assisted gastrectomy, Postoperative complication, Robot-assisted gastrectomy},
	pages = {3341--3354},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\9AQ7NLK4\\Wang et al. - 2019 - Severity and incidence of complications assessed b.pdf:application/pdf},
}

@article{wu_matching_2024,
	title = {Matching on {Generalized} {Propensity} {Scores} with {Continuous} {Exposures}},
	volume = {119},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2022.2144737},
	doi = {10.1080/01621459.2022.2144737},
	abstract = {In the context of a binary treatment, matching is a well-established approach in causal inference. However, in the context of a continuous treatment or exposure, matching is still underdeveloped. We propose an innovative matching approach to estimate an average causal exposure-response function under the setting of continuous exposures that relies on the generalized propensity score (GPS). Our approach maintains the following attractive features of matching: (a) clear separation between the design and the analysis; (b) robustness to model misspecification or to the presence of extreme values of the estimated GPS; (c) straightforward assessments of covariate balance. We first introduce an assumption of identifiability, called local weak unconfoundedness. Under this assumption and mild smoothness conditions, we provide theoretical guarantees that our proposed matching estimator attains point-wise consistency and asymptotic normality. In simulations, our proposed matching approach outperforms existing methods under settings with model misspecification or in the presence of extreme values of the estimated GPS. We apply our proposed method to estimate the average causal exposure-response function between long-term PM 2.5 exposure and all-cause mortality among 68.5 million Medicare enrollees, 2000–2016. We found strong evidence of a harmful effect of long-term PM 2.5 exposure on mortality. Code for the proposed matching approach is provided in the CausalGPS R package, which is available on CRAN and provides a computationally efficient implementation. Supplementary materials for this article are available online.},
	number = {545},
	urldate = {2024-10-12},
	journal = {Journal of the American Statistical Association},
	author = {Wu, Xiao and Mealli, Fabrizia and Kioumourtzoglou, Marianthi-Anna and Dominici, Francesca and Braun, Danielle},
	month = jan,
	year = {2024},
	pmid = {38524247},
	note = {Publisher: ASA Website
\_eprint: https://doi.org/10.1080/01621459.2022.2144737},
	keywords = {causal inference, generalized propensity scores},
	pages = {757--772},
}

@misc{wu_matching_2021,
	title = {Matching on {Generalized} {Propensity} {Scores} with {Continuous} {Exposures}},
	url = {http://arxiv.org/abs/1812.06575},
	abstract = {In the context of a binary treatment, matching is a well-established approach in causal inference. However, in the context of a continuous treatment or exposure, matching is still underdeveloped. We propose an innovative matching approach to estimate an average causal exposure-response function under the setting of continuous exposures that relies on the generalized propensity score (GPS). Our approach maintains the following attractive features of matching: a) clear separation between the design and the analysis; b) robustness to model misspecification or to the presence of extreme values of the estimated GPS; c) straightforward assessment of covariate balance. We first introduce an assumption of identifiability, called local weak unconfoundedness. Under this assumption and mild smoothness conditions, we provide theoretical guarantees that our proposed matching estimator attains point-wise consistency and asymptotic normality. In simulations, our proposed matching approach outperforms existing methods under settings of model misspecification or the presence of extreme values of the estimated GPS. We apply our proposed method to estimate the average causal exposure-response function between long-term PM\$\_\{2.5\}\$ exposure and all-cause mortality among 68.5 million Medicare enrollees, 2000-2016. We found strong evidence of a harmful effect of long-term PM\$\_\{2.5\}\$ exposure on mortality. Code for the proposed matching approach is provided in the CausalGPS R package, which is available on CRAN and provides a computationally efficient implementation.},
	urldate = {2024-10-12},
	publisher = {arXiv},
	author = {Wu, Xiao and Mealli, Fabrizia and Kioumourtzoglou, Marianthi-Anna and Dominici, Francesca and Braun, Danielle},
	month = aug,
	year = {2021},
	note = {arXiv:1812.06575},
	keywords = {causal inference, matching, generalized propensity scores},
	file = {Preprint PDF:C\:\\Users\\anbe6\\Zotero\\storage\\6W3XG9H6\\Wu et al. - 2021 - Matching on Generalized Propensity Scores with Con.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\TCPX3XI8\\1812.html:text/html},
}

@misc{khoshnevis_causalgps_2023,
	title = {{CausalGPS}: {An} {R} {Package} for {Causal} {Inference} {With} {Continuous} {Exposures}},
	shorttitle = {{CausalGPS}},
	url = {http://arxiv.org/abs/2310.00561},
	doi = {10.48550/arXiv.2310.00561},
	abstract = {Quantifying the causal effects of continuous exposures on outcomes of interest is critical for social, economic, health, and medical research. However, most existing software packages focus on binary exposures. We develop the CausalGPS R package that implements a collection of algorithms to provide algorithmic solutions for causal inference with continuous exposures. CausalGPS implements a causal inference workflow, with algorithms based on generalized propensity scores (GPS) as the core, extending propensity scores (the probability of a unit being exposed given pre-exposure covariates) from binary to continuous exposures. As the first step, the package implements efficient and flexible estimations of the GPS, allowing multiple user-specified modeling options. As the second step, the package provides two ways to adjust for confounding: weighting and matching, generating weighted and matched data sets, respectively. Lastly, the package provides built-in functions to fit flexible parametric, semi-parametric, or non-parametric regression models on the weighted or matched data to estimate the exposure-response function relating the outcome with the exposures. The computationally intensive tasks are implemented in C++, and efficient shared-memory parallelization is achieved by OpenMP API. This paper outlines the main components of the CausalGPS R package and demonstrates its application to assess the effect of long-term exposure to PM2.5 on educational attainment using zip code-level data from the contiguous United States from 2000-2016.},
	urldate = {2024-05-09},
	publisher = {arXiv},
	author = {Khoshnevis, Naeem and Wu, Xiao and Braun, Danielle},
	month = sep,
	year = {2023},
	note = {arXiv:2310.00561 [cs, econ, stat]},
	keywords = {causal inference, generalized propensity scores},
	file = {arXiv Fulltext PDF:C\:\\Users\\anbe6\\Zotero\\storage\\Q5FLLXEH\\Khoshnevis et al. - 2023 - CausalGPS An R Package for Causal Inference With .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\RN3JTZJN\\2310.html:text/html},
}

@article{austin_assessing_2019,
	title = {Assessing covariate balance when using the generalized propensity score with quantitative or continuous exposures},
	volume = {28},
	issn = {0962-2802},
	url = {https://doi.org/10.1177/0962280218756159},
	doi = {10.1177/0962280218756159},
	abstract = {Propensity score methods are increasingly being used to estimate the effects of treatments and exposures when using observational data. The propensity score was initially developed for use with binary exposures (e.g., active treatment vs. control). The generalized propensity score is an extension of the propensity score for use with quantitative exposures (e.g., dose or quantity of medication, income, years of education). A crucial component of any propensity score analysis is that of balance assessment. This entails assessing the degree to which conditioning on the propensity score (via matching, weighting, or stratification) has balanced measured baseline covariates between exposure groups. Methods for balance assessment have been well described and are frequently implemented when using the propensity score with binary exposures. However, there is a paucity of information on how to assess baseline covariate balance when using the generalized propensity score. We describe how methods based on the standardized difference can be adapted for use with quantitative exposures when using the generalized propensity score. We also describe a method based on assessing the correlation between the quantitative exposure and each covariate in the sample when weighted using generalized propensity score -based weights. We conducted a series of Monte Carlo simulations to evaluate the performance of these methods. We also compared two different methods of estimating the generalized propensity score: ordinary least squared regression and the covariate balancing propensity score method. We illustrate the application of these methods using data on patients hospitalized with a heart attack with the quantitative exposure being creatinine level.},
	language = {en},
	number = {5},
	urldate = {2024-10-15},
	journal = {Stat Methods Med Res},
	author = {Austin, Peter C},
	month = may,
	year = {2019},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {causal inference, generalized propensity scores},
	pages = {1365--1377},
	file = {SAGE PDF Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\27WCJAJS\\Austin - 2019 - Assessing covariate balance when using the general.pdf:application/pdf},
}

@article{austin_assessing_2018,
	title = {Assessing the performance of the generalized propensity score for estimating the effect of quantitative or continuous exposures on binary outcomes},
	volume = {37},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7615},
	doi = {10.1002/sim.7615},
	abstract = {Propensity score methods are increasingly being used to estimate the effects of treatments and exposures when using observational data. The propensity score was initially developed for use with binary exposures. The generalized propensity score (GPS) is an extension of the propensity score for use with quantitative or continuous exposures (eg, dose or quantity of medication, income, or years of education). We used Monte Carlo simulations to examine the performance of different methods of using the GPS to estimate the effect of continuous exposures on binary outcomes. We examined covariate adjustment using the GPS and weighting using weights based on the inverse of the GPS. We examined both the use of ordinary least squares to estimate the propensity function and the use of the covariate balancing propensity score algorithm. The use of methods based on the GPS was compared with the use of G-computation. All methods resulted in essentially unbiased estimation of the population dose-response function. However, GPS-based weighting tended to result in estimates that displayed greater variability and had higher mean squared error when the magnitude of confounding was strong. Of the methods based on the GPS, covariate adjustment using the GPS tended to result in estimates with lower variability and mean squared error when the magnitude of confounding was strong. We illustrate the application of these methods by estimating the effect of average neighborhood income on the probability of death within 1 year of hospitalization for an acute myocardial infarction.},
	language = {en},
	number = {11},
	urldate = {2024-10-15},
	journal = {Statistics in Medicine},
	author = {Austin, Peter C.},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.7615},
	keywords = {causal inference, generalized propensity scores},
	pages = {1874--1894},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\ZJHNGUIR\\Austin - 2018 - Assessing the performance of the generalized prope.pdf:application/pdf;Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\YX3Q3ZVE\\Austin - 2018 - Assessing the performance of the generalized prope.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\RDSWDXS2\\sim.html:text/html},
}

@article{zhang_causal_2016,
	title = {Causal inference with a quantitative exposure},
	volume = {25},
	issn = {0962-2802},
	url = {https://doi.org/10.1177/0962280212452333},
	doi = {10.1177/0962280212452333},
	abstract = {SummaryThe current statistical literature on causal inference is mostly concerned with binary or categorical exposures, even though exposures of a quantitative nature are frequently encountered in epidemiologic research. In this article, we review the available methods for estimating the dose–response curve for a quantitative exposure, which include ordinary regression based on an outcome regression model, inverse propensity weighting and stratification based on a propensity function model, and an augmented inverse propensity weighting method that is doubly robust with respect to the two models. We note that an outcome regression model often imposes an implicit constraint on the dose–response curve, and propose a flexible modeling strategy that avoids constraining the dose–response curve. We also propose two new methods: a weighted regression method that combines ordinary regression with inverse propensity weighting and a stratified regression method that combines ordinary regression with stratification. The proposed methods are similar to the augmented inverse propensity weighting method in the sense of double robustness, but easier to implement and more generally applicable. The methods are illustrated with an obstetric example and compared in simulation studies.},
	language = {en},
	number = {1},
	urldate = {2024-10-15},
	journal = {Stat Methods Med Res},
	author = {Zhang, Zhiwei and Zhou, Jie and Cao, Weihua and Zhang, Jun},
	month = feb,
	year = {2016},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {causal inference},
	pages = {315--335},
}

@article{liu_review_2024,
	title = {A {Review} of {Causal} {Inference} {Methods} for {Estimating} the {Effects} of {Exposure} {Change} when {Incident} {Exposure} {Is} {Unobservable}},
	volume = {11},
	issn = {2196-2995},
	url = {https://doi.org/10.1007/s40471-024-00343-5},
	doi = {10.1007/s40471-024-00343-5},
	abstract = {Research questions on exposure change and health outcomes are both relevant to clinical and policy decision making for public health. Causal inference methods can help investigators answer questions about exposure change when the first or incident exposure is unobserved or not well defined. This review aims to help researchers conceive of helpful causal research questions about exposure change and understand various statistical methods for answering these questions to promote wider adoption of causal inference methods in research on exposure change outside the field of pharmacoepidemiology.},
	language = {en},
	number = {4},
	urldate = {2024-10-15},
	journal = {Curr Epidemiol Rep},
	author = {Liu, Fangyu and Duchesneau, Emilie D. and Lund, Jennifer L. and Jackson, John W.},
	month = dec,
	year = {2024},
	keywords = {causal inference, propensity score, g-computation},
	pages = {185--198},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\RJGC6G2P\\Liu et al. - 2024 - A Review of Causal Inference Methods for Estimatin.pdf:application/pdf},
}

@article{subramaniam_new_2018,
	title = {New 5-{Factor} {Modified} {Frailty} {Index} {Using} {American} {College} of {Surgeons} {NSQIP} {Data}},
	volume = {226},
	issn = {1072-7515},
	url = {https://www.sciencedirect.com/science/article/pii/S1072751517320823},
	doi = {10.1016/j.jamcollsurg.2017.11.005},
	abstract = {Background
The modified frailty index (mFI-11) is a NSQIP-based 11-factor index that has been proven to adequately reflect frailty and predict mortality and morbidity. These 11 factors, made of 16 variables, map to the original 70-item Canada Study of Health and Aging Frailty Index. In past years, certain NSQIP variables have been removed from the database; as of 2015, only 5 of the original 11 factors remained. The predictive power and usefulness of these 5 factors in an index (mFI-5) have not been proven in past literature. The goal of our study was to compare the mFI-5 to the mFI-11 in terms of value and predictive ability for mortality, postoperative infection, and unplanned 30-day readmission.
Study Design
The mFI was calculated by dividing the number of factors present for a patient by the number of available factors for which there were no missing data. Spearman's rho was used to assess correlation between the mFI-5 and mFI-11. Predictive models, using both unadjusted and adjusted logistic regressions, were created for each outcome for 9 surgical sub-specialties using 2012 NSQIP data, the last year all mFI-11 variables existed.
Results
Correlation between the mFI-5 and mFI-11 was above 0.9 across all surgical specialties except for cardiac and vascular surgery. Adjusted and unadjusted models showed similar c-statistics for mFI-5 and mFI-11, and strong predictive ability for mortality and postoperative complications.
Conclusions
The mFI-5 and the mFI-11 are equally effective predictors in all sub-specialties and the mFI-5 is a strong predictor of mortality and postoperative complications. It has credibility for future use to study frailty within the NSQIP database. It also has potential in other databases and for clinical use.},
	number = {2},
	urldate = {2024-10-09},
	journal = {Journal of the American College of Surgeons},
	author = {Subramaniam, Sneha and Aalberg, Jeffrey J. and Soriano, Rainier P. and Divino, Celia M.},
	month = feb,
	year = {2018},
	pages = {173--181.e8},
	file = {ScienceDirect Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\Z9L54GXX\\S1072751517320823.html:text/html},
}

@article{carpenter_missing_2021,
	title = {Missing data: {A} statistical framework for practice},
	volume = {63},
	issn = {1521-4036},
	shorttitle = {Missing data},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bimj.202000196},
	doi = {10.1002/bimj.202000196},
	abstract = {Missing data are ubiquitous in medical research, yet there is still uncertainty over when restricting to the complete records is likely to be acceptable, when more complex methods (e.g. maximum likelihood, multiple imputation and Bayesian methods) should be used, how they relate to each other and the role of sensitivity analysis. This article seeks to address both applied practitioners and researchers interested in a more formal explanation of some of the results. For practitioners, the framework, illustrative examples and code should equip them with a practical approach to address the issues raised by missing data (particularly using multiple imputation), alongside an overview of how the various approaches in the literature relate. In particular, we describe how multiple imputation can be readily used for sensitivity analyses, which are still infrequently performed. For those interested in more formal derivations, we give outline arguments for key results, use simple examples to show how methods relate, and references for full details. The ideas are illustrated with a cohort study, a multi-centre case control study and a randomised clinical trial.},
	language = {en},
	number = {5},
	urldate = {2024-10-09},
	journal = {Biometrical Journal},
	author = {Carpenter, James R. and Smuk, Melanie},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/bimj.202000196},
	pages = {915--947},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\6A8BCBKJ\\Carpenter and Smuk - 2021 - Missing data A statistical framework for practice.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\RPVMREMR\\bimj.html:text/html},
}

@article{jandorf_culturally_2013,
	title = {Culturally {Targeted} {Patient} {Navigation} for {Increasing} {African} {Americans}' {Adherence} to {Screening} {Colonoscopy}: {A} {Randomized} {Clinical} {Trial}},
	volume = {22},
	issn = {1055-9965, 1538-7755},
	shorttitle = {Culturally {Targeted} {Patient} {Navigation} for {Increasing} {African} {Americans}' {Adherence} to {Screening} {Colonoscopy}},
	url = {https://aacrjournals.org/cebp/article/22/9/1577/69739/Culturally-Targeted-Patient-Navigation-for},
	doi = {10.1158/1055-9965.EPI-12-1275},
	abstract = {Background: Patient navigation has been an effective intervention to increase cancer screening rates. This study focuses on predicting outcomes of screening colonoscopy for colorectal cancer among African Americans using different patient navigation formats.
Methods: In a randomized clinical trial, patients more than 50 years of age without signiﬁcant comorbidities were randomized into three navigation groups: peer-patient navigation (n ¼ 181), pro-patient navigation (n ¼ 123), and standard (n ¼ 46). Pro-patient navigations were health care professionals who conducted culturally targeted navigation, whereas peer-patient navigations were community members trained in patient navigation who also discussed their personal experiences with screening colonoscopy. Two assessments gathered sociodemographic, medical, and intrapersonal information.
Results: Screening colonoscopy completion rate was 75.7\% across all groups with no signiﬁcant differences in completion between the three study arms. Annual income more than \$10,000 was an independent predictor of screening colonoscopy adherence. Unexpectedly, low social inﬂuence also predicted screening colonoscopy completion.
Conclusions: In an urban African American population, patient navigation was effective in increasing screening colonoscopy rates to 15\% above the national average, regardless of patient navigation type or content. Impact: Because patient navigation successfully increases colonoscopy adherence, cultural targeting may not be necessary in some populations. Cancer Epidemiol Biomarkers Prev; 22(9); 1577–87. Ó2013 AACR.},
	language = {en},
	number = {9},
	urldate = {2024-07-02},
	journal = {Cancer Epidemiology, Biomarkers \& Prevention},
	author = {Jandorf, Lina and Braschi, Caitlyn and Ernstoff, Elizabeth and Wong, Carrie R. and Thelemaque, Linda and Winkel, Gary and Thompson, Hayley S. and Redd, William H. and Itzkowitz, Steven H.},
	month = sep,
	year = {2013},
	keywords = {RCT, Colonoscopy},
	pages = {1577--1587},
	file = {Jandorf et al. - 2013 - Culturally Targeted Patient Navigation for Increas.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\S63DW82V\\Jandorf et al. - 2013 - Culturally Targeted Patient Navigation for Increas.pdf:application/pdf},
}

@article{pei_assessing_2018,
	title = {Assessing trends in laparoscopic colostomy reversal and evaluating outcomes when compared to open procedures},
	volume = {32},
	issn = {1432-2218},
	url = {https://doi.org/10.1007/s00464-017-5725-4},
	doi = {10.1007/s00464-017-5725-4},
	abstract = {Laparoscopic colostomy reversal has emerged as a viable option for Hartmann’s reversal but the trends in national adoption and postoperative complications are unknown. This study evaluates the practice trends for laparoscopic colostomy and compares complications, length of stay, and operative times between laparoscopic and open colostomy reversal.},
	language = {en},
	number = {2},
	urldate = {2024-10-04},
	journal = {Surg Endosc},
	author = {Pei, Kevin Y. and Davis, Kimberly A. and Zhang, Yawei},
	month = feb,
	year = {2018},
	keywords = {Colostomy, Laparoscopy, Quality improvement},
	pages = {695--701},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\LCSMMYJ8\\Pei et al. - 2018 - Assessing trends in laparoscopic colostomy reversa.pdf:application/pdf},
}

@article{beyhum_discussion_2023,
	title = {Discussion on “{Instrumented} {Difference}-in-{Differences}” by {Ting} {Ye}, {Ashkan} {Ertefaie}, {James} {Flory}, {Sean} {Hennessy}, and {Dylan} {S}. {Small}},
	volume = {79},
	issn = {0006-341X},
	url = {https://doi.org/10.1111/biom.13779},
	doi = {10.1111/biom.13779},
	abstract = {We discuss Ye et al. 2022, which combines instrumental variables methods with difference in differences. First, we compare the paper to other works in the difference in differences literatures and argue that the main contribution lies in the multiply robust estimation approach. Then, we reformulate the causal assumptions in Ye et al. 2022 in the usual theoretical framework of the instrumental variables literature. This clarifies in which sense the difference in differences design can weaken the standard instrumental variable conditions.},
	number = {2},
	urldate = {2024-07-19},
	journal = {Biometrics},
	author = {Beyhum, Jad and Florens, Jean-Pierre and Van Keilegom, Ingrid},
	month = jun,
	year = {2023},
	keywords = {causal inference, DiD, instrument},
	pages = {582--586},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\WXLNI8K7\\Beyhum et al. - 2023 - Discussion on “Instrumented Difference-in-Differen.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\IN236APN\\7513971.html:text/html},
}

@article{ye_instrumented_2023,
	title = {Instrumented {Difference}-in-{Differences}},
	volume = {79},
	copyright = {https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model},
	issn = {0006-341X, 1541-0420},
	url = {https://academic.oup.com/biometrics/article/79/2/569-581/7513952},
	doi = {10.1111/biom.13783},
	abstract = {Abstract
            Unmeasured confounding is a key threat to reliable causal inference based on observational studies. Motivated from two powerful natural experiment devices, the instrumental variables and difference-in-differences, we propose a new method called instrumented difference-in-differences that explicitly leverages exogenous randomness in an exposure trend to estimate the average and conditional average treatment effect in the presence of unmeasured confounding. We develop the identification assumptions using the potential outcomes framework. We propose a Wald estimator and a class of multiply robust and efficient semiparametric estimators, with provable consistency and asymptotic normality. In addition, we extend the instrumented difference-in-differences to a two-sample design to facilitate investigations of delayed treatment effect and provide a measure of weak identification. We demonstrate our results in simulated and real datasets.},
	language = {en},
	number = {2},
	urldate = {2024-07-19},
	journal = {Biometrics},
	author = {Ye, Ting and Ertefaie, Ashkan and Flory, James and Hennessy, Sean and Small, Dylan S.},
	month = jun,
	year = {2023},
	keywords = {causal inference, DiD, instrument},
	pages = {569--581},
	file = {Submitted Version:C\:\\Users\\anbe6\\Zotero\\storage\\MRH7VSJU\\Ye et al. - 2023 - Instrumented Difference-in-Differences.pdf:application/pdf},
}

@article{kang_discussion_2023,
	title = {Discussion on “{Instrumented} difference-in-differences” by {Ting} {Ye}, {Ashkan} {Ertefaie}, {James} {Flory}, {Sean} {Hennessy} \& {Dylan} {S}. {Small}},
	volume = {79},
	copyright = {© 2022 The Authors. Biometrics published by Wiley Periodicals LLC on behalf of International Biometric Society.},
	issn = {1541-0420},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.13784},
	doi = {10.1111/biom.13784},
	abstract = {We reinterpret the instrumented difference-in-differences (iDID) under a linear instrumental variables (IV) model. Under the linear IV model, we show why iDID is a clear improvement over two existing methods, difference-in-differences (DID) and a cross-sectional, IV analysis. We also re-express some of the assumptions of iDID using familiar, regression-based identification assumptions. We conclude with a method inspired by the linear IV model that can potentially remedy the weak identification problem in iDID.},
	language = {en},
	number = {2},
	urldate = {2024-07-19},
	journal = {Biometrics},
	author = {Kang, Hyunseung},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/biom.13784},
	keywords = {causal inference, DiD, instrument},
	pages = {592--596},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\WV2IHUUT\\Kang - 2023 - Discussion on “Instrumented difference-in-differen.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\U44T4B4C\\biom.html:text/html},
}

@article{holland_statistics_1986,
	title = {Statistics and {Causal} {Inference}},
	volume = {81},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1986.10478354},
	doi = {10.1080/01621459.1986.10478354},
	abstract = {Problems involving causal inference have dogged at the heels of statistics since its earliest days. Correlation does not imply causation, and yet causal conclusions drawn from a carefully designed experiment are often valid. What can a statistical model say about causation? This question is addressed by using a particular model for causal inference (Holland and Rubin 1983; Rubin 1974) to critique the discussions of other writers on causation and causal inference. These include selected philosophers, medical researchers, statisticians, econometricians, and proponents of causal modeling.},
	number = {396},
	urldate = {2024-07-02},
	journal = {Journal of the American Statistical Association},
	author = {Holland, Paul W.},
	month = dec,
	year = {1986},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1986.10478354},
	keywords = {causal inference},
	pages = {945--960},
}

@article{burkner_brms_2017,
	title = {brms: {An} {R} {Package} for {Bayesian}   {Multilevel} {Models} {Using} {Stan}},
	volume = {80},
	doi = {doi:10.18637/jss.v080.i01},
	number = {1},
	journal = {Journal of Statistical Software},
	author = {Bürkner, Paul-Christian},
	year = {2017},
	keywords = {R, Bayesian, brms},
	pages = {1--28},
}

@article{rubin_causal_2005,
	title = {Causal {Inference} {Using} {Potential} {Outcomes}: {Design}, {Modeling}, {Decisions}},
	volume = {100},
	issn = {0162-1459},
	shorttitle = {Causal {Inference} {Using} {Potential} {Outcomes}},
	url = {https://doi.org/10.1198/016214504000001880},
	doi = {10.1198/016214504000001880},
	abstract = {Causal effects are defined as comparisons of potential outcomes under different treatments on a common set of units. Observed values of the potential outcomes are revealed by the assignment mechanism—a probabilistic model for the treatment each unit receives as a function of covariates and potential outcomes. Fisher made tremendous contributions to causal inference through his work on the design of randomized experiments, but the potential outcomes perspective applies to other complex experiments and nonrandomized studies as well. As noted by Kempthorne in his 1976 discussion of Savage's Fisher lecture, Fisher never bridged his work on experimental design and his work on parametric modeling, a bridge that appears nearly automatic with an appropriate view of the potential outcomes framework, where the potential outcomes and covariates are given a Bayesian distribution to complete the model specification. Also, this framework crisply separates scientific inference for causal effects and decisions based on such inference, a distinction evident in Fisher's discussion of tests of significance versus tests in an accept/reject framework. But Fisher never used the potential outcomes framework, originally proposed by Neyman in the context of randomized experiments, and as a result he provided generally flawed advice concerning the use of the analysis of covariance to adjust for posttreatment concomitants in randomized trials.},
	number = {469},
	urldate = {2024-07-01},
	journal = {Journal of the American Statistical Association},
	author = {Rubin, Donald B},
	month = mar,
	year = {2005},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/016214504000001880},
	keywords = {causal inference},
	pages = {322--331},
}

@article{rubin_estimating_1974,
	title = {Estimating causal effects of treatments in randomized and nonrandomized studies},
	volume = {66},
	issn = {1939-2176},
	doi = {10.1037/h0037350},
	abstract = {Presents a discussion of matching, randomization, random sampling, and other methods of controlling extraneous variation. The objective was to specify the benefits of randomization in estimating causal effects of treatments. It is concluded that randomization should be employed whenever possible but that the use of carefully controlled nonrandomized data to estimate causal effects is a reasonable and necessary procedure in many cases. (15 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {5},
	journal = {Journal of Educational Psychology},
	author = {Rubin, Donald B.},
	year = {1974},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {causal inference},
	pages = {688--701},
	file = {Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\PGH7YLB4\\doiLanding.html:text/html},
}

@misc{noauthor_estimating_nodate,
	title = {Estimating causal effects of treatments in randomized and nonrandomized studies.},
	url = {https://psycnet.apa.org/record/1975-06502-001},
	urldate = {2024-07-01},
	file = {Estimating causal effects of treatments in randomized and nonrandomized studies.:C\:\\Users\\anbe6\\Zotero\\storage\\P25F2ITM\\1975-06502-001.html:text/html},
}

@article{sullivan_categorisation_2024,
	title = {Categorisation of continuous covariates for stratified randomisation: {How} should we adjust?},
	volume = {43},
	issn = {1097-0258},
	shorttitle = {Categorisation of continuous covariates for stratified randomisation},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.10060},
	doi = {10.1002/sim.10060},
	abstract = {To obtain valid inference following stratified randomisation, treatment effects should be estimated with adjustment for stratification variables. Stratification sometimes requires categorisation of a continuous prognostic variable (eg, age), which raises the question: should adjustment be based on randomisation categories or underlying continuous values? In practice, adjustment for randomisation categories is more common. We reviewed trials published in general medical journals and found none of the 32 trials that stratified randomisation based on a continuous variable adjusted for continuous values in the primary analysis. Using data simulation, this article evaluates the performance of different adjustment strategies for continuous and binary outcomes where the covariate-outcome relationship (via the link function) was either linear or non-linear. Given the utility of covariate adjustment for addressing missing data, we also considered settings with complete or missing outcome data. Analysis methods included linear or logistic regression with no adjustment for the stratification variable, adjustment for randomisation categories, or adjustment for continuous values assuming a linear covariate-outcome relationship or allowing for non-linearity using fractional polynomials or restricted cubic splines. Unadjusted analysis performed poorly throughout. Adjustment approaches that misspecified the underlying covariate-outcome relationship were less powerful and, alarmingly, biased in settings where the stratification variable predicted missing outcome data. Adjustment for randomisation categories tends to involve the highest degree of misspecification, and so should be avoided in practice. To guard against misspecification, we recommend use of flexible approaches such as fractional polynomials and restricted cubic splines when adjusting for continuous stratification variables in randomised trials.},
	language = {en},
	number = {11},
	urldate = {2024-07-01},
	journal = {Statistics in Medicine},
	author = {Sullivan, Thomas R. and Morris, Tim P. and Kahan, Brennan C. and Cuthbert, Alana R. and Yelland, Lisa N.},
	year = {2024},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.10060},
	keywords = {trial design, covariate adjustment},
	pages = {2083--2095},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\4TKY53CW\\Sullivan et al. - 2024 - Categorisation of continuous covariates for strati.pdf:application/pdf},
}

@misc{kurz_causal_2023,
	title = {Causal inference with {Bayesian} models},
	url = {https://solomonkurz.netlify.app/blog/2023-04-30-causal-inference-with-bayesian-models/},
	abstract = {Part 4 of the GLM and causal inference series.},
	language = {en},
	urldate = {2024-07-01},
	author = {Kurz, A. Solomon},
	year = {2023},
	keywords = {R, causal inference, Bayesian},
	file = {Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\WMDZT3IF\\2023-04-30-causal-inference-with-bayesian-models.html:text/html},
}

@article{borg_unmet_2024,
	title = {Unmet healthcare needs, access to services and experiences with health providers among persons with spinal cord injury in {Australia}},
	issn = {1362-4393, 1476-5624},
	url = {https://www.nature.com/articles/s41393-024-00997-4},
	doi = {10.1038/s41393-024-00997-4},
	abstract = {OBJECTIVES: Appropriate and timely lifelong access to healthcare following a spinal cord injury (SCI) is critical, yet unmet healthcare needs in this population are common. Poor experiences with healthcare providers can be a barrier to health-seeking behaviour, and we hypothesised that there would be an association between unmet healthcare needs and care experiences. This study aimed to: (1) describe healthcare provider utilisation in the past year, unmet care needs and satisfaction with healthcare services; (2) explore the association between experiences with healthcare providers and unmet healthcare needs; and (3) explore the association between healthcare provider utilisation and participant characteristics, including unmet healthcare needs. SETTING: Community.
METHODS: Analysis of data for 1579 Australians aged ≥ 18, who were ≥ 1-year post-SCI and living in the community. Bayesian penalised regression was used to model six binary outcomes: unmet healthcare needs; the use of general practitioners (GPs), allied health practitioners, rehabilitation specialists; medical specialists; and hospitalisations in the past 12-months.
RESULTS: Unmet needs were reported by 17\% of participants, with service cost the common deterrent. There was evidence of an effect for provider experiences on unmet healthcare needs, but no evidence that unmet healthcare needs was associated with the use of GPs, allied health practitioners, and rehabilitation or medical specialists.
CONCLUSIONS: Unmet healthcare needs were reported in the context of high healthcare use and large proportions of secondary conditions in a cohort with long-term SCI. Improved health access for people with SCI include better primary-secondary care collaboration is needed. Spinal Cord; https://doi.org/10.1038/s41393-024-00997-4},
	language = {en},
	urldate = {2024-07-01},
	journal = {Spinal Cord},
	author = {Borg, Samantha J. and Borg, David N. and Arora, Mohit and Middleton, James W. and Marshall, Ruth and Nunn, Andrew and Geraghty, Timothy},
	month = may,
	year = {2024},
	keywords = {Bayesian, brms example writing},
	file = {Borg et al. - 2024 - Unmet healthcare needs, access to services and exp.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\A8SJ2BN6\\Borg et al. - 2024 - Unmet healthcare needs, access to services and exp.pdf:application/pdf},
}

@article{muller_estimating_2014,
	title = {Estimating predicted probabilities from logistic regression: different methods correspond to different target populations},
	volume = {43},
	issn = {0300-5771},
	shorttitle = {Estimating predicted probabilities from logistic regression},
	url = {https://doi.org/10.1093/ije/dyu029},
	doi = {10.1093/ije/dyu029},
	abstract = {Background: We review three common methods to estimate predicted probabilities following confounder-adjusted logistic regression: marginal standardization (predicted probabilities summed to a weighted average reflecting the confounder distribution in the target population); prediction at the modes (conditional predicted probabilities calculated by setting each confounder to its modal value); and prediction at the means (predicted probabilities calculated by setting each confounder to its mean value). That each method corresponds to a different target population is underappreciated in practice. Specifically, prediction at the means is often incorrectly interpreted as estimating average probabilities for the overall study population, and furthermore yields nonsensical estimates in the presence of dichotomous confounders. Default commands in popular statistical software packages often lead to inadvertent misapplication of prediction at the means.Methods: Using an applied example, we demonstrate discrepancies in predicted probabilities across these methods, discuss implications for interpretation and provide syntax for SAS and Stata.Results: Marginal standardization allows inference to the total population from which data are drawn. Prediction at the modes or means allows inference only to the relevant stratum of observations. With dichotomous confounders, prediction at the means corresponds to a stratum that does not include any real-life observations.Conclusions: Marginal standardization is the appropriate method when making inference to the overall population. Other methods should be used with caution, and prediction at the means should not be used with binary confounders. Stata, but not SAS, incorporates simple methods for marginal standardization.},
	number = {3},
	urldate = {2024-07-01},
	journal = {International Journal of Epidemiology},
	author = {Muller, Clemma J and MacLehose, Richard F},
	month = jun,
	year = {2014},
	keywords = {causal inference},
	pages = {962--970},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\RI6VRULU\\Muller and MacLehose - 2014 - Estimating predicted probabilities from logistic r.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\6NQYCM89\\763470.html:text/html},
}

@article{butterly_comorbidity_2023,
	title = {Comorbidity and health-related quality of life in people with a chronic medical condition in randomised clinical trials: {An} individual participant data meta-analysis},
	volume = {20},
	issn = {1549-1676},
	shorttitle = {Comorbidity and health-related quality of life in people with a chronic medical condition in randomised clinical trials},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1004154},
	doi = {10.1371/journal.pmed.1004154},
	abstract = {Background Health-related quality of life metrics evaluate treatments in ways that matter to patients, so are often included in randomised clinical trials (hereafter trials). Multimorbidity, where individuals have 2 or more conditions, is negatively associated with quality of life. However, whether multimorbidity predicts change over time or modifies treatment effects for quality of life is unknown. Therefore, clinicians and guideline developers are uncertain about the applicability of trial findings to people with multimorbidity. We examined whether comorbidity count (higher counts indicating greater multimorbidity) (i) is associated with quality of life at baseline; (ii) predicts change in quality of life over time; and/or (iii) modifies treatment effects on quality of life. Methods and findings Included trials were registered on the United States trials registry for selected index medical conditions and drug classes, phase 2/3, 3 or 4, had ≥300 participants, a nonrestrictive upper age limit, and were available on 1 of 2 trial repositories on 21 November 2016 and 18 May 2018, respectively. Of 124 meeting these criteria, 56 trials (33,421 participants, 16 index conditions, and 23 drug classes) collected a generic quality of life outcome measure (35 EuroQol-5 dimension (EQ-5D), 31 36-item short form survey (SF-36) with 10 collecting both). Blinding and completeness of follow up were examined for each trial. Using trials where individual participant data (IPD) was available from 2 repositories, a comorbidity count was calculated from medical history and/or prescriptions data. Linear regressions were fitted for the association between comorbidity count and (i) quality of life at baseline; (ii) change in quality of life during trial follow up; and (iii) treatment effects on quality of life. These results were then combined in Bayesian linear models. Posterior samples were summarised via the mean, 2.5th and 97.5th percentiles as credible intervals (95\% CI) and via the proportion with values less than 0 as the probability (PBayes) of a negative association. All results are in standardised units (obtained by dividing the EQ-5D/SF-36 estimates by published population standard deviations). Per additional comorbidity, adjusting for age and sex, across all index conditions and treatment comparisons, comorbidity count was associated with lower quality of life at baseline and with a decline in quality of life over time (EQ-5D −0.02 [95\% CI −0.03 to −0.01], PBayes {\textgreater} 0.999). Associations were similar, but with wider 95\% CIs crossing the null for SF-36-PCS and SF-36-MCS (−0.05 [−0.10 to 0.01], PBayes = 0.956 and −0.05 [−0.10 to 0.01], PBayes = 0.966, respectively). Importantly, there was no evidence of any interaction between comorbidity count and treatment efficacy for either EQ-5D or SF-36 (EQ-5D −0.0035 [95\% CI −0.0153 to −0.0065], PBayes = 0.746; SF-36-MCS (−0.0111 [95\% CI −0.0647 to 0.0416], PBayes = 0.70 and SF-36-PCS −0.0092 [95\% CI −0.0758 to 0.0476], PBayes = 0.631. Conclusions Treatment effects on quality of life did not differ by multimorbidity (measured via a comorbidity count) at baseline—for the medical conditions studied, types and severity of comorbidities and level of quality of life at baseline, suggesting that evidence from clinical trials is likely to be applicable to settings with (at least modestly) higher levels of comorbidity. Trial registration A prespecified protocol was registered on PROSPERO (CRD42018048202).},
	language = {en},
	number = {1},
	urldate = {2024-06-29},
	journal = {PLOS Medicine},
	author = {Butterly, Elaine W. and Hanlon, Peter and Shah, Anoop S. V. and Hannigan, Laurie J. and McIntosh, Emma and Lewsey, Jim and Wild, Sarah H. and Guthrie, Bruce and Mair, Frances S. and Kent, David M. and Dias, Sofia and Welton, Nicky J. and McAllister, David A.},
	month = jan,
	year = {2023},
	note = {Publisher: Public Library of Science},
	keywords = {Bayesian, trial design, brms},
	pages = {e1004154},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\J6KM73K5\\Butterly et al. - 2023 - Comorbidity and health-related quality of life in .pdf:application/pdf},
}

@article{noauthor_bayesian_2022,
	title = {Bayesian {Survival} {Analysis} of {Acute} {Encephalitis} {Syndrome} with {Censoring} {Mechanism} using {Brms} {Package}},
	volume = {11},
	issn = {20908423, 20908431},
	url = {https://www.naturalspublishing.com/Article.asp?ArtcID=25525},
	doi = {10.18576/jsap/110319},
	abstract = {Acute encephalitis syndrome(AES) most commonly affects children and young adults and can lead to considerable morbidity and mortality. In June 2019, the outbreak of acute encephalitis syndrome occurred in Muzaffarpur district and their neighbouring district of Bihar. This paper presents the Bayesian survival analysis of AES data of the Muzaffarpur district. AES data extracted from the SKMCH and KM hospital of Muzaffarpur. The Weibull, Log-normal, and Exponential, these survival models have been used for ﬁtting of AES data with the help of brms packages of R and compared these models with the Leave one out cross-validation. brms package uses the Hamiltonian Monte Carlo(HMC) sampler and its extension, no-U-turn sampler (NUTS) algorithm of MCMC, for the simulation study. In addition, the Logistic regression model is used to predict the risk of death on the basis of observed characteristics or covariates.},
	language = {en},
	number = {3},
	urldate = {2024-06-29},
	journal = {J. Stat. Appl. Pro.},
	month = sep,
	year = {2022},
	keywords = {R, survival analysis, Bayesian, brms},
	pages = {963--980},
	file = {2022 - Bayesian Survival Analysis of Acute Encephalitis S.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\K92PXPS8\\2022 - Bayesian Survival Analysis of Acute Encephalitis S.pdf:application/pdf},
}

@article{falk_multilevel_2024,
	title = {Multilevel mediation analysis in {R}: {A} comparison of bootstrap and {Bayesian} approaches},
	volume = {56},
	issn = {1554-3528},
	shorttitle = {Multilevel mediation analysis in {R}},
	url = {https://doi.org/10.3758/s13428-023-02079-4},
	doi = {10.3758/s13428-023-02079-4},
	abstract = {Mediation analysis in repeated measures studies can shed light on the mechanisms through which experimental manipulations change the outcome variable. However, the literature on interval estimation for the indirect effect in the 1-1-1 single mediator model is sparse. Most simulation studies to date evaluating mediation analysis in multilevel data considered scenarios that do not match the expected numbers of level 1 and level 2 units typically encountered in experimental studies, and no study to date has compared resampling and Bayesian methods for constructing intervals for the indirect effect in this context. We conducted a simulation study to compare statistical properties of interval estimates of the indirect effect obtained using four bootstrap and two Bayesian methods in the 1-1-1 mediation model with and without random effects. Bayesian credibility intervals had coverage closest to the nominal value and no instances of excessive Type I error rates, but lower power than resampling methods. Findings indicated that the pattern of performance for resampling methods often depended on the presence of random effects. We provide suggestions for selecting an interval estimator for the indirect effect depending on the most important statistical property for a given study, as well as code in R for implementing all methods evaluated in the simulation study. Findings and code from this project will hopefully support the use of mediation analysis in experimental research with repeated measures.},
	language = {en},
	number = {2},
	urldate = {2024-06-29},
	journal = {Behav Res},
	author = {Falk, Carl F. and Vogel, Todd A. and Hammami, Sarah and Miočević, Milica},
	month = feb,
	year = {2024},
	keywords = {R, Bayesian, mediation},
	pages = {750--764},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\7YLMJDR4\\Falk et al. - 2024 - Multilevel mediation analysis in R A comparison o.pdf:application/pdf},
}

@article{katona_assessment_2022,
	title = {Assessment of variation in long-term outcomes of integrated care initiatives in {Dutch} health care},
	volume = {25},
	issn = {2053-4345},
	url = {https://doi.org/10.1177/20534345221109429},
	doi = {10.1177/20534345221109429},
	abstract = {Introduction
The care for many patients with diabetes mellitus type 2 in the Netherlands, is contracted by a local care group. The healthcare providers, who collectively shape a care group, provide protocolled diabetes care. Differences exist between care groups in terms of their organizational and financial arrangements. These differences may result in variation in outcomes. The aim of this study is to assess whether variation in healthcare costs, diabetes complications and related hospital admissions on the level of care groups exist.
Methods
A quantitative cohort study was conducted. Patients who used diabetes medication (more than 180 days of defined daily doses per year) for the first time between the years 2014 and 2019 were included. Data were extracted from health insurance claims between 2014 and 2019. Generalized linear mixed models were used to analyse patient variation in healthcare costs (two and six years follow-up), diabetes-related complications and hospital admission days. Intraclass correlation coefficients were calculated to estimate the amount of variation that was attributable to the care groups.
Results
A large variation in outcome variables was observed between patients and a small variation between care groups. The intraclass correlation coefficient for long-term costs was 0.4\%; for short-term costs between 0.1\% and 0.3\%; for complications 1\% and for hospital days 4\%.
Discussion
A large variation between patients with diabetes mellitus type 2 exists in terms of their healthcare costs and complications. In our study, care groups accounted minimally for this variation. A generalized linear mixed model in combination with year cohorts is a tool to study variations in the long-term outcomes of integrated care initiatives.},
	language = {en},
	number = {2-3},
	urldate = {2024-06-29},
	journal = {International Journal of Care Coordination},
	author = {Katona, Katalin and Menting, Malou Dorine and Pisters, Ylva Michelle},
	month = jun,
	year = {2022},
	note = {Publisher: SAGE Publications},
	keywords = {Bayesian, brms example writing},
	pages = {86--97},
	file = {SAGE PDF Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\VUA7WYUG\\Katona et al. - 2022 - Assessment of variation in long-term outcomes of i.pdf:application/pdf},
}

@article{brooks_general_1998,
	title = {General {Methods} for {Monitoring} {Convergence} of {Iterative} {Simulations}},
	volume = {7},
	issn = {1061-8600},
	url = {https://www.tandfonline.com/doi/abs/10.1080/10618600.1998.10474787},
	doi = {10.1080/10618600.1998.10474787},
	abstract = {We generalize the method proposed by Gelman and Rubin (1992a) for monitoring the convergence of iterative simulations by comparing between and within variances of multiple chains, in order to obtain a family of tests for convergence. We review methods of inference from simulations in order to develop convergence-monitoring summaries that are relevant for the purposes for which the simulations are used. We recommend applying a battery of tests for mixing based on the comparison of inferences from individual sequences and from the mixture of sequences. Finally, we discuss multivariate analogues, for assessing convergence of several parameters simultaneously.},
	number = {4},
	urldate = {2024-06-25},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Brooks, Stephen P. and Gelman, Andrew},
	month = dec,
	year = {1998},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/10618600.1998.10474787},
	keywords = {Bayesian},
	pages = {434--455},
	file = {Brooks and Gelman - 1998 - General Methods for Monitoring Convergence of Iter.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\N6PT3FZK\\Brooks and Gelman - 1998 - General Methods for Monitoring Convergence of Iter.pdf:application/pdf},
}

@incollection{brooks_inference_2011,
	address = {New York},
	edition = {1},
	title = {Inference from {Simulations} and {Monitoring} {Convergence}},
	isbn = {978-0-429-13850-8},
	url = {https://www.taylorfrancis.com/books/9780429138508/chapters/10.1201/b10905-7},
	language = {en},
	urldate = {2024-06-25},
	booktitle = {Handbook of {Markov} {Chain} {Monte} {Carlo}},
	publisher = {Chapman and Hall/CRC},
	author = {Gelman, Andrew and Shirley, Kenneth},
	collaborator = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
	month = may,
	year = {2011},
	doi = {10.1201/b10905-7},
	keywords = {Bayesian},
	pages = {162--174},
	file = {Gelman and Shirley - 2011 - Inference from Simulations and Monitoring Converge.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\A49YE7ZY\\Gelman and Shirley - 2011 - Inference from Simulations and Monitoring Converge.pdf:application/pdf},
}

@article{burkner_ordinal_2019,
	title = {Ordinal {Regression} {Models} in {Psychology}: {A} {Tutorial}},
	volume = {2},
	issn = {2515-2459},
	shorttitle = {Ordinal {Regression} {Models} in {Psychology}},
	url = {https://doi.org/10.1177/2515245918823199},
	doi = {10.1177/2515245918823199},
	abstract = {Ordinal variables, although extremely common in psychology, are almost exclusively analyzed with statistical models that falsely assume them to be metric. This practice can lead to distorted effect-size estimates, inflated error rates, and other problems. We argue for the application of ordinal models that make appropriate assumptions about the variables under study. In this Tutorial, we first explain the three major classes of ordinal models: the cumulative, sequential, and adjacent-category models. We then show how to fit ordinal models in a fully Bayesian framework with the R package brms, using data sets on opinions about stem-cell research and time courses of marriage. The appendices provide detailed mathematical derivations of the models and a discussion of censored ordinal models. Compared with metric models, ordinal models provide better theoretical interpretation and numerical inference from ordinal data, and we recommend their widespread adoption in psychology.},
	language = {en},
	number = {1},
	urldate = {2024-06-21},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Bürkner, Paul-Christian and Vuorre, Matti},
	month = mar,
	year = {2019},
	note = {Publisher: SAGE Publications Inc},
	keywords = {R, Bayesian},
	pages = {77--101},
	file = {SAGE PDF Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\H64YXC29\\Bürkner and Vuorre - 2019 - Ordinal Regression Models in Psychology A Tutoria.pdf:application/pdf},
}

@article{huber_direct_2020,
	title = {Direct and indirect effects of continuous treatments based on generalized propensity score weighting},
	volume = {35},
	copyright = {© 2020 John Wiley \& Sons, Ltd.},
	issn = {1099-1255},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.2765},
	doi = {10.1002/jae.2765},
	abstract = {This paper proposes semi- and nonparametric methods for disentangling the total causal effect of a continuous treatment on an outcome variable into its natural direct effect and the indirect effect that operates through one or several intermediate variables called mediators jointly. Our approach is based on weighting observations by the inverse of two versions of the generalized propensity score (GPS), namely the conditional density of treatment either given observed covariates or given covariates and the mediator. Our effect estimators are shown to be asymptotically normal when the GPS is estimated by either a parametric or a nonparametric kernel-based method. We also provide a simulation study and an empirical illustration based on the Job Corps experimental study.},
	language = {en},
	number = {7},
	urldate = {2024-06-10},
	journal = {Journal of Applied Econometrics},
	author = {Huber, Martin and Hsu, Yu-Chin and Lee, Ying-Ying and Lettry, Layal},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jae.2765},
	keywords = {causal inference, propensity score, mediation},
	pages = {814--840},
	file = {Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\6AING78G\\jae.html:text/html;Submitted Version:C\:\\Users\\anbe6\\Zotero\\storage\\TZMBHNWQ\\Huber et al. - 2020 - Direct and indirect effects of continuous treatmen.pdf:application/pdf},
}

@article{yu_multilevel_2021,
	title = {Multilevel mediation analysis on time-to-event outcomes: {Exploring} racial/ethnic disparities in breast cancer survival in {California}},
	volume = {2},
	issn = {2632-0843},
	shorttitle = {Multilevel mediation analysis on time-to-event outcomes},
	url = {https://doi.org/10.1177/26320843211061292},
	doi = {10.1177/26320843211061292},
	abstract = {Background
Third-variable effect refers to the effect from a third-variable that explains an observed relationship between an exposure and an outcome. Depending on whether there is a causal relationship from the exposure to the third variable, the third-variable is called a mediator or a confounder. The multilevel mediation analysis is used to differentiate third-variable effects from data of hierarchical structures.
Data Collection and Analysis
We developed a multilevel mediation analysis method to deal with time-to-event outcomes and implemented the method in the mlma R package. With the method, third-variable effects from different levels of data can be estimated. The method uses multilevel additive models that allow for transformations of variables to take into account potential nonlinear relationships among variables in the mediation analysis. We apply the proposed method to explore the racial/ethnic disparities in survival among patients diagnosed with breast cancer in California between 2006 and 2017, using both individual risk factors and census tract level environmental factors. The individual risk factors are collected by cancer registries and the census tract level factors are collected by the Public Health Alliance of Southern California in partnership with the Virginia Commonwealth University's Center on Society and Health. The National Cancer Institute work group linked variables at the census tract level with each patient and performed the analysis for this study.
Results
We found that the racial disparity in survival were mostly explained at the census tract level and partially explained at the individual level. The associations among variables were depicted. Conclusion: The multilevel mediation analysis method can be used to differentiate mediation/confounding effects for factors originated from different levels. The method is implemented in the R package mlma.},
	language = {en},
	number = {4},
	urldate = {2024-06-07},
	journal = {Research Methods in Medicine \& Health Sciences},
	author = {Yu, Qingzhao and Yu, Mandi and Zou, Joe and Wu, Xiaocheng and Gomez, Scarlett L and Li, Bin},
	month = sep,
	year = {2021},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {causal inference, mediation, disparity},
	pages = {157--167},
	file = {SAGE PDF Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\8XKG9996\\Yu et al. - 2021 - Multilevel mediation analysis on time-to-event out.pdf:application/pdf},
}

@article{imai_identification_2013,
	title = {Identification and {Sensitivity} {Analysis} for {Multiple} {Causal} {Mechanisms}: {Revisiting} {Evidence} from {Framing} {Experiments}},
	volume = {21},
	copyright = {https://www.cambridge.org/core/terms},
	issn = {1047-1987, 1476-4989},
	shorttitle = {Identification and {Sensitivity} {Analysis} for {Multiple} {Causal} {Mechanisms}},
	url = {https://www.cambridge.org/core/product/identifier/S1047198700013334/type/journal_article},
	doi = {10.1093/pan/mps040},
	abstract = {Social scientists are often interested in testing multiple causal mechanisms through which a treatment affects outcomes. A predominant approach has been to use linear structural equation models and examine the statistical significance of the corresponding path coefficients. However, this approach implicitly assumes that the multiple mechanisms are causally independent of one another. In this article, we consider a set of alternative assumptions that are sufficient to identify the average causal mediation effects when multiple, causally related mediators exist. We develop a new sensitivity analysis for examining the robustness of empirical findings to the potential violation of a key identification assumption. We apply the proposed methods to three political psychology experiments, which examine alternative causal pathways between media framing and public opinion. Our analysis reveals that the validity of original conclusions is highly reliant on the assumed independence of alternative causal mechanisms, highlighting the importance of proposed sensitivity analysis. All of the proposed methods can be implemented via an open source R package,
              mediation
              .},
	language = {en},
	number = {2},
	urldate = {2024-06-07},
	journal = {Polit. anal.},
	author = {Imai, Kosuke and Yamamoto, Teppei},
	year = {2013},
	keywords = {causal inference, mediation},
	pages = {141--171},
	file = {Imai and Yamamoto - 2013 - Identification and Sensitivity Analysis for Multip.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\JHJ8IXEI\\Imai and Yamamoto - 2013 - Identification and Sensitivity Analysis for Multip.pdf:application/pdf},
}

@article{imai_identification_2010,
	title = {Identification, {Inference} and {Sensitivity} {Analysis} for {Causal} {Mediation} {Effects}},
	volume = {25},
	issn = {0883-4237},
	url = {https://projecteuclid.org/journals/statistical-science/volume-25/issue-1/Identification-Inference-and-Sensitivity-Analysis-for-Causal-Mediation-Effects/10.1214/10-STS321.full},
	doi = {10.1214/10-STS321},
	abstract = {Causal mediation analysis is routinely conducted by applied researchers in a variety of disciplines. The goal of such an analysis is to investigate alternative causal mechanisms by examining the roles of intermediate variables that lie in the causal paths between the treatment and outcome variables. In this paper we ﬁrst prove that under a particular version of sequential ignorability assumption, the average causal mediation effect (ACME) is nonparametrically identiﬁed. We compare our identiﬁcation assumption with those proposed in the literature. Some practical implications of our identiﬁcation result are also discussed. In particular, the popular estimator based on the linear structural equation model (LSEM) can be interpreted as an ACME estimator once additional parametric assumptions are made. We show that these assumptions can easily be relaxed within and outside of the LSEM framework and propose simple nonparametric estimation strategies. Second, and perhaps most importantly, we propose a new sensitivity analysis that can be easily implemented by applied researchers within the LSEM framework. Like the existing identifying assumptions, the proposed sequential ignorability assumption may be too strong in many applied settings. Thus, sensitivity analysis is essential in order to examine the robustness of empirical ﬁndings to the possible existence of an unmeasured confounder. Finally, we apply the proposed methods to a randomized experiment from political psychology. We also make easy-to-use software available to implement the proposed methods.},
	language = {en},
	number = {1},
	urldate = {2024-06-06},
	journal = {Statist. Sci.},
	author = {Imai, Kosuke and Keele, Luke and Yamamoto, Teppei},
	month = feb,
	year = {2010},
	keywords = {causal inference, mediation},
	file = {Imai et al. - 2010 - Identification, Inference and Sensitivity Analysis.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\8F7RXAPP\\Imai et al. - 2010 - Identification, Inference and Sensitivity Analysis.pdf:application/pdf},
}

@article{quan_coding_2005,
	title = {Coding {Algorithms} for {Defining} {Comorbidities} in {ICD}-9-{CM} and {ICD}-10 {Administrative} {Data}},
	volume = {43},
	issn = {0025-7079},
	url = {https://journals.lww.com/lww-medicalcare/abstract/2005/11000/coding_algorithms_for_defining_comorbidities_in.10.aspx},
	doi = {10.1097/01.mlr.0000182534.19832.83},
	abstract = {Objectives: 
          Implementation of the International Statistical Classification of Disease and Related Health Problems, 10th Revision (ICD-10) coding system presents challenges for using administrative data. Recognizing this, we conducted a multistep process to develop ICD-10 coding algorithms to define Charlson and Elixhauser comorbidities in administrative data and assess the performance of the resulting algorithms.
          Methods: 
          ICD-10 coding algorithms were developed by “translation” of the ICD-9-CM codes constituting Deyo's (for Charlson comorbidities) and Elixhauser's coding algorithms and by physicians’ assessment of the face-validity of selected ICD-10 codes. The process of carefully developing ICD-10 algorithms also produced modified and enhanced ICD-9-CM coding algorithms for the Charlson and Elixhauser comorbidities. We then used data on in-patients aged 18 years and older in ICD-9-CM and ICD-10 administrative hospital discharge data from a Canadian health region to assess the comorbidity frequencies and mortality prediction achieved by the original ICD-9-CM algorithms, the enhanced ICD-9-CM algorithms, and the new ICD-10 coding algorithms.
          Results: 
          Among 56,585 patients in the ICD-9-CM data and 58,805 patients in the ICD-10 data, frequencies of the 17 Charlson comorbidities and the 30 Elixhauser comorbidities remained generally similar across algorithms. The new ICD-10 and enhanced ICD-9-CM coding algorithms either matched or outperformed the original Deyo and Elixhauser ICD-9-CM coding algorithms in predicting in-hospital mortality. The C-statistic was 0.842 for Deyo's ICD-9-CM coding algorithm, 0.860 for the ICD-10 coding algorithm, and 0.859 for the enhanced ICD-9-CM coding algorithm, 0.868 for the original Elixhauser ICD-9-CM coding algorithm, 0.870 for the ICD-10 coding algorithm and 0.878 for the enhanced ICD-9-CM coding algorithm.
          Conclusions: 
          These newly developed ICD-10 and ICD-9-CM comorbidity coding algorithms produce similar estimates of comorbidity prevalence in administrative data, and may outperform existing ICD-9-CM coding algorithms.},
	language = {en-US},
	number = {11},
	urldate = {2024-06-05},
	journal = {Medical Care},
	author = {Quan, Hude and Sundararajan, Vijaya and Halfon, Patricia and Fong, Andrew and Burnand, Bernard and Luthi, Jean-Christophe and Saunders, L. Duncan and Beck, Cynthia A. and Feasby, Thomas E. and Ghali, William A.},
	month = nov,
	year = {2005},
	keywords = {palliative care},
	pages = {1130},
	file = {Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\67RMCAM6\\coding_algorithms_for_defining_comorbidities_in.10.html:text/html},
}

@article{yu_mma_2017,
	title = {mma: {An} {R} {Package} for {Mediation} {Analysis} with {Multiple} {Mediators}},
	volume = {5},
	copyright = {http://creativecommons.org/licenses/by/4.0},
	issn = {2049-9647},
	shorttitle = {mma},
	url = {https://openresearchsoftware.metajnl.com/article/10.5334/jors.160/},
	doi = {10.5334/jors.160},
	language = {en},
	number = {1},
	urldate = {2024-06-04},
	journal = {JORS},
	author = {Yu, Qingzhao and Li, Bin},
	month = apr,
	year = {2017},
	keywords = {R, causal inference, mediation},
	pages = {11},
	file = {Yu and Li - 2017 - mma An R Package for Mediation Analysis with Mult.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\LDW9I876\\Yu and Li - 2017 - mma An R Package for Mediation Analysis with Mult.pdf:application/pdf},
}

@article{tingley_mediation_2014,
	title = {\textbf{mediation} : \textit{{R}} {Package} for {Causal} {Mediation} {Analysis}},
	volume = {59},
	issn = {1548-7660},
	shorttitle = {\textbf{mediation}},
	url = {http://www.jstatsoft.org/v59/i05/},
	doi = {10.18637/jss.v059.i05},
	abstract = {In this paper, we describe the R package mediation for conducting causal mediation analysis in applied empirical research. In many scientiﬁc disciplines, the goal of researchers is not only estimating causal eﬀects of a treatment but also understanding the process in which the treatment causally aﬀects the outcome. Causal mediation analysis is frequently used to assess potential causal mechanisms. The mediation package implements a comprehensive suite of statistical tools for conducting such an analysis. The package is organized into two distinct approaches. Using the model-based approach, researchers can estimate causal mediation eﬀects and conduct sensitivity analysis under the standard research design. Furthermore, the design-based approach provides several analysis tools that are applicable under diﬀerent experimental designs. This approach requires weaker assumptions than the model-based approach. We also implement a statistical method for dealing with multiple (causally dependent) mediators, which are often encountered in practice. Finally, the package also oﬀers a methodology for assessing causal mediation in the presence of treatment noncompliance, a common problem in randomized trials.},
	language = {en},
	number = {5},
	urldate = {2024-06-04},
	journal = {J. Stat. Soft.},
	author = {Tingley, Dustin and Yamamoto, Teppei and Hirose, Kentaro and Keele, Luke and Imai, Kosuke},
	year = {2014},
	keywords = {R, causal inference, mediation},
	file = {Tingley et al. - 2014 - mediation  R Package for Causal Med.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\7GMQCE7F\\Tingley et al. - 2014 - mediation  R Package for Causal Med.pdf:application/pdf},
}

@article{dallow_better_2018,
	title = {Better decision making in drug development through adoption of formal prior elicitation},
	volume = {17},
	copyright = {Copyright © 2018 John Wiley \& Sons, Ltd.},
	issn = {1539-1612},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pst.1854},
	doi = {10.1002/pst.1854},
	abstract = {With the continued increase in the use of Bayesian methods in drug development, there is a need for statisticians to have tools to develop robust and defensible informative prior distributions. Whilst relevant empirical data should, where possible, provide the basis for such priors, it is often the case that limitations in data and/or our understanding may preclude direct construction of a data-based prior. Formal expert elicitation methods are a key technique that can be used to determine priors in these situations. Within GlaxoSmithKline, we have adopted a structured approach to prior elicitation on the basis of the SHELF elicitation framework and routinely use this in conjunction with calculation of probability of success (assurance) of the next study(s) to inform internal decision making at key project milestones. The aim of this paper is to share our experiences of embedding the use of prior elicitation within a large pharmaceutical company, highlighting both the benefits and challenges of prior elicitation through a series of case studies. We have found that putting team beliefs into the shape of a quantitative probability distribution provides a firm anchor for all internal decision making, enabling teams to provide investment boards with formally appropriate estimates of the probability of trial success as well as robust plans for interim decision rules where appropriate. As an added benefit, the elicitation process provides transparency about the beliefs and risks of the potential medicine, ultimately enabling better portfolio and company-wide decision making.},
	language = {en},
	number = {4},
	urldate = {2024-06-03},
	journal = {Pharmaceutical Statistics},
	author = {Dallow, Nigel and Best, Nicky and Montague, Timothy H},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/pst.1854},
	keywords = {Bayesian, trial design},
	pages = {301--316},
	file = {Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\97XDGN7J\\pst.html:text/html;Submitted Version:C\:\\Users\\anbe6\\Zotero\\storage\\3GSEFWBG\\Dallow et al. - 2018 - Better decision making in drug development through.pdf:application/pdf;Submitted Version:C\:\\Users\\anbe6\\Zotero\\storage\\FBF7N287\\Dallow et al. - 2018 - Better decision making in drug development through.pdf:application/pdf},
}

@article{zugna_applied_2022,
	title = {Applied causal inference methods for sequential mediators},
	volume = {22},
	issn = {1471-2288},
	url = {https://doi.org/10.1186/s12874-022-01764-w},
	doi = {10.1186/s12874-022-01764-w},
	abstract = {Mediation analysis aims at estimating to what extent the effect of an exposure on an outcome is explained by a set of mediators on the causal pathway between the exposure and the outcome. The total effect of the exposure on the outcome can be decomposed into an indirect effect, i.e. the effect explained by the mediators jointly, and a direct effect, i.e. the effect unexplained by the mediators. However finer decompositions are possible in presence of independent or sequential mediators.},
	number = {1},
	urldate = {2024-06-03},
	journal = {BMC Medical Research Methodology},
	author = {Zugna, D. and Popovic, M. and Fasanelli, F. and Heude, B. and Scelo, G. and Richiardi, L.},
	month = nov,
	year = {2022},
	keywords = {causal inference, mediation},
	pages = {301},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\8S6RM85K\\Zugna et al. - 2022 - Applied causal inference methods for sequential me.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\2I4BRVJQ\\s12874-022-01764-w.html:text/html},
}

@article{rijnhart_mediation_2021,
	title = {Mediation analysis methods used in observational research: a scoping review and recommendations},
	volume = {21},
	issn = {1471-2288},
	shorttitle = {Mediation analysis methods used in observational research},
	url = {https://doi.org/10.1186/s12874-021-01426-3},
	doi = {10.1186/s12874-021-01426-3},
	abstract = {Mediation analysis methodology underwent many advancements throughout the years, with the most recent and important advancement being the development of causal mediation analysis based on the counterfactual framework. However, a previous review showed that for experimental studies the uptake of causal mediation analysis remains low. The aim of this paper is to review the methodological characteristics of mediation analyses performed in observational epidemiologic studies published between 2015 and 2019 and to provide recommendations for the application of mediation analysis in future studies.},
	number = {1},
	urldate = {2024-06-03},
	journal = {BMC Medical Research Methodology},
	author = {Rijnhart, Judith J. M. and Lamp, Sophia J. and Valente, Matthew J. and MacKinnon, David P. and Twisk, Jos W. R. and Heymans, Martijn W.},
	month = oct,
	year = {2021},
	keywords = {causal inference, mediation},
	pages = {226},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\RFYH3F7R\\Rijnhart et al. - 2021 - Mediation analysis methods used in observational r.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\7GIHV8VD\\s12874-021-01426-3.html:text/html},
}

@article{rudolph_causal_2019,
	title = {Causal {Mediation} {Analysis} {With} {Observational} {Data}: {Considerations} and {Illustration} {Examining} {Mechanisms} {Linking} {Neighborhood} {Poverty} to {Adolescent} {Substance} {Use}},
	volume = {188},
	issn = {0002-9262},
	shorttitle = {Causal {Mediation} {Analysis} {With} {Observational} {Data}},
	url = {https://doi.org/10.1093/aje/kwy248},
	doi = {10.1093/aje/kwy248},
	abstract = {Understanding the mediation mechanisms by which an exposure or intervention affects an outcome can provide a look into what has been called a “black box” of many epidemiologic associations, thereby providing further evidence of a relationship and possible points of intervention. Rapid methodologic developments in mediation analyses mean that there are a growing number of approaches for researchers to consider, each with its own set of assumptions, advantages, and disadvantages. This has understandably resulted in some confusion among applied researchers. Here, we provide a brief overview of the mediation methods available and discuss points for consideration when choosing a method. We provide an in-depth explication of 2 of the many potential estimators for illustrative purposes: the Baron and Kenny mediation approach, because it is the most commonly used, and a recently developed approach for estimating stochastic direct and indirect effects, because it relies on far fewer assumptions. We illustrate the decision process and analytical procedure by estimating potential school- and peer-based mechanisms linking neighborhood poverty to adolescent substance use in the National Comorbidity Survey Adolescent Supplement.},
	number = {3},
	urldate = {2024-06-03},
	journal = {American Journal of Epidemiology},
	author = {Rudolph, Kara E and Goin, Dana E and Paksarian, Diana and Crowder, Rebecca and Merikangas, Kathleen R and Stuart, Elizabeth A},
	month = mar,
	year = {2019},
	keywords = {causal inference, mediation},
	pages = {598--608},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\LNNQGEU8\\Rudolph et al. - 2019 - Causal Mediation Analysis With Observational Data.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\Q4GRPKZC\\5252009.html:text/html},
}

@article{steen_flexible_2017,
	title = {Flexible {Mediation} {Analysis} {With} {Multiple} {Mediators}},
	volume = {186},
	issn = {0002-9262, 1476-6256},
	url = {https://academic.oup.com/aje/article/186/2/184/3791460},
	doi = {10.1093/aje/kwx051},
	language = {en},
	number = {2},
	urldate = {2024-06-03},
	journal = {American Journal of Epidemiology},
	author = {Steen, Johan and Loeys, Tom and Moerkerke, Beatrijs and Vansteelandt, Stijn},
	month = jul,
	year = {2017},
	keywords = {causal inference, mediation},
	pages = {184--193},
	file = {Steen et al. - 2017 - Flexible Mediation Analysis With Multiple Mediator.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\DISU9MF3\\Steen et al. - 2017 - Flexible Mediation Analysis With Multiple Mediator.pdf:application/pdf},
}

@article{vanderweele_mediation_2016,
	title = {Mediation {Analysis}: {A} {Practitioner}'s {Guide}},
	volume = {37},
	issn = {0163-7525, 1545-2093},
	shorttitle = {Mediation {Analysis}},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-publhealth-032315-021402},
	doi = {10.1146/annurev-publhealth-032315-021402},
	abstract = {This article provides an overview of recent developments in mediation analysis, that is, analyses used to assess the relative magnitude of different pathways and mechanisms by which an exposure may affect an outcome. Traditional approaches to mediation in the biomedical and social sciences are described. Attention is given to the confounding assumptions required for a causal interpretation of direct and indirect effect estimates. Methods from the causal inference literature to conduct mediation in the presence of exposuremediator interactions, binary outcomes, binary mediators, and case-control study designs are presented. Sensitivity analysis techniques for unmeasured confounding and measurement error are introduced. Discussion is given to extensions to time-to-event outcomes and multiple mediators. Further ﬂexible modeling strategies arising from the precise counterfactual deﬁnitions of direct and indirect effects are also described. The focus throughout is on methodology that is easily implementable in practice across a broad range of potential applications.},
	language = {en},
	number = {1},
	urldate = {2024-06-03},
	journal = {Annu. Rev. Public Health},
	author = {VanderWeele, Tyler J.},
	month = mar,
	year = {2016},
	keywords = {causal inference, mediation},
	pages = {17--32},
	file = {VanderWeele - 2016 - Mediation Analysis A Practitioner's Guide.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\GNGGAZRU\\VanderWeele - 2016 - Mediation Analysis A Practitioner's Guide.pdf:application/pdf},
}

@article{valeri_mediation_2013,
	title = {Mediation analysis allowing for exposure–mediator interactions and causal interpretation: {Theoretical} assumptions and implementation with {SAS} and {SPSS} macros.},
	volume = {18},
	copyright = {http://www.apa.org/pubs/journals/resources/open-access.aspx},
	issn = {1939-1463, 1082-989X},
	shorttitle = {Mediation analysis allowing for exposure–mediator interactions and causal interpretation},
	url = {https://doi.apa.org/doi/10.1037/a0031034},
	doi = {10.1037/a0031034},
	abstract = {Mediation analysis is a useful and widely employed approach to studies in the field of psychology and in the social and biomedical sciences. The contributions of this article are several-fold. First we seek to bring the developments in mediation analysis for nonlinear models within the counterfactual framework to the psychology audience in an accessible format and compare the sorts of inferences about mediation that are possible in the presence of exposure–mediator interaction when using a counterfactual versus the standard statistical approach. Second, the work by VanderWeele and Vansteelandt (2009, 2010) is extended here to allow for dichotomous mediators and count outcomes. Third, we provide SAS and SPSS macros to implement all of these mediation analysis techniques automatically, and we compare the types of inferences about mediation that are allowed by a variety of software macros.},
	language = {en},
	number = {2},
	urldate = {2024-06-03},
	journal = {Psychological Methods},
	author = {Valeri, Linda and VanderWeele, Tyler J.},
	month = jun,
	year = {2013},
	keywords = {causal inference, mediation},
	pages = {137--150},
	file = {Valeri and VanderWeele - 2013 - Mediation analysis allowing for exposure–mediator .pdf:C\:\\Users\\anbe6\\Zotero\\storage\\QIS2JMJU\\Valeri and VanderWeele - 2013 - Mediation analysis allowing for exposure–mediator .pdf:application/pdf},
}

@article{vanderweele_mediation_2009,
	title = {Mediation and {Mechanism}},
	volume = {24},
	issn = {0393-2990},
	url = {https://www.jstor.org/stable/40284115},
	abstract = {The concepts of mediation and mechanism are contrasted and logical implications holding between theses two concepts are described. The concept of mediation can be formalized using counterfactual definitions of indirect effects; the concept of mechanism can be formalized within the sufficient cause framework. It is shown that both concepts can be illustrated using a single causal diagram. It is also shown that mediation implies mechanism but mechanism need not imply mediation. Discussion is given regarding how the distinction between "statistical causality" and "mechanistic causality" is blurred by recent work in causal inference concerning methods for testing for mediation and mechanism.},
	number = {5},
	urldate = {2024-06-03},
	journal = {European Journal of Epidemiology},
	author = {VanderWeele, Tyler J.},
	year = {2009},
	note = {Publisher: Springer},
	keywords = {causal inference, mediation},
	pages = {217--224},
	file = {VanderWeele - 2009 - Mediation and Mechanism.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\YFRZIZC3\\VanderWeele - 2009 - Mediation and Mechanism.pdf:application/pdf},
}

@article{vanderweele_three-way_2013,
	title = {A {Three}-way {Decomposition} of a {Total} {Effect} into {Direct}, {Indirect}, and {Interactive} {Effects}},
	volume = {24},
	issn = {1044-3983},
	url = {https://journals.lww.com/epidem/fulltext/2013/03000/a_three_way_decomposition_of_a_total_effect_into.9.aspx},
	doi = {10.1097/EDE.0b013e318281a64e},
	abstract = {Recent theory in causal inference has provided concepts for mediation analysis and effect decomposition that allow one to decompose a total effect into a direct and an indirect effect. Here, it is shown that what is often taken as an indirect effect can in fact be further decomposed into a “pure” indirect effect and a mediated interactive effect, thus yielding a three-way decomposition of a total effect (direct, indirect, and interactive). This three-way decomposition applies to difference scales and also to additive ratio scales and additive hazard scales. Assumptions needed for the identification of each of these three effects are discussed and simple formulae are given for each when regression models allowing for interaction are used. The three-way decomposition is illustrated by examples from genetic and perinatal epidemiology, and discussion is given to what is gained over the traditional two-way decomposition into a direct and an indirect effect.},
	language = {en-US},
	number = {2},
	urldate = {2024-06-03},
	journal = {Epidemiology},
	author = {VanderWeele, Tyler J.},
	month = mar,
	year = {2013},
	keywords = {causal inference, mediation},
	pages = {224},
	file = {Accepted Version:C\:\\Users\\anbe6\\Zotero\\storage\\LLBRYAHX\\VanderWeele - 2013 - A Three-way Decomposition of a Total Effect into D.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\3BLGSY4Y\\a_three_way_decomposition_of_a_total_effect_into.9.html:text/html},
}

@article{pearl_statistics_2003,
	title = {Statistics and causal inference: {A} review},
	volume = {12},
	copyright = {http://www.springer.com/tdm},
	issn = {1133-0686, 1863-8260},
	shorttitle = {Statistics and causal inference},
	url = {http://link.springer.com/10.1007/BF02595718},
	doi = {10.1007/BF02595718},
	abstract = {This paper aims at assisting empirical researchers beneﬁt from recent advances in causal inference. The paper stresses the paradigmatic shifts that must be undertaken in moving from traditional statistical analysis to causal analysis of multivariate data. Special emphasis is placed on the assumptions that underly all causal inferences, the languages used in formulating those assumptions, and the conditional nature of causal claims inferred from nonexperimental studies. These emphases are illustrated through a brief survey of recent results, including the control of confounding, the assessment of causal eﬀects, the interpretation of counterfactuals, and a symbiosis between counterfactual and graphical methods of analysis.},
	language = {en},
	number = {2},
	urldate = {2024-06-03},
	journal = {Test},
	author = {Pearl, Judea},
	month = dec,
	year = {2003},
	keywords = {causal inference},
	pages = {281--345},
	file = {Pearl - 2003 - Statistics and causal inference A review.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\N5PQCD5E\\Pearl - 2003 - Statistics and causal inference A review.pdf:application/pdf},
}

@article{robins_identifiability_1992,
	title = {Identifiability and {Exchangeability} for {Direct} and {Indirect} {Effects}},
	volume = {3},
	issn = {1044-3983},
	url = {https://www.jstor.org/stable/3702894},
	abstract = {We consider the problem of separating the direct effects of an exposure from effects relayed through an intermediate variable (indirect effects). We show that adjustment for the intermediate variable, which is the most common method of estimating direct effects, can be biased. We also show that, even in a randomized crossover trial of exposure, direct and indirect effects cannot be separated without special assumptions; in other words, direct and indirect effects are not separately identifiable when only exposure is randomized. If the exposure and intermediate never interact to cause disease and if intermediate effects can be controlled, that is, blocked by a suitable intervention, then a trial randomizing both exposure and the intervention can separate direct from indirect effects. Nonetheless, the estimation must be carried out using the G-computation algorithm. Conventional adjustment methods remain biased. When exposure and the intermediate interact to cause disease, direct and indirect effects will not be separable even in a trial in which both the exposure and the intervention blocking intermediate effects are randomly assigned. Nonetheless, in such a trial, one can still estimate the fraction of exposure-induced disease that could be prevented by control of the intermediate. Even in the absence of an intervention blocking the intermediate effect, the fraction of exposure-induced disease that could be prevented by control of the intermediate can be estimated with the G-computation algorithm if data are obtained on additional confounding variables.},
	number = {2},
	urldate = {2024-06-03},
	journal = {Epidemiology},
	author = {Robins, James M. and Greenland, Sander},
	year = {1992},
	note = {Publisher: Lippincott Williams \& Wilkins},
	keywords = {causal inference, mediation},
	pages = {143--155},
	file = {Robins and Greenland - 1992 - Identifiability and Exchangeability for Direct and.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\9EMCB2R7\\Robins and Greenland - 1992 - Identifiability and Exchangeability for Direct and.pdf:application/pdf},
}

@article{whitcomb_defining_2021,
	title = {Defining, {Quantifying}, and {Interpreting} “{Noncollapsibility}” in {Epidemiologic} {Studies} of {Measures} of “{Effect}”},
	volume = {190},
	copyright = {https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model},
	issn = {0002-9262, 1476-6256},
	url = {https://academic.oup.com/aje/article/190/5/697/6030882},
	doi = {10.1093/aje/kwaa267},
	abstract = {Abstract},
	language = {en},
	number = {5},
	urldate = {2024-05-30},
	journal = {American Journal of Epidemiology},
	author = {Whitcomb, Brian W and Naimi, Ashley I},
	month = may,
	year = {2021},
	keywords = {non-collapsibility},
	pages = {697--700},
	file = {Whitcomb and Naimi - 2021 - Defining, Quantifying, and Interpreting “Noncollap.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\MJQ8NMI7\\Whitcomb and Naimi - 2021 - Defining, Quantifying, and Interpreting “Noncollap.pdf:application/pdf},
}

@article{graetz_structural_2022,
	title = {Structural {Racism} and {Quantitative} {Causal} {Inference}: {A} {Life} {Course} {Mediation} {Framework} for {Decomposing} {Racial} {Health} {Disparities}},
	volume = {63},
	issn = {0022-1465, 2150-6000},
	shorttitle = {Structural {Racism} and {Quantitative} {Causal} {Inference}},
	url = {http://journals.sagepub.com/doi/10.1177/00221465211066108},
	doi = {10.1177/00221465211066108},
	abstract = {Quantitative studies of racial health disparities often use static measures of self-reported race and conventional regression estimators, which critics argue is inconsistent with social-constructivist theories of race, racialization, and racism. We demonstrate an alternative counterfactual approach to explain how multiple racialized systems dynamically shape health over time, examining racial inequities in cardiometabolic risk in the National Longitudinal Study of Adolescent to Adult Health. This framework accounts for the dynamics of time-varying confounding and mediation that is required in operationalizing a “race” variable as part of a social process (racism) rather than a separable, individual characteristic. We decompose the observed disparity into three types of effects: a controlled direct effect (“unobserved racism”), proportions attributable to interaction (“racial discrimination”), and pure indirect effects (“emergent discrimination”). We discuss the limitations of counterfactual approaches while highlighting how they can be combined with critical theories to quantify how interlocking systems produce racial health inequities.},
	language = {en},
	number = {2},
	urldate = {2024-05-23},
	journal = {J Health Soc Behav},
	author = {Graetz, Nick and Boen, Courtney E. and Esposito, Michael H.},
	month = jun,
	year = {2022},
	keywords = {causal inference, mediation, health disparity},
	pages = {232--249},
	file = {Graetz et al. - 2022 - Structural Racism and Quantitative Causal Inferenc.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\T49TD89H\\Graetz et al. - 2022 - Structural Racism and Quantitative Causal Inferenc.pdf:application/pdf},
}

@article{wallace_r_2017,
	title = {An {R} {Package} for {G}-estimation of {Structural} {Nested} {Mean} {Models}},
	volume = {28},
	issn = {1044-3983},
	url = {https://journals.lww.com/epidem/Fulltext/2017/03000/An_R_Package_for_G_estimation_of_Structural_Nested.29.aspx},
	doi = {10.1097/EDE.0000000000000586},
	abstract = {An abstract is unavailable.},
	language = {en-US},
	number = {2},
	urldate = {2024-05-22},
	journal = {Epidemiology},
	author = {Wallace, Michael P. and Moodie, Erica E. M. and Stephens, David A.},
	month = mar,
	year = {2017},
	keywords = {R, causal inference, g-computation},
	pages = {e18},
	file = {Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\ECZP8Q63\\An_R_Package_for_G_estimation_of_Structural_Nested.29.html:text/html},
}

@article{loh_tutorial_2023,
	title = {A {Tutorial} on {Causal} {Inference} in {Longitudinal} {Data} {With} {Time}-{Varying} {Confounding} {Using} {G}-{Estimation}},
	volume = {6},
	issn = {2515-2459},
	url = {https://doi.org/10.1177/25152459231174029},
	doi = {10.1177/25152459231174029},
	abstract = {In psychological research, longitudinal study designs are often used to examine the effects of a naturally observed predictor (i.e., treatment) on an outcome over time. But causal inference of longitudinal data in the presence of time-varying confounding is notoriously challenging. In this tutorial, we introduce g-estimation, a well-established estimation strategy from the causal inference literature. G-estimation is a powerful analytic tool designed to handle time-varying confounding variables affected by treatment. We offer step-by-step guidance on implementing the g-estimation method using standard parametric regression functions familiar to psychological researchers and commonly available in statistical software. To facilitate hands-on usage, we provide software code at each step using the open-source statistical software R. All the R code presented in this tutorial are publicly available online.},
	language = {en},
	number = {3},
	urldate = {2024-05-22},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Loh, Wen Wei and Ren, Dongning},
	month = jul,
	year = {2023},
	note = {Publisher: SAGE Publications Inc},
	keywords = {causal inference, g-computation},
	pages = {25152459231174029},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\9HXCCFXW\\Loh and Ren - 2023 - A Tutorial on Causal Inference in Longitudinal Dat.pdf:application/pdf},
}

@article{saville_utility_2014,
	title = {The utility of {Bayesian} predictive probabilities for interim monitoring of clinical trials},
	url = {https://journals.sagepub.com/doi/epub/10.1177/1740774514531352},
	language = {en},
	urldate = {2024-05-21},
	journal = {Clinical Trials},
	author = {Saville, Benjamin R and Connor, Jason T and Ayers, Gregory D and Alvarez, JoAnn},
	year = {2014},
	doi = {10.1177/1740774514531352},
	keywords = {Bayesian, trial design},
	file = {Accepted Version:C\:\\Users\\anbe6\\Zotero\\storage\\M6RFDI7T\\The utility of Bayesian predictive probabilities f.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\8KV8DHEL\\1740774514531352.html:text/html},
}

@article{hong_improving_2022,
	title = {Improving clinical trials using {Bayesian} adaptive designs: a breast cancer example},
	volume = {22},
	issn = {1471-2288},
	shorttitle = {Improving clinical trials using {Bayesian} adaptive designs},
	url = {https://doi.org/10.1186/s12874-022-01603-y},
	doi = {10.1186/s12874-022-01603-y},
	abstract = {To perform virtual re-executions of a breast cancer clinical trial with a time-to-event outcome to demonstrate what would have happened if the trial had used various Bayesian adaptive designs instead.},
	language = {en},
	number = {1},
	urldate = {2024-05-21},
	journal = {BMC Med Res Methodol},
	author = {Hong, Wei and McLachlan, Sue-Anne and Moore, Melissa and Mahar, Robert K.},
	month = may,
	year = {2022},
	keywords = {survival analysis, Bayesian, trial design},
	pages = {133},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\NEAPRNBG\\Hong et al. - 2022 - Improving clinical trials using Bayesian adaptive .pdf:application/pdf},
}

@article{criner_improving_2019,
	title = {Improving {Lung} {Function} in {Severe} {Heterogenous} {Emphysema} with the {Spiration} {Valve} {System} ({EMPROVE}). {A} {Multicenter}, {Open}-{Label} {Randomized} {Controlled} {Clinical} {Trial}},
	volume = {200},
	issn = {1073-449X, 1535-4970},
	url = {https://www.atsjournals.org/doi/10.1164/rccm.201902-0383OC},
	doi = {10.1164/rccm.201902-0383OC},
	language = {en},
	number = {11},
	urldate = {2024-05-21},
	journal = {Am J Respir Crit Care Med},
	author = {Criner, Gerard J. and Delage, Antoine and Voelker, Kirk and Hogarth, D. Kyle and Majid, Adnan and Zgoda, Michael and Lazarus, Donald R. and Casal, Roberto and Benzaquen, Sadia B. and Holladay, Robert C. and Wellikoff, Adam and Calero, Karel and Rumbak, Mark J. and Branca, Paul R. and Abu-Hijleh, Muhanned and Mallea, Jorge M. and Kalhan, Ravi and Sachdeva, Ashutosh and Kinsey, C. Matthew and Lamb, Carla R. and Reed, Michael F. and Abouzgheib, Wissam B. and Kaplan, Phillip V. and Marrujo, Gregory X. and Johnstone, David W. and Gasparri, Mario G. and Meade, Arturo A. and Hergott, Christopher A. and Reddy, Chakravarthy and Mularski, Richard A. and Case, Amy Hajari and Makani, Samir S. and Shepherd, Ray W. and Chen, Benson and Holt, Gregory E. and Martel, Simon},
	month = dec,
	year = {2019},
	keywords = {Bayesian, trial design},
	pages = {1354--1362},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\EFCGRMBX\\Criner et al. - 2019 - Improving Lung Function in Severe Heterogenous Emp.pdf:application/pdf},
}

@article{broglio_not_2014,
	title = {Not {Too} {Big}, {Not} {Too} {Small}: {A} {Goldilocks} {Approach} {To} {Sample} {Size} {Selection}},
	volume = {24},
	issn = {1054-3406, 1520-5711},
	shorttitle = {Not {Too} {Big}, {Not} {Too} {Small}},
	url = {https://www.tandfonline.com/doi/full/10.1080/10543406.2014.888569},
	doi = {10.1080/10543406.2014.888569},
	language = {en},
	number = {3},
	urldate = {2024-05-21},
	journal = {Journal of Biopharmaceutical Statistics},
	author = {Broglio, Kristine R. and Connor, Jason T. and Berry, Scott M.},
	month = may,
	year = {2014},
	keywords = {sample size, Bayesian, trial design},
	pages = {685--705},
	file = {Broglio et al. - 2014 - Not Too Big, Not Too Small A Goldilocks Approach .pdf:C\:\\Users\\anbe6\\Zotero\\storage\\ID9QDS7I\\Broglio et al. - 2014 - Not Too Big, Not Too Small A Goldilocks Approach .pdf:application/pdf},
}

@article{muehlemann_tutorial_2023,
	title = {A {Tutorial} on {Modern} {Bayesian} {Methods} in {Clinical} {Trials}},
	volume = {57},
	issn = {2168-4804},
	url = {https://doi.org/10.1007/s43441-023-00515-3},
	doi = {10.1007/s43441-023-00515-3},
	abstract = {Clinical trials continue to be the gold standard for evaluating new medical technologies. New advancements in modern computation power have led to increasing interest in Bayesian methods. Despite the multiple benefits of Bayesian approaches, application to clinical trials has been limited. Based on insights from the survey of clinical researchers in drug development conducted by the Drug Information Association Bayesian Scientific Working Group (DIA BSWG), insufficient knowledge of Bayesian approaches was ranked as the most important perceived barrier to implementing Bayesian methods. Results of the same survey indicate that clinical researchers may find the interpretation of results from a Bayesian analysis to be more useful than conventional interpretations. In this article, we illustrate key concepts tied to Bayesian methods, starting with familiar concepts widely used in clinical practice before advancing in complexity, and use practical illustrations from clinical development.},
	language = {en},
	number = {3},
	urldate = {2024-05-21},
	journal = {Ther Innov Regul Sci},
	author = {Muehlemann, Natalia and Zhou, Tianjian and Mukherjee, Rajat and Hossain, Munshi Imran and Roychoudhury, Satrajit and Russek-Cohen, Estelle},
	month = may,
	year = {2023},
	keywords = {Bayesian, trial design},
	pages = {402--416},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\SRUIR29X\\Muehlemann et al. - 2023 - A Tutorial on Modern Bayesian Methods in Clinical .pdf:application/pdf},
}

@article{callaway_difference--differences_2021,
	title = {Difference-in-{Differences} with a {Continuous} {Treatment}},
	abstract = {This paper analyzes difference-in-differences designs with a continuous treatment. We show that treatment effect on the treated-type parameters can be identified under a generalized parallel trends assumption that is similar to the binary treatment setup. However, interpreting differences in these parameters across different values of the treatment can be particularly challenging due to selection bias that is not ruled out by the parallel trends assumption. We discuss alternative, typically stronger, assumptions that alleviate these challenges. We also provide a variety of treatment effect decomposition results, highlighting that parameters associated with popular linear two-way fixed-effect (TWFE) specifications can be hard to interpret, even when there are only two time periods. We introduce alternative estimation procedures that do not suffer from these TWFE drawbacks, and show in an application that they can lead to different conclusions.},
	language = {en},
	author = {Callaway, Brantly and Goodman-Bacon, Andrew and Sant’Anna, Pedro H C},
	year = {2021},
	keywords = {DiD},
	file = {Callaway et al. - Difference-in-Differences with a Continuous Treatm.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\SEVLU9SD\\Callaway et al. - Difference-in-Differences with a Continuous Treatm.pdf:application/pdf},
}

@article{callaway_difference--differences_2021-1,
	series = {Themed {Issue}: {Treatment} {Effect} 1},
	title = {Difference-in-{Differences} with multiple time periods},
	volume = {225},
	issn = {0304-4076},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407620303948},
	doi = {10.1016/j.jeconom.2020.12.001},
	abstract = {In this article, we consider identification, estimation, and inference procedures for treatment effect parameters using Difference-in-Differences (DiD) with (i) multiple time periods, (ii) variation in treatment timing, and (iii) when the “parallel trends assumption” holds potentially only after conditioning on observed covariates. We show that a family of causal effect parameters are identified in staggered DiD setups, even if differences in observed characteristics create non-parallel outcome dynamics between groups. Our identification results allow one to use outcome regression, inverse probability weighting, or doubly-robust estimands. We also propose different aggregation schemes that can be used to highlight treatment effect heterogeneity across different dimensions as well as to summarize the overall effect of participating in the treatment. We establish the asymptotic properties of the proposed estimators and prove the validity of a computationally convenient bootstrap procedure to conduct asymptotically valid simultaneous (instead of pointwise) inference. Finally, we illustrate the relevance of our proposed tools by analyzing the effect of the minimum wage on teen employment from 2001–2007. Open-source software is available for implementing the proposed methods.},
	number = {2},
	urldate = {2024-05-20},
	journal = {Journal of Econometrics},
	author = {Callaway, Brantly and Sant’Anna, Pedro H. C.},
	month = dec,
	year = {2021},
	keywords = {DiD},
	pages = {200--230},
	file = {ScienceDirect Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\IPBDIANW\\S0304407620303948.html:text/html;Submitted Version:C\:\\Users\\anbe6\\Zotero\\storage\\GKRK59CV\\Callaway and Sant’Anna - 2021 - Difference-in-Differences with multiple time perio.pdf:application/pdf},
}

@incollection{gelman_propensity_2004,
	edition = {1},
	title = {The {Propensity} {Score} with {Continuous} {Treatments}},
	isbn = {978-0-470-09043-5 978-0-470-09045-9},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0470090456.ch7},
	language = {en},
	urldate = {2024-05-16},
	booktitle = {Wiley {Series} in {Probability} and {Statistics}},
	publisher = {Wiley},
	author = {Hirano, Keisuke and Imbens, Guido W.},
	editor = {Gelman, Andrew and Meng, Xiao‐Li},
	month = jul,
	year = {2004},
	doi = {10.1002/0470090456.ch7},
	keywords = {causal inference, propensity score},
	pages = {73--84},
	file = {Hirano and Imbens - 2004 - The Propensity Score with Continuous Treatments.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\8WBMW922\\Hirano and Imbens - 2004 - The Propensity Score with Continuous Treatments.pdf:application/pdf},
}

@misc{arkhangelsky_synthetic_2021,
	title = {Synthetic {Difference} in {Differences}},
	url = {http://arxiv.org/abs/1812.09970},
	doi = {10.48550/arXiv.1812.09970},
	abstract = {We present a new estimator for causal effects with panel data that builds on insights behind the widely used difference in differences and synthetic control methods. Relative to these methods we find, both theoretically and empirically, that this "synthetic difference in differences" estimator has desirable robustness properties, and that it performs well in settings where the conventional estimators are commonly used in practice. We study the asymptotic behavior of the estimator when the systematic part of the outcome model includes latent unit factors interacted with latent time factors, and we present conditions for consistency and asymptotic normality.},
	urldate = {2024-05-16},
	publisher = {arXiv},
	author = {Arkhangelsky, Dmitry and Athey, Susan and Hirshberg, David A. and Imbens, Guido W. and Wager, Stefan},
	month = jul,
	year = {2021},
	note = {arXiv:1812.09970 [stat]},
	keywords = {causal inference, DiD},
	file = {arXiv Fulltext PDF:C\:\\Users\\anbe6\\Zotero\\storage\\3MRQNM53\\Arkhangelsky et al. - 2021 - Synthetic Difference in Differences.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\IQLETMMF\\1812.html:text/html},
}

@article{cornell_social_2023,
	title = {Social {Work} {Staffing} and {Use} of {Palliative} {Care} {Among} {Recently} {Hospitalized} {Veterans}},
	volume = {6},
	issn = {2574-3805},
	url = {https://doi.org/10.1001/jamanetworkopen.2022.49731},
	doi = {10.1001/jamanetworkopen.2022.49731},
	abstract = {Palliative care improves quality of life for patients and families but may be underused.To assess the association of an intervention to increase social work staffing in Veterans Health Administration primary care teams with use of palliative care among veterans with a recent hospitalization.This cohort study used differences-in-differences analyses of the change in palliative care use associated with implementation of the Social Work Patient Aligned Care Team (PACT) staffing program, conducted from October 1, 2016, to September 30, 2019. The study included 71 VA primary care sites serving rural veterans. Participants were adult veterans who received primary care services from a site enrolled in the program and who received inpatient hospital care. Data were analyzed from January 2020 to August 2022.The PACT staffing program was a clinic-level intervention that provided 3-year seed funding to Veterans Health Administration medical centers to hire 1 or more additional social workers in primary care teams. Staggered timing of the intervention enabled comparison of mean outcomes across sites before and after the intervention.The primary outcome was the number of individuals per 1000 veterans who had any palliative care use in 30 days after an inpatient hospital stay.The analytic sample included 43 200 veterans (mean [SD] age, 65.34 [13.95] years; 37 259 [86.25\%] men) and a total of 91 675 episodes of inpatient hospital care. Among the total cohort, 8611 veterans (9.39\%) were Black, 77 069 veterans (84.07\%) were White, and 2679 veterans (2.92\%) were another race (including American Indian or Alaskan Native, Asian, and Native Hawaiian or other Pacific Islander). A mean of 14.5 individuals per 1000 veterans (1329 individuals in all) used palliative care after a hospital stay. After the intervention, there was an increase of 15.6 (95\% CI, 9.2-22.3) individuals per 1000 veterans using palliative or hospice care after a hospital stay, controlling for national time trends and veteran characteristics—a 2-fold difference relative to the mean.This cohort study found significant increases in use of palliative care for recently hospitalized veterans whose primary care team had additional social work staffing. These findings suggest that social workers may increase access to and/or use of palliative care. Future work should assess the mechanism for this association and whether the increase in palliative care is associated with other health or health care outcomes.},
	number = {1},
	urldate = {2024-05-15},
	journal = {JAMA Network Open},
	author = {Cornell, Portia Y. and Halladay, Christopher W. and Montano, Anna-Rae and Celardo, Caitlin and Chmelka, Gina and Silva, Jennifer W. and Rudolph, James L.},
	month = jan,
	year = {2023},
	keywords = {DiD, palliative care},
	pages = {e2249731},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\BIW3F69Q\\Cornell et al. - 2023 - Social Work Staffing and Use of Palliative Care Am.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\3KBDEXNT\\2800000.html:text/html},
}

@article{lima_medicaid_2024,
	title = {Medicaid expansion and palliative care for advanced-stage liver cancer},
	volume = {28},
	issn = {1091-255X},
	url = {https://www.sciencedirect.com/science/article/pii/S1091255X24001495},
	doi = {10.1016/j.gassur.2024.01.042},
	abstract = {Background
Medicaid expansion (ME) has contributed to transforming the United States healthcare system. However, its effect on palliative care of primary liver cancers remains unknown. This study aimed to evaluate the association between ME and the receipt of palliative treatment in advanced-stage liver cancer.
Methods
Patients diagnosed with stage IV hepatocellular carcinoma or intrahepatic cholangiocarcinoma were identified from the National Cancer Database and divided into pre-expansion (2010-2013) and postexpansion (2015-2019) cohorts. Logistic regression identified predictors of palliative treatment. Difference-in-difference (DID) analysis assessed changes in palliative care use between patients living in ME states and patients living in non-ME states.
Results
Among 12,516 patients, 4582 (36.6\%) were diagnosed before expansion, and 7934 (63.6\%) were diagnosed after expansion. Overall, rates of palliative treatment increased after ME (18.1\% [pre-expansion] vs 22.3\% [postexpansion]; P {\textless} .001) and are more pronounced among ME states. Before expansion, only cancer type and education attainment were associated with the receipt of palliative treatment. Conversely, after expansion, race, insurance, location, cancer type, and ME status (odds ratio [OR], 1.23; 95\% CI, 1.06-1.44; P = .018) were all associated with palliative care. Interestingly, the odds were higher if treatment involved receipt of pain management (OR, 2.05; 95\% CI, 1.23-2.43; P = .006). Adjusted DID analysis confirmed increased rates of palliative treatment among patients living in ME states relative to non-ME states (DID, 4.4\%; 95\% CI, 1.2-7.7; P = .008); however, racial disparities persist (White, 5.6; 95\% CI, 1.4-9.8; P = .009; minority, 2.6; 95\% CI, −2.5 to 7.6; P = .333).
Conclusion
The implementation of ME contributed to increased rates of palliative treatment for patients residing in ME states after expansion. However, racial disparities persist even after ME, resulting in inequitable access to palliative care.},
	number = {4},
	urldate = {2024-05-15},
	journal = {Journal of Gastrointestinal Surgery},
	author = {Lima, Henrique A. and Mavani, Parit and Munir, Muhammad Musaab and Endo, Yutaka and Woldesenbet, Selamawit and Khan, Muhammad Muntazir Mehdi and Rawicz-Pruszyński, Karol and Waqar, Usama and Katayama, Erryk and Resende, Vivian and Khalil, Mujtaba and Pawlik, Timothy M.},
	month = apr,
	year = {2024},
	keywords = {Difference in difference, Hepatocellular carcinoma, Intrahepatic cholangiocarcinoma, Medicaid expansion, Palliative treatment},
	pages = {434--441},
	file = {ScienceDirect Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\CCDVMBZN\\S1091255X24001495.html:text/html},
}

@article{carlson_regulating_2008,
	title = {Regulating {Palliative} {Care}: {The} {Case} of {Hospice}},
	volume = {36},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {08853924},
	shorttitle = {Regulating {Palliative} {Care}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0885392408001152},
	doi = {10.1016/j.jpainsymman.2007.09.014},
	abstract = {Palliative care services provided to patients and families vary substantially across hospices. Literature suggests regulation can act as a standardizing force in health care delivery. However, little is known about the effect of regulation on the delivery of palliative care in hospice and whether its effect differs for different types of hospice providers. We estimated the association between regulation, deﬁned as Medicare hospice certiﬁcation, and the delivery of palliative care in hospice using a nationally representative data set of 9,409 patients from 2,066 hospices surveyed in the National Home and Hospice Care Survey, 1992e2000.},
	language = {en},
	number = {2},
	urldate = {2024-05-15},
	journal = {Journal of Pain and Symptom Management},
	author = {Carlson, Melissa D.A. and Schlesinger, Mark and Holford, Theodore R. and Morrison, R. Sean and Bradley, Elizabeth H.},
	month = aug,
	year = {2008},
	pages = {107--116},
	file = {Carlson et al. - 2008 - Regulating Palliative Care The Case of Hospice.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\6LHS63JW\\Carlson et al. - 2008 - Regulating Palliative Care The Case of Hospice.pdf:application/pdf},
}

@article{hua_association_2020,
	title = {Association {Between} the {Implementation} of {Hospital}-{Based} {Palliative} {Care} and {Use} of {Intensive} {Care} {During} {Terminal} {Hospitalizations}},
	volume = {3},
	issn = {2574-3805},
	url = {https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2758406},
	doi = {10.1001/jamanetworkopen.2019.18675},
	abstract = {OBJECTIVE To determine whether implementation of hospital-based palliative care services is associated with decreased intensive care unit (ICU) use during terminal hospitalizations. DESIGN, SETTING, AND PARTICIPANTS This cohort study included 51 hospitals in New York State that either did or did not implement a palliative care program between 2008 and 2014. Hospitals that consistently had a palliative care program during the study period were excluded. Participants were adult patients who died during hospitalization. Data analysis was performed between January 2018 and July 2019. EXPOSURE Implementation of a palliative care program. MAIN OUTCOMES AND MEASURES The primary outcome was ICU use. A difference-in-differences analysis was performed using multilevel regression to assess the association between implementing a palliative care program and ICU use during terminal hospitalizations while adjusting for patient and hospital characteristics and time trends.
RESULTS During the study period, 73 370 patients (mean [SD] age, 76.5 [14.1] years; 38 467 [52.4\%] women) died during hospitalization, of whom 37 628 (51.3\%) received care in hospitals that implemented palliative care services and 35 742 (48.7\%) received care in a hospital without palliative care implementation. Patients who received care in hospitals after implementation of palliative care services were less likely to receive intensive care than patients admitted to the same hospitals before implementation (49.3\% vs 52.8\%; difference 3.5\%; 95\% CI, 2.5\%-4.5\%; P {\textless} .001). Compared with hospitals that never had a palliative care program, the implementation of palliative care was associated with a 10\% reduction in ICU use during terminal hospitalizations (adjusted relative risk, 0.90; 95\% CI, 0.85-0.95; P {\textless} .001).
CONCLUSIONS AND RELEVANCE The implementation of hospital-based palliative care services in New York State was associated with a modest reduction in ICU use during terminal hospitalizations.},
	language = {en},
	number = {1},
	urldate = {2024-05-15},
	journal = {JAMA Netw Open},
	author = {Hua, May and Lu, Yewei and Ma, Xiaoyue and Morrison, R. Sean and Li, Guohua and Wunsch, Hannah},
	month = jan,
	year = {2020},
	keywords = {DiD, palliative care},
	pages = {e1918675},
	file = {Hua et al. - 2020 - Association Between the Implementation of Hospital.pdf:C\:\\Users\\anbe6\\Zotero\\storage\\K44JY6UL\\Hua et al. - 2020 - Association Between the Implementation of Hospital.pdf:application/pdf},
}

@article{wing_designing_2018,
	title = {Designing {Difference} in {Difference} {Studies}: {Best} {Practices} for {Public} {Health} {Policy} {Research}},
	volume = {39},
	issn = {0163-7525, 1545-2093},
	shorttitle = {Designing {Difference} in {Difference} {Studies}},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev-publhealth-040617-013507},
	doi = {10.1146/annurev-publhealth-040617-013507},
	abstract = {The difference in difference (DID) design is a quasi-experimental research design that researchers often use to study causal relationships in public health settings where randomized controlled trials (RCTs) are infeasible or unethical. However, causal inference poses many challenges in DID designs. In this article, we review key features of DID designs with an emphasis on public health policy research. Contemporary researchers should take an active approach to the design of DID studies, seeking to construct comparison groups, sensitivity analyses, and robustness checks that help validate the method\&apos;s assumptions. We explain the key assumptions of the design and discuss analytic tactics, supplementary analysis, and approaches to statistical inference that are often important in applied research. The DID design is not a perfect substitute for randomized experiments, but it often represents a feasible way to learn about casual relationships. We conclude by noting that combining elements from multiple quasi-experimental techniques may be important in the next wave of innovations to the DID approach.},
	language = {en},
	number = {Volume 39, 2018},
	urldate = {2024-05-15},
	journal = {Annual Review of Public Health},
	author = {Wing, Coady and Simon, Kosali and Bello-Gomez, Ricardo A.},
	month = apr,
	year = {2018},
	note = {Publisher: Annual Reviews},
	keywords = {causal inference, DiD},
	pages = {453--469},
	file = {Full Text:C\:\\Users\\anbe6\\Zotero\\storage\\RBCL586A\\Wing et al. - 2018 - Designing Difference in Difference Studies Best P.pdf:application/pdf;Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\S85ND9D8\\annurev-publhealth-040617-013507.html:text/html},
}

@article{robins_marginal_2000,
	title = {Marginal {Structural} {Models} and {Causal} {Inference} in {Epidemiology}},
	volume = {11},
	issn = {1044-3983},
	url = {https://journals.lww.com/epidem/fulltext/2000/09000/marginal_structural_models_and_causal_inference_in.11.aspx},
	abstract = {In observational studies with exposures or treatments that vary over time, standard approaches for adjustment of confounding are biased when there exist time-dependent confounders that are also affected by previous treatment. This paper introduces marginal structural models, a new class of causal models that allow for improved adjustment of confounding in those situations. The parameters of a marginal structural model can be consistently estimated using a new class of estimators, the inverse-probability-of-treatment weighted estimators.},
	language = {en-US},
	number = {5},
	urldate = {2024-05-15},
	journal = {Epidemiology},
	author = {Robins, James M. and Hernán, Miguel Ángel and Brumback, Babette},
	month = sep,
	year = {2000},
	keywords = {causal inference, propensity score},
	pages = {550},
	file = {Snapshot:C\:\\Users\\anbe6\\Zotero\\storage\\VRSQ8QR8\\marginal_structural_models_and_causal_inference_in.11.html:text/html},
}

@article{caniglia_difference--difference_2020,
	title = {Difference-in-{Difference} in the {Time} of {Cholera}: a {Gentle} {Introduction} for {Epidemiologists}},
	volume = {7},
	issn = {2196-2995},
	shorttitle = {Difference-in-{Difference} in the {Time} of {Cholera}},
	url = {https://doi.org/10.1007/s40471-020-00245-2},
	doi = {10.1007/s40471-020-00245-2},
	abstract = {The goal of this article is to provide an introduction to the intuition behind the difference-in-difference method for epidemiologists. We focus on the theoretical aspects of this tool, including the types of questions for which difference-in-difference is appropriate, and what assumptions must hold for the results to be causally interpretable.},
	language = {en},
	number = {4},
	urldate = {2024-05-14},
	journal = {Curr Epidemiol Rep},
	author = {Caniglia, Ellen C. and Murray, Eleanor J.},
	month = dec,
	year = {2020},
	keywords = {causal inference, DiD},
	pages = {203--211},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\5HJF2D6H\\Caniglia and Murray - 2020 - Difference-in-Difference in the Time of Cholera a.pdf:application/pdf},
}

@misc{card_minimum_1993,
	type = {Working {Paper}},
	series = {Working {Paper} {Series}},
	title = {Minimum {Wages} and {Employment}: {A} {Case} {Study} of the {Fast} {Food} {Industry} in {New} {Jersey} and {Pennsylvania}},
	shorttitle = {Minimum {Wages} and {Employment}},
	url = {https://www.nber.org/papers/w4509},
	doi = {10.3386/w4509},
	abstract = {On April 1, 1992 New Jersey's minimum wage increased from 4.25 to 5.05 per hour. To evaluate the impact of the law we surveyed 410 fast food restaurants in New Jersey and Pennsylvania before and after the rise in the minimum. Comparisons of the changes in wages, employment, and prices at stores in New Jersey relative to stores in Pennsylvania (where the minimum wage remained fixed at \$4.25 per hour) yield simple estimates of the effect of the higher minimum wage. Our empirical findings challenge the prediction that a rise in the minimum reduces employment. Relative to stores in Pennsylvania, fast food restaurants in New Jersey increased employment by 13 percent. We also compare employment growth at stores in New Jersey that were initially paying high wages (and were unaffected by the new law) to employment changes at lower-wage stores. Stores that were unaffected by the minimum wage had the same employment growth as stores in Pennsylvania, while stores that had to increase their wages increased their employment.},
	urldate = {2024-05-14},
	publisher = {National Bureau of Economic Research},
	author = {Card, David and Krueger, Alan B.},
	month = oct,
	year = {1993},
	doi = {10.3386/w4509},
	keywords = {causal inference, DiD},
	file = {Full Text PDF:C\:\\Users\\anbe6\\Zotero\\storage\\Q6SHU4HX\\Card and Krueger - 1993 - Minimum Wages and Employment A Case Study of the .pdf:application/pdf},
}
